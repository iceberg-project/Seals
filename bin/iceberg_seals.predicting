#!/usr/bin/env python
"""
Predict sealnet
================================================================================

CNN deployment script for ICEBERG seals use case. Splits a raster into tiles 
and predicts seal counts and locations in tiles with 'predict_sealnet.py'.

Author: Bento Goncalves, Ioannis Paraskevakos
License: MIT
Copyright: 2018-2019
"""

import torch
import warnings
import argparse
import os
import shutil
from iceberg_seals.search.utils.model_library import *
from iceberg_seals.search.predicting.predict_sealnet import predict_patch, parse_args
warnings.filterwarnings('ignore', module='PIL')


if __name__ == "__main__":
    args = parse_args()

    # check for invalid inputs
    assert args.model_architecture in model_archs, "Invalid architecture -- see supported" \
                                                   " architectures: %s" % list(model_archs.keys())

    assert args.hyperparameter_set in hyperparameters, "Hyperparameter combination is not defined in " \
                                                       "./utils/model_library.py"

    # find pipeline
    pipeline = model_archs[args.model_architecture]['pipeline']
    
    # create model instance
    model = model_defs[pipeline][args.model_architecture]

    # check for GPU support and set model to evaluation mode
    use_gpu = torch.cuda.is_available()
    if use_gpu:
        model.cuda()
    model.eval()

    # load saved model weights from pt_train.py
    model.load_state_dict(torch.load("%s/%s.tar" % (args.models_folder, args.model_name)))

    # run validation to get confusion matrix
    predict_patch(input_image=args.input_dir.split('/')[-1], model=model,
                  input_size=model_archs[args.model_architecture]['input_size'],
                  input_dir=args.input_dir, output_dir=args.output_dir,
                  batch_size=hyperparameters[args.hyperparameter_set]['batch_size_test'],
                  num_workers=hyperparameters[args.hyperparameter_set]['num_workers_train'],
                  remove_tiles=True)
