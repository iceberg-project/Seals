{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seal Detection Pipeline\n",
    "---\n",
    "\n",
    "This jupyter notebook will go through assembling the main components of a complete pipeline for counting seals in high-resolution satellite imagery (figure 1, steps 3 and 4) and show some experimental results with different pipeline designs. The ultimate goal of this pipeline is to perform a pan-Antarctic pack-ice seal census. ** Running this code will require input satellite imagery and at least one GPU with >8GB of memory **\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"jupyter_notebook_images/Base Pipeline.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "---\n",
    "* [Getting started](#intro)\n",
    "    * [Setup](#setup)\n",
    "    * [Visualize training set](#vis_imgs)\n",
    "* [Pipeline 1 - Seal haulout detector](#1)\n",
    "    * [Training](#1T)\n",
    "    * [Validation](#1V)\n",
    "* [Pipeline 1.1 - Seal haulout detector + count](#1.1)\n",
    "    * [Training](#1.1T)\n",
    "    * [Validation](#1.1V)\n",
    "    * [Testing](#1.1T)\n",
    "* [Pipeline 1.2 - Seal haulout detector + single seal detector](#1.2)\n",
    "    * [Training](#1.2T)\n",
    "    * [Validation](#1.2V)\n",
    "    * [Testing](#1.2T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started<a name=\"intro\"></a>\n",
    "---\n",
    "\n",
    "If you followed the *training_set_generation* jupyter notebook (also present in this repo), you should have training sets generated and hyperparameter sets to try out, and be ready to search for a best performing seal detection pipeline.  Output files in this repository are organized as follows: *'./saved_models/{pipeline}/{model_settings}/{model_settings}_{file}'*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment<a name=\"setup\"></a>\n",
    "\n",
    "Before training and validating model/hyperparameter combinations inside the pipelines, we need to load the required python modules and a few global variables. Running this script will also display a list of training classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crabeater', 'crack', 'emperor', 'glacier', 'ice-sheet', 'marching-emperor', 'open-water', 'other', 'pack-ice', 'rock', 'weddell']\n"
     ]
    }
   ],
   "source": [
    "# import required packages\n",
    "import os\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from functools import reduce\n",
    "from utils.model_library import * \n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi']= 400\n",
    "\n",
    "# display class names\n",
    "class_names = sorted([subdir for subdir in os.listdir('./training_sets/training_set_vanilla/training')])\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training images (Optional)<a name=\"vis_imgs\"></a>\n",
    "\n",
    "To get a better sense for what the training set is like, the next cell will display a few random images from the training classes. Displayed images are extracted from a pool of ~70000 training images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store images\n",
    "images = []\n",
    "\n",
    "# loop over labels\n",
    "for label in class_names:\n",
    "    for path, _, files in os.walk('./training_sets/training_set_vanilla/training/{}'.format(label)):\n",
    "        files = np.random.choice(files, 5)\n",
    "        for filename in files:\n",
    "            images.append(np.asarray(Image.open(os.path.join(path, filename))))\n",
    "\n",
    "images = np.array(images)\n",
    "\n",
    "# display images \n",
    "ncols=len(class_names)\n",
    "nindex, height, width, intensity = images.shape\n",
    "nrows = nindex//ncols\n",
    "assert nindex == nrows*ncols\n",
    "result = (images.reshape(nrows, ncols, height, width, intensity)\n",
    "          .swapaxes(1,2)\n",
    "          .reshape(height*nrows, width*ncols, intensity))\n",
    "\n",
    "plt.imshow(result)\n",
    "cur_axes = plt.gca()\n",
    "cur_axes.axes.get_xaxis().set_visible(False)\n",
    "cur_axes.axes.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1 - haulout detector<a name=\"1\"></a>\n",
    "---\n",
    "\n",
    "The simplest pipeline we can use is one that just uses an object classification step to find seal haulouts or penguins colonies. The obvious downside for this approach is that we often have more than one seal in a haulout, which is hardly usefull if we are looking for a count. However, we will use this as a 'pre-preocessing' step, where we narrow down the totality of patches to the subset where the haulout detection CNN flagged groups of seals. To validate the usefulness of this preprocessing step we can compare results obtained with the full pipeline (i.e. haul out detector + count) to one that simply tries to count on all tiles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"1T\"></a>\n",
    "\n",
    "The first step to find a best performing model is to train different model setups using our training set. To keep track of which combinations we have tried, how well they performed and the specifics of each model setup, we will store results in folders (under './saved_models') named after each specific model combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate model combinations\n",
    "combinations_1 = {'model_architecture': ['Resnet18'] * 4 + ['NasnetA'] * 4,\n",
    "                  'training_dir': ['training_set_vanilla', 'training_set_multiscale_A'] * 4,\n",
    "                  'hyperparameter_set': ['A'] * 4 + ['B'] * 4,\n",
    "                  'cv_weights': ['NO', 'NO', 'WCV', 'WCV'] * 2}\n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_1 = pd.DataFrame(combinations_1)\n",
    "                    \n",
    "\n",
    "# create folders for resulting files\n",
    "for row in combinations_1.iterrows():\n",
    "    mdl = '{}_{}_{}'.format(row[1]['model_architecture'], row[1]['training_dir'],\\\n",
    "                            row[1]['cv_weights'])                     \n",
    "    if not os.path.exists(\"./saved_models/Pipeline1/{}\".format(mdl)):\n",
    "        os.makedirs(\"./saved_models/Pipeline1/{}\".format(mdl)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then provide model combinations created above as arguments to the training script, *train_sealnet.py*. A list of required arguments can be displayed by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run train_sealnet.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet18_training_set_vanilla_NO was already trained\n",
      "Resnet18_training_set_multiscale_A_NO was already trained\n",
      "\n",
      "training Resnet18_training_set_vanilla_WCV\n",
      "\n",
      "Epoch 1/5\n",
      "----------\n",
      "training Loss: 0.0403 training Acc: 0.5774\n",
      "validation Loss: 2.3834 validation Acc: 0.4523\n",
      "training time: 0.0h 13m 21s\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n",
      "training Loss: 0.0264 training Acc: 0.7323\n",
      "validation Loss: 2.2305 validation Acc: 0.3492\n",
      "training time: 0.0h 26m 36s\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n",
      "training Loss: 0.0170 training Acc: 0.8299\n",
      "validation Loss: 0.2666 validation Acc: 0.7554\n",
      "training time: 0.0h 39m 50s\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n",
      "training Loss: 0.0119 training Acc: 0.8762\n",
      "validation Loss: 0.2057 validation Acc: 0.8429\n",
      "training time: 0.0h 53m 52s\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n",
      "training Loss: 0.0098 training Acc: 0.8988\n",
      "validation Loss: 0.2278 validation Acc: 0.8379\n",
      "training time: 1.0h 8m 9s\n",
      "\n",
      "Training complete in 1.0h 8m 9s\n",
      "\n",
      "training Resnet18_training_set_multiscale_A_WCV\n",
      "\n",
      "Epoch 1/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n",
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/utils/_process_posix.py\", line 161, in system\n",
      "    res_idx = child.expect_list(patterns, self.read_timeout)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/spawnbase.py\", line 345, in expect_list\n",
      "    return exp.expect_loop(timeout)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/expect.py\", line 99, in expect_loop\n",
      "    incoming = spawn.read_nonblocking(spawn.maxread, timeout)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\", line 452, in read_nonblocking\n",
      "    r, w, e = select_ignore_interrupts([self.child_fd], [], [], timeout)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/utils.py\", line 138, in select_ignore_interrupts\n",
      "    return select.select(iwtd, owtd, ewtd, timeout)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-17a9dfb9545c>\", line 19, in <module>\n",
      "    get_ipython().system('python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch                              --hyperparameter_set=$hyp_st --cv_weights=$cv_wgt                              --output_name=$out --pipeline=$pipeline')\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2197, in system_piped\n",
      "    self.user_ns['_exit_code'] = system(self.var_expand(cmd, depth=1))\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/utils/_process_posix.py\", line 172, in system\n",
      "    child.sendline(chr(3))\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\", line 535, in sendline\n",
      "    return self.send(s + self.linesep)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\", line 520, in send\n",
      "    time.sleep(self.delaybeforesend)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/inspect.py\", line 1438, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/inspect.py\", line 690, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1809, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1371, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1279, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1128, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1066, in format_exception_as_a_whole\n",
      "    records = self.get_records(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1105, in get_records\n",
      "    traceback.print_exc(file=self.ostream)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 159, in print_exc\n",
      "    print_exception(*sys.exc_info(), limit=limit, file=file, chain=chain)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 100, in print_exception\n",
      "    type(value), value, tb, limit=limit).format(chain=chain):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 497, in __init__\n",
      "    capture_locals=capture_locals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 358, in extract\n",
      "    f.line\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 282, in line\n",
      "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 16, in getline\n",
      "    lines = getlines(filename, module_globals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 454, in open\n",
      "    encoding, lines = detect_encoding(buffer.readline)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 423, in detect_encoding\n",
      "    first = read_or_stop()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 381, in read_or_stop\n",
      "    return readline()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2879, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1821, in showtraceback\n",
      "    print('\\n' + self.get_exception_only(), file=sys.stderr)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1766, in get_exception_only\n",
      "    msg = traceback.format_exception_only(etype, value)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 136, in format_exception_only\n",
      "    return list(TracebackException(etype, value, None).format_exception_only())\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 510, in __init__\n",
      "    self._load_lines()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 522, in _load_lines\n",
      "    self.__context__._load_lines()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 522, in _load_lines\n",
      "    self.__context__._load_lines()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 522, in _load_lines\n",
      "    self.__context__._load_lines()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 520, in _load_lines\n",
      "    frame.line\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 282, in line\n",
      "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 16, in getline\n",
      "    lines = getlines(filename, module_globals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 454, in open\n",
      "    encoding, lines = detect_encoding(buffer.readline)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 423, in detect_encoding\n",
      "    first = read_or_stop()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 381, in read_or_stop\n",
      "    return readline()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/inspect.py\", line 1442, in getframeinfo\n",
      "    lines, lnum = findsource(frame)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 177, in findsource\n",
      "    lines = linecache.getlines(file, globals_dict)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 454, in open\n",
      "    encoding, lines = detect_encoding(buffer.readline)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 423, in detect_encoding\n",
      "    first = read_or_stop()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 381, in read_or_stop\n",
      "    return readline()\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:Uncaught exception, closing connection.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/utils/_process_posix.py\", line 161, in system\n",
      "    res_idx = child.expect_list(patterns, self.read_timeout)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/spawnbase.py\", line 345, in expect_list\n",
      "    return exp.expect_loop(timeout)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/expect.py\", line 99, in expect_loop\n",
      "    incoming = spawn.read_nonblocking(spawn.maxread, timeout)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\", line 452, in read_nonblocking\n",
      "    r, w, e = select_ignore_interrupts([self.child_fd], [], [], timeout)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/utils.py\", line 138, in select_ignore_interrupts\n",
      "    return select.select(iwtd, owtd, ewtd, timeout)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-17a9dfb9545c>\", line 19, in <module>\n",
      "    get_ipython().system('python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch                              --hyperparameter_set=$hyp_st --cv_weights=$cv_wgt                              --output_name=$out --pipeline=$pipeline')\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2197, in system_piped\n",
      "    self.user_ns['_exit_code'] = system(self.var_expand(cmd, depth=1))\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/utils/_process_posix.py\", line 172, in system\n",
      "    child.sendline(chr(3))\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\", line 535, in sendline\n",
      "    return self.send(s + self.linesep)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\", line 520, in send\n",
      "    time.sleep(self.delaybeforesend)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/inspect.py\", line 1438, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/inspect.py\", line 690, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1809, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1371, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1279, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1128, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1066, in format_exception_as_a_whole\n",
      "    records = self.get_records(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1105, in get_records\n",
      "    traceback.print_exc(file=self.ostream)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 159, in print_exc\n",
      "    print_exception(*sys.exc_info(), limit=limit, file=file, chain=chain)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 100, in print_exception\n",
      "    type(value), value, tb, limit=limit).format(chain=chain):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 497, in __init__\n",
      "    capture_locals=capture_locals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 358, in extract\n",
      "    f.line\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 282, in line\n",
      "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 16, in getline\n",
      "    lines = getlines(filename, module_globals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 454, in open\n",
      "    encoding, lines = detect_encoding(buffer.readline)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 423, in detect_encoding\n",
      "    first = read_or_stop()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 381, in read_or_stop\n",
      "    return readline()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2879, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1821, in showtraceback\n",
      "    print('\\n' + self.get_exception_only(), file=sys.stderr)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1766, in get_exception_only\n",
      "    msg = traceback.format_exception_only(etype, value)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 136, in format_exception_only\n",
      "    return list(TracebackException(etype, value, None).format_exception_only())\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 510, in __init__\n",
      "    self._load_lines()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 522, in _load_lines\n",
      "    self.__context__._load_lines()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 522, in _load_lines\n",
      "    self.__context__._load_lines()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 522, in _load_lines\n",
      "    self.__context__._load_lines()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 520, in _load_lines\n",
      "    frame.line\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 282, in line\n",
      "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 16, in getline\n",
      "    lines = getlines(filename, module_globals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 454, in open\n",
      "    encoding, lines = detect_encoding(buffer.readline)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 423, in detect_encoding\n",
      "    first = read_or_stop()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 381, in read_or_stop\n",
      "    return readline()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1809, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1371, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1279, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\n",
      "TypeError: must be str, not list\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 237, in dispatch_shell\n",
      "    self.log.error(\"Exception in message handler:\", exc_info=True)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 1335, in error\n",
      "    self._log(ERROR, msg, args, **kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 1442, in _log\n",
      "    self.handle(record)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 1452, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 1514, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 863, in handle\n",
      "    self.emit(record)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 992, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 838, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 117, in format\n",
      "    return super(LevelFormatter, self).format(record)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 583, in format\n",
      "    record.exc_text = self.formatException(record.exc_info)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 533, in formatException\n",
      "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 100, in print_exception\n",
      "    type(value), value, tb, limit=limit).format(chain=chain):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 497, in __init__\n",
      "    capture_locals=capture_locals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 354, in extract\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/compilerop.py\", line 140, in check_linecache_ipython\n",
      "    linecache._checkcache_ori(*args)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 74, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:tornado.general:Uncaught exception, closing connection.\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/utils/_process_posix.py\", line 161, in system\n",
      "    res_idx = child.expect_list(patterns, self.read_timeout)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/spawnbase.py\", line 345, in expect_list\n",
      "    return exp.expect_loop(timeout)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/expect.py\", line 99, in expect_loop\n",
      "    incoming = spawn.read_nonblocking(spawn.maxread, timeout)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\", line 452, in read_nonblocking\n",
      "    r, w, e = select_ignore_interrupts([self.child_fd], [], [], timeout)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/utils.py\", line 138, in select_ignore_interrupts\n",
      "    return select.select(iwtd, owtd, ewtd, timeout)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2862, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-3-17a9dfb9545c>\", line 19, in <module>\n",
      "    get_ipython().system('python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch                              --hyperparameter_set=$hyp_st --cv_weights=$cv_wgt                              --output_name=$out --pipeline=$pipeline')\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2197, in system_piped\n",
      "    self.user_ns['_exit_code'] = system(self.var_expand(cmd, depth=1))\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/utils/_process_posix.py\", line 172, in system\n",
      "    child.sendline(chr(3))\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\", line 535, in sendline\n",
      "    return self.send(s + self.linesep)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\", line 520, in send\n",
      "    time.sleep(self.delaybeforesend)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1090, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 311, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 345, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/inspect.py\", line 1480, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/inspect.py\", line 1438, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/inspect.py\", line 690, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1809, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1371, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1279, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1128, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1066, in format_exception_as_a_whole\n",
      "    records = self.get_records(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1105, in get_records\n",
      "    traceback.print_exc(file=self.ostream)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 159, in print_exc\n",
      "    print_exception(*sys.exc_info(), limit=limit, file=file, chain=chain)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 100, in print_exception\n",
      "    type(value), value, tb, limit=limit).format(chain=chain):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 497, in __init__\n",
      "    capture_locals=capture_locals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 358, in extract\n",
      "    f.line\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 282, in line\n",
      "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 16, in getline\n",
      "    lines = getlines(filename, module_globals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 454, in open\n",
      "    encoding, lines = detect_encoding(buffer.readline)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 423, in detect_encoding\n",
      "    first = read_or_stop()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 381, in read_or_stop\n",
      "    return readline()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2802, in run_ast_nodes\n",
      "    if self.run_code(code, result):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2879, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1821, in showtraceback\n",
      "    print('\\n' + self.get_exception_only(), file=sys.stderr)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1766, in get_exception_only\n",
      "    msg = traceback.format_exception_only(etype, value)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 136, in format_exception_only\n",
      "    return list(TracebackException(etype, value, None).format_exception_only())\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 510, in __init__\n",
      "    self._load_lines()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 522, in _load_lines\n",
      "    self.__context__._load_lines()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 522, in _load_lines\n",
      "    self.__context__._load_lines()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 522, in _load_lines\n",
      "    self.__context__._load_lines()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 520, in _load_lines\n",
      "    frame.line\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 282, in line\n",
      "    self._line = linecache.getline(self.filename, self.lineno).strip()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 16, in getline\n",
      "    lines = getlines(filename, module_globals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 47, in getlines\n",
      "    return updatecache(filename, module_globals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 136, in updatecache\n",
      "    with tokenize.open(fullname) as fp:\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 454, in open\n",
      "    encoding, lines = detect_encoding(buffer.readline)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 423, in detect_encoding\n",
      "    first = read_or_stop()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/tokenize.py\", line 381, in read_or_stop\n",
      "    return readline()\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1806, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 235, in dispatch_shell\n",
      "    handler(stream, idents, msg)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 399, in execute_request\n",
      "    user_expressions, allow_stdin)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 533, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2698, in run_cell\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 1809, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1371, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1279, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/ultratb.py\", line 1140, in structured_traceback\n",
      "    formatted_exceptions += self.prepare_chained_exception_message(evalue.__cause__)\n",
      "TypeError: must be str, not list\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n",
      "    self._handle_recv()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n",
      "    self._run_callback(callback, msg)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n",
      "    callback(*args, **kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/tornado/stack_context.py\", line 277, in null_wrapper\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 283, in dispatcher\n",
      "    return self.dispatch_shell(stream, msg)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 237, in dispatch_shell\n",
      "    self.log.error(\"Exception in message handler:\", exc_info=True)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 1335, in error\n",
      "    self._log(ERROR, msg, args, **kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 1442, in _log\n",
      "    self.handle(record)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 1452, in handle\n",
      "    self.callHandlers(record)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 1514, in callHandlers\n",
      "    hdlr.handle(record)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 863, in handle\n",
      "    self.emit(record)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 992, in emit\n",
      "    msg = self.format(record)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 838, in format\n",
      "    return fmt.format(record)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/traitlets/config/application.py\", line 117, in format\n",
      "    return super(LevelFormatter, self).format(record)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 583, in format\n",
      "    record.exc_text = self.formatException(record.exc_info)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/logging/__init__.py\", line 533, in formatException\n",
      "    traceback.print_exception(ei[0], ei[1], tb, None, sio)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 100, in print_exception\n",
      "    type(value), value, tb, limit=limit).format(chain=chain):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 497, in __init__\n",
      "    capture_locals=capture_locals)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/traceback.py\", line 354, in extract\n",
      "    linecache.checkcache(filename)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/IPython/core/compilerop.py\", line 140, in check_linecache_ipython\n",
      "    linecache._checkcache_ori(*args)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/linecache.py\", line 74, in checkcache\n",
      "    stat = os.stat(fullname)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# iterate over combinations\n",
    "pipeline = 'Pipeline1'\n",
    "for row in combinations_1.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st, cv_wgt = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                                  row[1]['hyperparameter_set'], row[1]['cv_weights']\n",
    "    out = '{}_{}_{}'.format(arch, t_dir, cv_wgt)\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./saved_models/{}/{}/'.format(pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                             --hyperparameter_set=$hyp_st --cv_weights=$cv_wgt \\\n",
    "                             --output_name=$out --pipeline=$pipeline\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"1V\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the models we just trained to get measurements of precision and recall for all positive classes. For every model combination we trained, *validate_sealnet.py* will run a full validation round and write given label/correct label pairs to a .csv file. The resulting .csv file is then imported by an R script, *plot_confusion_matrix.R*, which saves a confusion matrix figure and a .csv spreadsheet with precision and recall for all classes of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validating Resnet18_training_set_vanilla_NO\n",
      "\n",
      "^C\n",
      "\n",
      "Execution halted\n",
      "\n",
      "validating Resnet18_training_set_multiscale_A_NO\n",
      "\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"validate_sealnet.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/__init__.py\", line 284, in <module>\n",
      "    import torch.distributions\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/distributions/__init__.py\", line 75, in <module>\n",
      "    from .chi2 import Chi2\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 951, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 894, in _find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1157, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1129, in _get_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 1271, in find_spec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 96, in _path_isfile\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 91, in _path_is_mode_type\n",
      "KeyboardInterrupt\n",
      "^C\n",
      "\n",
      "Execution halted\n",
      "Resnet18_training_set_vanilla_WCV has not been trained yet\n",
      "Resnet18_training_set_multiscale_A_WCV has not been trained yet\n",
      "\n",
      "validating NasnetA_training_set_vanilla_NO\n",
      "\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"validate_sealnet.py\", line 2, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pandas/__init__.py\", line 23, in <module>\n",
      "    from pandas.compat.numpy import *\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/pandas/compat/__init__.py\", line 361, in <module>\n",
      "    from dateutil import parser as _date_parser\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/dateutil/parser.py\", line 43, in <module>\n",
      "    from . import tz\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/dateutil/tz/__init__.py\", line 1, in <module>\n",
      "    from .tz import *\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/dateutil/tz/tz.py\", line 18, in <module>\n",
      "    from ._common import tzname_in_python2, _tzinfo, _total_seconds\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 779, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 487, in _compile_bytecode\n",
      "KeyboardInterrupt\n",
      "^C\n",
      "\n",
      "Execution halted\n",
      "\n",
      "validating NasnetA_training_set_multiscale_A_NO\n",
      "\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"validate_sealnet.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/__init__.py\", line 78, in <module>\n",
      "    from torch._C import *\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'child' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Vanilla Pexpect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mflush\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions)\u001b[0m\n\u001b[1;32m    196\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    296\u001b[0m         self.ptyproc = self._spawnpty(self.args, env=self.env,\n\u001b[0;32m--> 297\u001b[0;31m                                      cwd=self.cwd, **kwargs)\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0;34m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mptyprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPtyProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ptyprocess/ptyprocess.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_native_pty_fork\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m             \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpty\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/pty.py\u001b[0m in \u001b[0;36mfork\u001b[0;34m()\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mpid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforkpty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a68a7326dfbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;31m# extract performance metrics and plot confusion matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rscript plot_confusion_matrix.R --input_file=$out --pipeline=$pipeline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# accumulate performance scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2195\u001b[0m         \u001b[0;31m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2196\u001b[0m         \u001b[0;31m# Instead, we store the exit_code in user_ns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2197\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msystem_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# (the character is known as ETX for 'End of Text', see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# curses.ascii.ETX).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0;31m# Read and print any more output the program might produce on its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;31m# way out.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'child' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# DataFrame to combine all metrics \n",
    "comb_prec_recall = pd.DataFrame()\n",
    "pipeline = 'Pipeline1'\n",
    "\n",
    "# iterate over trained models\n",
    "for row in combinations_1.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st, cv_wgt = row[1]['training_dir'], row[1]['model_architecture'],\\\n",
    "                                  row[1]['hyperparameter_set'], row[1]['cv_weights']\n",
    "    out = '{}_{}_{}'.format(arch, t_dir, cv_wgt)\n",
    "    \n",
    "    # check if model file is available\n",
    "    if \"{}.tar\".format(out) not in os.listdir('./saved_models/{}/{}/'.format(pipeline, out)): \n",
    "        print('{} has not been trained yet'.format(out))\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        print()\n",
    "        !echo validating $out\n",
    "        print()\n",
    "        \n",
    "        #run validation\n",
    "        !python validate_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                                    --hyperparameter_set=$hyp_st --model_name=$out \\\n",
    "                                    --pipeline=$pipeline\n",
    "        \n",
    "        # extract performance metrics and plot confusion matrix\n",
    "        !Rscript plot_confusion_matrix.R --input_file=$out --pipeline=$pipeline\n",
    "        \n",
    "        # accumulate performance scores\n",
    "        comb_prec_recall = comb_prec_recall.append(pd.read_csv('./saved_models/{}/{}/{}_prec_recall.csv'.format(pipeline, out, out)))\n",
    "    \n",
    "    \n",
    "# Write combined metrics to csv and plot combined metrics\n",
    "pooled_data_path = './saved_models/{}/pooled_prec_recall.csv'.format(pipeline)\n",
    "comb_prec_recall.to_csv(pooled_data_path)\n",
    "!Rscript plot_comparison.R --input_file=$pooled_data_path \\\n",
    "                           --output_file='./saved_models/Pipeline1/comparison_plot.png' \\\n",
    "                           --x='recall' --y='precision' --facet='label'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1.1 - haulout detector + count<a name=\"1.1\"></a>\n",
    "---\n",
    "\n",
    "Here we will generate seal counting CNNs, train them and validate them. Seal counting CNNs will be trained to minimize the mean squared error (MSE) between predicted counts and ground-truth counts. Though they will be trained and validated separately from the haul out detector (Pipeline 1), these approaches will be tested on top of the haul out detector and as standalones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"1.1T\"></a>\n",
    "\n",
    "Similar to the previous pipeline, we will store results in folders (under './saved_models') named after each specific model combination for bookkeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate model combinations\n",
    "combinations_11 = {'model_architecture': ['Resnet18count'] * 2 + ['Resnet34count'] * 2 \\\n",
    "                                         + ['Resnet50count'] * 2 + ['NasnetAcount'] * 2,\n",
    "                   'training_dir': ['training_set_vanilla','training_set_multiscale_A'] * 4,\n",
    "                   'hyperparameter_set': ['A'] * 6 + ['B'] * 2}\n",
    "        \n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_11 = pd.DataFrame(combinations_11)\n",
    "                    \n",
    "\n",
    "# create folders for resulting files\n",
    "for row in combinations_11.iterrows():\n",
    "    mdl = '{}_{}'.format(row[1]['model_architecture'], row[1]['training_dir'])                     \n",
    "    if not os.path.exists(\"./saved_models/Pipeline1.1/{}\".format(mdl)):\n",
    "        os.makedirs(\"./saved_models/Pipeline1.1/{}\".format(mdl)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a counting model, model combinations created above are used as argument to to a new training script, *train_sealnet_count.py*, which uses MSE loss. It accepts the same arguments as the previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resnet18count_training_set_vanilla was already trained\n",
      "Resnet18count_training_set_multiscale_A was already trained\n",
      "\n",
      "training Resnet34count_training_set_vanilla\n",
      "\n",
      "Epoch 1/5\n",
      "----------\n",
      "training Loss: 0.4095\n",
      "validation Loss: 1.4072\n",
      "training time: 0.0h 6m 16s\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n",
      "training Loss: 0.4060\n",
      "validation Loss: 1.3695\n",
      "training time: 0.0h 10m 16s\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n",
      "training Loss: 0.4114\n",
      "validation Loss: 1.4294\n",
      "training time: 0.0h 13m 48s\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n",
      "training Loss: 0.4160\n",
      "validation Loss: 1.3933\n",
      "training time: 0.0h 17m 29s\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n",
      "training Loss: 0.4130\n",
      "validation Loss: 1.3805\n",
      "training time: 0.0h 20m 59s\n",
      "\n",
      "Training complete in 0.0h 20m 59s\n",
      "\n",
      "training Resnet34count_training_set_multiscale_A\n",
      "\n",
      "Epoch 1/5\n",
      "----------\n",
      "training Loss: 0.0921\n",
      "validation Loss: 0.6452\n",
      "training time: 0.0h 6m 25s\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n",
      "training Loss: 0.0910\n",
      "validation Loss: 0.6414\n",
      "training time: 0.0h 10m 60s\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n",
      "training Loss: 0.0921\n",
      "validation Loss: 0.6421\n",
      "training time: 0.0h 15m 6s\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n",
      "training Loss: 0.0851\n",
      "validation Loss: 0.6415\n",
      "training time: 0.0h 18m 60s\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n",
      "training Loss: 0.0894\n",
      "validation Loss: 0.6407\n",
      "training time: 0.0h 22m 54s\n",
      "\n",
      "Training complete in 0.0h 22m 54s\n",
      "Resnet50count_training_set_vanilla was already trained\n",
      "Resnet50count_training_set_multiscale_A was already trained\n",
      "\n",
      "training NasnetAcount_training_set_vanilla\n",
      "\n",
      "Epoch 1/5\n",
      "----------\n",
      "training Loss: 1.5384\n",
      "validation Loss: 7.5351\n",
      "training time: 0.0h 51m 49s\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n",
      "\n",
      "training NasnetAcount_training_set_multiscale_A\n",
      "\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train_sealnet_count.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/__init__.py\", line 12, in <module>\n",
      "    import platform\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/platform.py\", line 116, in <module>\n",
      "    import sys, os, re, subprocess\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/subprocess.py\", line 138, in <module>\n",
      "    import selectors\n",
      "  File \"<frozen importlib._bootstrap>\", line 971, in _find_and_load\n",
      "  File \"<frozen importlib._bootstrap>\", line 955, in _find_and_load_unlocked\n",
      "  File \"<frozen importlib._bootstrap>\", line 665, in _load_unlocked\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 764, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 833, in get_data\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# iterate over combinations\n",
    "pipeline = 'Pipeline1.1'\n",
    "for row in combinations_11.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = '{}_{}'.format(arch, t_dir)\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./saved_models/{}/{}/'.format(pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet_count.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                                   --hyperparameter_set=$hyp_st --output_name=$out  \\\n",
    "                                   --pipeline=$pipeline\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"1.1V\"></a>\n",
    "\n",
    "Validating counting models is a little bit simpler then with seal haul out models: for each model we just extract the mean squared error, running time at inference and number of model parameters. To test if classifying images before counting is helpful, performance stats during counting validation will be later compared to those where images where classified prior to counting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "validating Resnet18count_training_set_vanilla\n",
      "\n",
      "Validation complete in 0.0h 0m 16s\n",
      "\n",
      "validating Resnet18count_training_set_multiscale_A\n",
      "\n",
      "Validation complete in 0.0h 0m 18s\n",
      "\n",
      "validating Resnet34count_training_set_vanilla\n",
      "\n",
      "Validation complete in 0.0h 0m 18s\n",
      "\n",
      "validating Resnet34count_training_set_multiscale_A\n",
      "\n",
      "Validation complete in 0.0h 0m 20s\n",
      "\n",
      "validating Resnet50count_training_set_vanilla\n",
      "\n",
      "Validation complete in 0.0h 0m 26s\n",
      "\n",
      "validating Resnet50count_training_set_multiscale_A\n",
      "\n",
      "Validation complete in 0.0h 0m 26s\n",
      "NasnetAcount_training_set_vanilla has not been trained yet\n",
      "NasnetAcount_training_set_multiscale_A has not been trained yet\n",
      "null device \n",
      "          1 \n",
      "null device \n",
      "          1 \n"
     ]
    }
   ],
   "source": [
    "# DataFrame to combine all metrics \n",
    "comb_mse = pd.DataFrame()\n",
    "pipeline = 'Pipeline1.1'\n",
    "\n",
    "# iterate over trained models\n",
    "for row in combinations_11.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'],\\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = '{}_{}'.format(arch, t_dir)\n",
    "    \n",
    "    # check if model file is available\n",
    "    if \"{}.tar\".format(out) not in os.listdir('./saved_models/{}/{}/'.format(pipeline, out)): \n",
    "        print('{} has not been trained yet'.format(out))\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        print()\n",
    "        !echo validating $out\n",
    "        print()\n",
    "        \n",
    "        #run validation\n",
    "        !python validate_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                                    --hyperparameter_set=$hyp_st --model_name=$out \\\n",
    "                                    --pipeline=$pipeline\n",
    "        \n",
    "        # extract performance metrics and plot confusion matrix\n",
    "        !Rscript get_mse.R --input_file=$out --pipeline=$pipeline\n",
    "        \n",
    "        # accumulate performance scores\n",
    "        comb_mse = comb_mse.append(pd.read_csv('./saved_models/{}/{}/{}_mse.csv'.format(pipeline, out, out)))\n",
    "    \n",
    "    \n",
    "# Write combined metrics to csv and plot combined metrics\n",
    "pooled_data_path = './saved_models/{}/pooled_mse.csv'.format(pipeline)\n",
    "comb_mse.to_csv(pooled_data_path)\n",
    "!Rscript plot_comparison.R --input_file=$pooled_data_path \\\n",
    "                           --output_file='comparison_running_time.png' \\\n",
    "                           --x='running_time' --y='MSE'\n",
    "        \n",
    "!Rscript plot_comparison.R --input_file=$pooled_data_path \\\n",
    "                           --output_file='comparison_n_parameters.png' \\\n",
    "                           --x='n_parameters' --y='MSE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing -- ablation study<a name=\"1.1T\"></a>\n",
    "\n",
    "The idea of this step is to test the validity of the full pipeline (classify + count) against one that will simply count in every patch. To do that we will run *predict_sealnet.py* on our validation images with the best classification model from pipeline 1, keep the names of those that have any seals in it and get the MSE of counting models on that subset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-13-a896de6427d8>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-13-a896de6427d8>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    for path, _, files in os.walk()\u001b[0m\n\u001b[0m                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# classify validation images\n",
    "!python predict_sealnet_ablation.py --pipeline='Pipeline1' \\\n",
    "                                    --model_architecture='Nasnet_A' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1.2 - haulout detector + single seal detector<a name=\"1.2\"></a>\n",
    "---\n",
    "\n",
    "Pipeline 1.2 adds an individual seal detection CNN on top of the seal haul out detector (Pipeline 1. Individual seal detection CNNs will be trained to localize detection points and minimize the MSE between the number of detections and ground-truth count. In this approach, counts will be obtained by adding up the number of detections. Though they will be trained and validated separately from the haul out detector (Pipeline 1), these approaches will be tested on top of the haul out detector and as standalones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"1.2T\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"1.2V\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing -- ablation study<a name=\"1.2T\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation - scene level\n",
    "\n",
    "The first step to validate models at the scene level is to tile out rasters to model input sizes. The following cell will search for all rasters in a dir, check if they are present in the scene_bank (see  *training_set_generation.ipynb*), store an affine matrix for that scene to go from the tile's index in the scene to projected coordinates and create tiles with the correct dimensions(including multi-scale models) for all model architectures in the 'combinations' DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load scene bank\n",
    "scene_bank = pd.read_csv('./training_sets/seals_scene_bank.csv')\n",
    "\n",
    "# point to raster dir\n",
    "raster_dir = '/home/bento/imagery'\n",
    "\n",
    "# create output folder\n",
    "%mkdir './tiled_images'\n",
    "\n",
    "# store data transforms\n",
    "affine_transforms = {}\n",
    "\n",
    "# loop through rasters creating tiled versions\n",
    "print('\\nTiling rasters:\\n')\n",
    "for path, _, files in os.walk(raster_dir):\n",
    "    # filter rasters to include only ones present in the scene bank\n",
    "    files = [file for file in files if file in pd.unique(scene_bank['scene'])]\n",
    "    for count, filename in enumerate(files):\n",
    "        filename_lower = filename.lower()\n",
    "        input_path = (os.path.join(path, filename))\n",
    "        # extract data transform to go from raster index to ESPG3031 coordinates\n",
    "        with rasterio.open(input_path) as src:\n",
    "            affine_transforms[filename] = [src.transform[1], src.transform[2], src.transform[0], src.transform[4], src.transform[5], src.transform[3]]\n",
    "        # loop over models\n",
    "        for row in combinations_haul.iterrows():\n",
    "            # get model input size\n",
    "            input_size = model_archs[row[1]['model_architecture']]\n",
    "            ts_scales = training_sets[row[1]['training_dir']]['scale_bands']\n",
    "            scales = [str(int(input_size * scale / ts_scales[0])) for scale in ts_scales]\n",
    "            scale_bands = reduce(lambda x,y: x + '_' + y, scales)\n",
    "            output_folder = './tiled_images/{}/{}'.format(input_path, scale_bands)\n",
    "            # create a tiled out image for that model if it doesn't exist\n",
    "            if os.path.exists(output_folder):\n",
    "                print('  {} was already split into tiles'.format(filename))\n",
    "                continue\n",
    "            !python tile_raster.py --scale_bands=$scale_bands  --input_image=$input_path --output_folder='./tiled_images'\n",
    "        print('\\nProcessed {} out of {} rasters'.format(count + 1, len(files)))\n",
    "\n",
    "# write data transforms to csv\n",
    "affine_transforms = pd.DataFrame(affine_transforms)\n",
    "affine_transforms.to_csv('affine_transforms.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With rasters tiled to the correct dimensions we can now run validation at the scene level. This procedure will work as follows: for each model, classify all images tiled in the previous step and save classification results to a pandas DataFrame. Results will be compared with the scene_bank to measure precision and recall at detecting which scenes contain positive entries (i.e. seals, penguins or even both, depending on the scene_bank being used). Locations marked as having seal haulouts will be used to detect individual seals and validate models at the seal level. To get validation results, run the following cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifying tiles:\n",
      "\n",
      "  Tiles were already processed with model1\n",
      "  Tiles were already processed with model2\n",
      "  Tiles were already processed with model3\n",
      "  Tiles were already processed with model4\n",
      "  Tiles were already processed with model5\n",
      "  Tiles were already processed with model6\n",
      "  Tiles were already processed with model7\n",
      "  Tiles were already processed with model8\n",
      "\n",
      "Validating models:\n",
      "\n",
      "  Validated 1 out of 8 models\n",
      "  Validated 2 out of 8 models\n",
      "  Validated 3 out of 8 models\n",
      "  Validated 4 out of 8 models\n",
      "  Validated 5 out of 8 models\n",
      "  Validated 6 out of 8 models\n",
      "  Validated 7 out of 8 models\n",
      "  Validated 8 out of 8 models\n",
      "Loading required package: methods\n",
      "null device \n",
      "          1 \n"
     ]
    }
   ],
   "source": [
    "# loop through models classifying tiles\n",
    "print('\\nClassifying tiles:\\n')\n",
    "for row in combinations_haul.iterrows():\n",
    "    # store model classifications to get seal locations for seal level validation\n",
    "    val_scene_model = pd.DataFrame()\n",
    "    \n",
    "    # get model settings\n",
    "    arch = row[1]['model_architecture']\n",
    "    model_name = row[1]['output_name']\n",
    "    t_dir = row[1]['training_dir']\n",
    "    hyp_st = row[1]['hyperparameter_set']\n",
    "    \n",
    "    # check if model was already processed\n",
    "    if \"{}_scene_val.csv\".format(model_name) in os.listdir('./saved_models/haulout/{}/'.format(model_name)): \n",
    "        print('  Tiles were already processed with {}'.format(model_name))\n",
    "        continue\n",
    "    \n",
    "    # get scale bands\n",
    "    input_size = model_archs[arch]['input_size']\n",
    "    ts_scales = training_sets[t_dir]['scale_bands']\n",
    "    scales = [str(int(input_size * scale / ts_scales[0])) for scale in ts_scales]\n",
    "    scale_bands = reduce(lambda x,y: x + '_' + y, scales)\n",
    "    \n",
    "    # loop through tiled rasters\n",
    "    scenes = os.listdir('./tiled_images')\n",
    "    print('\\nClassifying tiled scenes with {}:'.format(model_name))\n",
    "    for count, scene in enumerate(scenes):\n",
    "        input_folder = './tiled_images/{}/{}'.format(scene, scale_bands)\n",
    "        %run predict_sealnet.py --training_dir=$t_dir --model_architecture=$arch --hyperparameter_set=$hyp_st --model_name=$model_name --data_dir=$input_folder --affine_transforms='affine_transforms.csv' --scene=$scene        \n",
    "        val_scene_model = val_scene_model.append(pd.read_csv('./saved_models/haulout/{}/{}_scene_val_tmp.csv'.format(model_name, model_name)), ignore_index=True)\n",
    "        print('\\n  Classified {} out of {} tiled scenes'.format(count + 1, len(scenes)))\n",
    "    \n",
    "    # save final classifications to csv and cleanup temporary stats\n",
    "    val_scene_model.to_csv('./saved_models/haulout/{}/{}_scene_val.csv'.format(model_name, model_name))\n",
    "    to_rm = './saved_models/haulout/{}/{}_scene_val_tmp.csv'.format(model_name, model_name)\n",
    "    !rm $to_rm\n",
    "\n",
    "# get precision and recall for classification at all scene banks\n",
    "comb_prec_rec = pd.DataFrame()\n",
    "\n",
    "print('\\nValidating models:\\n')\n",
    "# loop thorugh models again to get precision and recall\n",
    "for count, row in enumerate(combinations_haul.iterrows()):\n",
    "    # get model name\n",
    "    model_name = row[1]['output_name']\n",
    "    # get validation stats\n",
    "    %run validate_sealnet_scene.py --model_name=$model_name\n",
    "    comb_prec_rec = comb_prec_rec.append(pd.read_csv('./saved_models/haulout/{}/{}_scene_prec_recall.csv'.format(model_name, model_name)), ignore_index=True)\n",
    "    print('  Validated {} out of {} models'.format(count + 1, len(combinations_haul)))\n",
    "    \n",
    "# save combined precision recall to csv and plot it\n",
    "comb_prec_rec.to_csv('./saved_models/haulout/pooled_scene_prec_recall.csv', index=False)\n",
    "!Rscript plot_comparison.R './saved_models/haulout/pooled_scene_prec_recall.csv' './saved_models/haulout/scene_comparison_plot.png'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation -- single seal level\n",
    "\n",
    "With seal haulout locations determined, we can go ahead and try to count seals inside flagged seal haul out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "----------\n",
      "training Loss: 2.1029\n",
      "validation Loss: 9.4041\n",
      "training time: 0.0h 37m 56s\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n",
      "training Loss: 2.1184\n",
      "validation Loss: 9.4700\n",
      "training time: 1.0h 15m 25s\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n",
      "training Loss: 2.0883\n",
      "validation Loss: 9.3499\n",
      "training time: 1.0h 53m 40s\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n",
      "training Loss: 2.0652\n",
      "validation Loss: 9.5114\n",
      "training time: 2.0h 33m 18s\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-21:\n",
      "Process Process-23:\n",
      "Process Process-24:\n",
      "Process Process-22:\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/train_sealnet_count.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 245\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/train_sealnet_count.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;31m# start training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     model_ft = train_model(model, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0;32m--> 241\u001b[0;31m                            num_epochs=hyperparameters[args.hyperparameter_set]['epochs'])\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/train_sealnet_count.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m    159\u001b[0m                 \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/custom_architectures/nasnet_scalable_count.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/custom_architectures/nasnet_scalable_count.py\u001b[0m in \u001b[0;36mfeatures\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    549\u001b[0m         \u001b[0mx_cell_11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_11\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cell_10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cell_9\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m         \u001b[0mx_reduction_cell_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduction_cell_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_cell_11\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cell_10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mx_cell_12\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell_12\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_reduction_cell_1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_cell_10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/custom_architectures/nasnet_scalable_count.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, x_prev)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mx_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_1x1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0mx_comb_iter_0_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomb_iter_0_left\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_right\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0mx_comb_iter_0_right\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcomb_iter_0_right\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         \u001b[0mx_comb_iter_0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_comb_iter_0_left\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx_comb_iter_0_right\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/custom_architectures/nasnet_scalable_count.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseparable_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn_sep_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/batchnorm.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     47\u001b[0m         return F.batch_norm(\n\u001b[1;32m     48\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             self.training or not self.track_running_stats, self.momentum, self.eps)\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   1192\u001b[0m     return torch.batch_norm(\n\u001b[1;32m   1193\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_mean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrunning_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1194\u001b[0;31m         \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmomentum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m     )\n\u001b[1;32m   1196\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/bento/PycharmProjects/Seals_branches/Master/Seals/utils/data_loader_train_det.py\", line 110, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/bento/PycharmProjects/Seals_branches/Master/Seals/utils/data_loader_train_det.py\", line 167, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/bento/PycharmProjects/Seals_branches/Master/Seals/utils/data_loader_train_det.py\", line 149, in pil_loader\n",
      "    img = Image.open(f)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 2552, in open\n",
      "    prefix = fp.read(16)\n",
      "KeyboardInterrupt\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "%run train_sealnet_count.py --training_dir=\"training_set_vanilla_count\" --model_architecture='NasnetACount' --hyperparameter_set='B' --cv_weights='NO' --output_name='count-ception'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n",
      "training Loss: 2.2607 Acc: 0.7001\n",
      "validation Loss: 14.8348 Acc: 0.6339\n",
      "training time: 1.0h 54m 43s\n",
      "\n",
      "Epoch 2/10\n",
      "----------\n",
      "training Loss: 2.2659 Acc: 0.8595\n",
      "validation Loss: 18.3803 Acc: 0.5907\n",
      "training time: 3.0h 49m 29s\n",
      "\n",
      "Epoch 3/10\n",
      "----------\n",
      "training Loss: 2.1439 Acc: 0.8928\n",
      "validation Loss: 22.7197 Acc: 0.5870\n",
      "training time: 5.0h 44m 12s\n",
      "\n",
      "Epoch 4/10\n",
      "----------\n",
      "training Loss: 2.2004 Acc: 0.9147\n",
      "validation Loss: 22.0916 Acc: 0.5890\n",
      "training time: 7.0h 38m 49s\n",
      "\n",
      "Epoch 5/10\n",
      "----------\n",
      "training Loss: 2.2068 Acc: 0.9235\n",
      "validation Loss: 20.3537 Acc: 0.5889\n",
      "training time: 9.0h 33m 22s\n",
      "\n",
      "Epoch 6/10\n",
      "----------\n",
      "training Loss: 2.1304 Acc: 0.9363\n",
      "validation Loss: 22.2404 Acc: 0.5895\n",
      "training time: 11.0h 27m 54s\n",
      "\n",
      "Epoch 7/10\n",
      "----------\n",
      "training Loss: 2.1302 Acc: 0.9412\n",
      "validation Loss: 21.8812 Acc: 0.5890\n",
      "training time: 13.0h 22m 26s\n",
      "\n",
      "Epoch 8/10\n",
      "----------\n",
      "training Loss: 2.0879 Acc: 0.9478\n",
      "validation Loss: 25.9171 Acc: 0.5904\n",
      "training time: 15.0h 17m 2s\n",
      "\n",
      "Epoch 9/10\n",
      "----------\n",
      "training Loss: 2.1310 Acc: 0.9510\n",
      "validation Loss: 21.6361 Acc: 0.5889\n",
      "training time: 17.0h 14m 34s\n",
      "\n",
      "Epoch 10/10\n",
      "----------\n",
      "training Loss: 2.1503 Acc: 0.9538\n",
      "validation Loss: 22.4288 Acc: 0.5892\n",
      "training time: 19.0h 9m 48s\n",
      "\n",
      "Training complete in 19.0h 9m 48s\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'saved_models/single_seal/NasnetAe2e_V/NasnetAe2e_V.tar'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/train_sealnet_count_e2e.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/train_sealnet_count_e2e.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    260\u001b[0m     train_model(model, optimizer_ft, exp_lr_scheduler, criterion_class=criterion_class,\n\u001b[1;32m    261\u001b[0m                 \u001b[0mcriterion_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 num_epochs=hyperparameters[args.hyperparameter_set]['epochs'])\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/train_sealnet_count_e2e.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, criterion_class, num_epochs, criterion_count)\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saved_models/haulout/{}/{}.tar'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'saved_models/single_seal/{}/{}.tar'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m     \"\"\"\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_with_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_with_file_like\u001b[0;34m(f, mode, body)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbody\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'saved_models/single_seal/NasnetAe2e_V/NasnetAe2e_V.tar'"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-54:\n",
      "Process Process-51:\n",
      "Process Process-53:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Process Process-52:\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 52, in _worker_loop\n",
      "    r = index_queue.get()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/queues.py\", line 335, in get\n",
      "    res = self._reader.recv_bytes()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 216, in recv_bytes\n",
      "    buf = self._recv_bytes(maxlength)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 407, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/train_sealnet_count_e2e.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/train_sealnet_count_e2e.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    260\u001b[0m     train_model(model, optimizer_ft, exp_lr_scheduler, criterion_class=criterion_class,\n\u001b[1;32m    261\u001b[0m                 \u001b[0mcriterion_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion_count\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m                 num_epochs=hyperparameters[args.hyperparameter_set]['epochs'])\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/train_sealnet_count_e2e.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, scheduler, criterion_class, num_epochs, criterion_count)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;31m# forward\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m                 \u001b[0mout_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m                 \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m                 \u001b[0mout_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mele\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mele\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mout_count\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/custom_architectures/nasnet_scalable_e2e.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    587\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 589\u001b[0;31m         \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    590\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/custom_architectures/nasnet_scalable_e2e.py\u001b[0m in \u001b[0;36mlogits\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0mx1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_linear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/modules/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mdropout\u001b[0;34m(input, p, training, inplace)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/dropout.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(cls, ctx, input, p, train, inplace)\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m             \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbernoulli_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m         \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/bento/PycharmProjects/Seals_branches/Master/Seals/utils/data_loader_train_det.py\", line 118, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/bento/PycharmProjects/Seals_branches/Master/Seals/utils/data_loader_train_det.py\", line 167, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/bento/PycharmProjects/Seals_branches/Master/Seals/utils/data_loader_train_det.py\", line 150, in pil_loader\n",
      "    return img.convert('RGB')\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 877, in convert\n",
      "    self.load()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/PIL/ImageFile.py\", line 236, in load\n",
      "    n, err_code = decoder.decode(b)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "%run train_sealnet_count_e2e.py --training_dir=\"training_set_vanilla_count\" --model_architecture='NasnetAe2e' --hyperparameter_set='E' --cv_weights='NO' --output_name='NasnetAe2e_V'\n",
    "%run train_sealnet_count_e2e.py --training_dir=\"training_set_multiscale_A_count\" --model_architecture='NasnetAe2e' --hyperparameter_set='E' --cv_weights='NO' --output_name='NasnetAe2e_MS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'class_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-e9d06a47c3d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mkdir classified_images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclass_names\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./classified_images/'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mkdir -v $dir'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'class_names' is not defined"
     ]
    }
   ],
   "source": [
    "%mkdir classified_images\n",
    "for x in class_names:\n",
    "    dir = './classified_images/' + x\n",
    "    %mkdir -v $dir\n",
    "    \n",
    "%run predict_sealnet.py \"training_set_vanilla\" \"NasnetA\" \"model5\" \"./to_classify\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: './saved_models/haulout/pooled_haul_prec_recall.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/New_master/Seals/rename_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/New_master/Seals/rename_model.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mcsv_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfilename\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcsv_files\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: './saved_models/haulout/pooled_haul_prec_recall.csv'"
     ]
    }
   ],
   "source": [
    "path = './saved_models/haulout'\n",
    "for folder in os.listdir(path):\n",
    "    folder_path = os.path.join(path, folder)\n",
    "    %run rename_model.py --folder=$folder_path --target_name=$folder\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
