{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model search\n",
    "\n",
    "## Getting started\n",
    "\n",
    "This script will go through assembling the main components of a complete pipeline for counting seals in high-resolution satellite imagery. \n",
    "\n",
    "<img src=\"jupyter_notebook_images/pipeline.png\">\n",
    "** *environmental covariates not currently incorporated **\n",
    "\n",
    "If you followed the *training_set_generation* jupyter notebook (also present in this repo), you should have training sets generated and hyperparameter sets to try out, and be ready to search for a best performing model setup. Models will be evaluated at the three levels of the detection pipeline: **1)** their ability to identify scenes with seals, **2)** retrieve seal haul outs **3)** and count individual seals. Before training and validating model/hyperparameter combinations, we need to load the required python modules. Running this script will also display a list of training classes. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['crabeater', 'crack', 'emperor', 'glacier', 'ice-sheet', 'marching-emperor', 'open-water', 'other', 'pack-ice', 'rock', 'weddell']\n"
     ]
    }
   ],
   "source": [
    "# import required packages\n",
    "import os\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from functools import reduce\n",
    "from model_library import * \n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi']= 400\n",
    "\n",
    "\n",
    "# display class names\n",
    "class_names = sorted([subdir for subdir in os.listdir('./training_sets/training_set_vanilla/training')])\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing training images (Optional)\n",
    "\n",
    "To get a better sense for what the training set is like, the next cell will display a few random images from the training classes. Displayed images are extracted from a pool of ~70000 training images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store images\n",
    "images = []\n",
    "\n",
    "# loop over labels\n",
    "for label in class_names:\n",
    "    for path, _, files in os.walk('./training_sets/training_set_vanilla/training/{}'.format(label)):\n",
    "        files = np.random.choice(files, 5)\n",
    "        for filename in files:\n",
    "            images.append(np.asarray(Image.open(os.path.join(path, filename))))\n",
    "\n",
    "images = np.array(images)\n",
    "\n",
    "# display images \n",
    "ncols=len(class_names)\n",
    "nindex, height, width, intensity = images.shape\n",
    "nrows = nindex//ncols\n",
    "assert nindex == nrows*ncols\n",
    "result = (images.reshape(nrows, ncols, height, width, intensity)\n",
    "          .swapaxes(1,2)\n",
    "          .reshape(height*nrows, width*ncols, intensity))\n",
    "\n",
    "plt.imshow(result)\n",
    "cur_axes = plt.gca()\n",
    "cur_axes.axes.get_xaxis().set_visible(False)\n",
    "cur_axes.axes.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - haulout detector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step to find a best performing model is to train different model setups using our training set. To keep track of which combinations we have tried and the specifics of each model setup, we will create a pandas DataFrame with model definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate model combinations\n",
    "combinations_haul = {'model_architecture': ['Resnet18'] * 4 + ['NasnetA'] * 4,\n",
    "                     'training_dir': ['training_set_vanilla', 'training_set_multiscale_A'] * 4,\n",
    "                     'hyperparameter_set': ['A'] * 4 + ['B'] * 4,\n",
    "                     'cv_weights': ['NO', 'NO', 'A', 'A'] * 2,\n",
    "                     'output_name': ['model{}'.format(i) for i in range(1,9)]}\n",
    "\n",
    "combinations_haul = pd.DataFrame(combinations_haul)\n",
    "\n",
    "# create folders for resulting files\n",
    "for mdl in combinations_haul['output_name']:\n",
    "    if not os.path.exists(\"./saved_models/haulout/{}\".format(mdl)):\n",
    "        os.makedirs(\"./saved_models/haulout/{}\".format(mdl)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then provide model combinations created above as arguments to the training script, *train_sealnet.py*. A list of required arguments can be displayed by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run train_sealnet.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_haul.iterrows():\n",
    "    t_dir, arch, hyp_st, cv_wgt, out = row[1]['training_dir'], row[1]['model_architecture'], row[1]['hyperparameter_set'], row[1]['cv_weights'], row[1]['output_name']\n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./saved_models/haulout/{}/'.format(out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch --hyperparameter_set=$hyp_st --cv_weights=$cv_wgt --output_name=$out\n",
    "    \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training - single seal detector\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to training a model at the haulout level, we start by training a number of model combinations. To keep track of which combinations we have tried and the specifics of each model setup, we will create a pandas DataFrame with model definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate model combinations\n",
    "combinations_single = {'model_architecture': ['WideResnetA'] * 4,\n",
    "                       'training_dir': ['training_set_vanilla', 'training_set_multiscale_A'] * 2,\n",
    "                       'hyperparameter_set': ['C'] * 4,\n",
    "                       'cv_weights': ['NO', 'NO', 'A', 'A'],\n",
    "                       'output_name': ['model{}'.format(i) for i in range(1,5)]}\n",
    "\n",
    "combinations_single = pd.DataFrame(combinations_single)\n",
    "\n",
    "# create folders for resulting files\n",
    "for mdl in combinations_single['output_name']:\n",
    "    if not os.path.exists(\"./saved_models/single_seal/{}\".format(mdl)):\n",
    "        os.makedirs(\"./saved_models/single_seal/{}\".format(mdl)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the haulout models, we can provide model combinations created above as arguments to the training script, *train_sealnet.py*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training model1\n",
      "\n",
      "Epoch 1/30\n",
      "----------\n",
      "training Loss: 0.0616 Acc: 0.5316\n",
      "validation Loss: 0.3756 Acc: 0.6643\n",
      "training time: 0.0h 1m 23s\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "training Loss: 0.0499 Acc: 0.6131\n",
      "validation Loss: 0.3693 Acc: 0.6852\n",
      "training time: 0.0h 2m 46s\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "training Loss: 0.0463 Acc: 0.6395\n",
      "validation Loss: 0.3615 Acc: 0.6668\n",
      "training time: 0.0h 4m 9s\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n",
      "training Loss: 0.0448 Acc: 0.6495\n",
      "validation Loss: 0.3048 Acc: 0.7343\n",
      "training time: 0.0h 5m 33s\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "training Loss: 0.0433 Acc: 0.6628\n",
      "validation Loss: 0.3437 Acc: 0.6872\n",
      "training time: 0.0h 6m 57s\n",
      "\n",
      "Epoch 6/30\n",
      "----------\n",
      "training Loss: 0.0416 Acc: 0.6740\n",
      "validation Loss: 0.3373 Acc: 0.6710\n",
      "training time: 0.0h 8m 20s\n",
      "\n",
      "Epoch 7/30\n",
      "----------\n",
      "training Loss: 0.0411 Acc: 0.6803\n",
      "validation Loss: 0.3202 Acc: 0.6830\n",
      "training time: 0.0h 9m 43s\n",
      "\n",
      "Epoch 8/30\n",
      "----------\n",
      "training Loss: 0.0401 Acc: 0.6873\n",
      "validation Loss: 0.3305 Acc: 0.6888\n",
      "training time: 0.0h 11m 8s\n",
      "\n",
      "Epoch 9/30\n",
      "----------\n",
      "training Loss: 0.0396 Acc: 0.6908\n",
      "validation Loss: 0.3059 Acc: 0.7019\n",
      "training time: 0.0h 12m 32s\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "training Loss: 0.0394 Acc: 0.6928\n",
      "validation Loss: 0.2970 Acc: 0.7196\n",
      "training time: 0.0h 13m 55s\n",
      "\n",
      "Epoch 11/30\n",
      "----------\n",
      "training Loss: 0.0387 Acc: 0.6988\n",
      "validation Loss: 0.2890 Acc: 0.7172\n",
      "training time: 0.0h 15m 19s\n",
      "\n",
      "Epoch 12/30\n",
      "----------\n",
      "training Loss: 0.0379 Acc: 0.7037\n",
      "validation Loss: 0.2856 Acc: 0.7356\n",
      "training time: 0.0h 16m 42s\n",
      "\n",
      "Epoch 13/30\n",
      "----------\n",
      "training Loss: 0.0375 Acc: 0.7090\n",
      "validation Loss: 0.2779 Acc: 0.7460\n",
      "training time: 0.0h 18m 5s\n",
      "\n",
      "Epoch 14/30\n",
      "----------\n",
      "training Loss: 0.0370 Acc: 0.7109\n",
      "validation Loss: 0.2546 Acc: 0.7822\n",
      "training time: 0.0h 19m 28s\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "training Loss: 0.0365 Acc: 0.7128\n",
      "validation Loss: 0.2612 Acc: 0.7580\n",
      "training time: 0.0h 20m 51s\n",
      "\n",
      "Epoch 16/30\n",
      "----------\n",
      "training Loss: 0.0365 Acc: 0.7145\n",
      "validation Loss: 0.2644 Acc: 0.7516\n",
      "training time: 0.0h 22m 15s\n",
      "\n",
      "Epoch 17/30\n",
      "----------\n",
      "training Loss: 0.0362 Acc: 0.7165\n",
      "validation Loss: 0.2796 Acc: 0.7143\n",
      "training time: 0.0h 23m 37s\n",
      "\n",
      "Epoch 18/30\n",
      "----------\n",
      "training Loss: 0.0357 Acc: 0.7211\n",
      "validation Loss: 0.2671 Acc: 0.7454\n",
      "training time: 0.0h 25m 2s\n",
      "\n",
      "Epoch 19/30\n",
      "----------\n",
      "training Loss: 0.0354 Acc: 0.7226\n",
      "validation Loss: 0.2366 Acc: 0.7910\n",
      "training time: 0.0h 26m 25s\n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "training Loss: 0.0353 Acc: 0.7242\n",
      "validation Loss: 0.2508 Acc: 0.7687\n",
      "training time: 0.0h 27m 48s\n",
      "\n",
      "Epoch 21/30\n",
      "----------\n",
      "training Loss: 0.0348 Acc: 0.7280\n",
      "validation Loss: 0.2627 Acc: 0.7439\n",
      "training time: 0.0h 29m 12s\n",
      "\n",
      "Epoch 22/30\n",
      "----------\n",
      "training Loss: 0.0347 Acc: 0.7282\n",
      "validation Loss: 0.2643 Acc: 0.7625\n",
      "training time: 0.0h 30m 36s\n",
      "\n",
      "Epoch 23/30\n",
      "----------\n",
      "training Loss: 0.0341 Acc: 0.7348\n",
      "validation Loss: 0.2488 Acc: 0.7820\n",
      "training time: 0.0h 31m 59s\n",
      "\n",
      "Epoch 24/30\n",
      "----------\n",
      "training Loss: 0.0339 Acc: 0.7357\n",
      "validation Loss: 0.2398 Acc: 0.7840\n",
      "training time: 0.0h 33m 22s\n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "training Loss: 0.0337 Acc: 0.7343\n",
      "validation Loss: 0.2363 Acc: 0.7920\n",
      "training time: 0.0h 34m 45s\n",
      "\n",
      "Epoch 26/30\n",
      "----------\n",
      "training Loss: 0.0332 Acc: 0.7404\n",
      "validation Loss: 0.2464 Acc: 0.7769\n",
      "training time: 0.0h 36m 11s\n",
      "\n",
      "Epoch 27/30\n",
      "----------\n",
      "training Loss: 0.0335 Acc: 0.7383\n",
      "validation Loss: 0.2461 Acc: 0.7785\n",
      "training time: 0.0h 37m 34s\n",
      "\n",
      "Epoch 28/30\n",
      "----------\n",
      "training Loss: 0.0334 Acc: 0.7390\n",
      "validation Loss: 0.2543 Acc: 0.7642\n",
      "training time: 0.0h 38m 57s\n",
      "\n",
      "Epoch 29/30\n",
      "----------\n",
      "training Loss: 0.0330 Acc: 0.7421\n",
      "validation Loss: 0.2279 Acc: 0.8003\n",
      "training time: 0.0h 40m 20s\n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "training Loss: 0.0333 Acc: 0.7404\n",
      "validation Loss: 0.2335 Acc: 0.7924\n",
      "training time: 0.0h 41m 44s\n",
      "\n",
      "Training complete in 0.0h 41m 44s\n",
      "\n",
      "training model2\n",
      "\n",
      "Epoch 1/30\n",
      "----------\n",
      "training Loss: 0.0549 Acc: 0.5850\n",
      "validation Loss: 0.2673 Acc: 0.7877\n",
      "training time: 0.0h 5m 54s\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "training Loss: 0.0405 Acc: 0.7038\n",
      "validation Loss: 0.2601 Acc: 0.7887\n",
      "training time: 0.0h 8m 28s\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "training Loss: 0.0365 Acc: 0.7343\n",
      "validation Loss: 0.2398 Acc: 0.7997\n",
      "training time: 0.0h 10m 12s\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n",
      "training Loss: 0.0336 Acc: 0.7557\n",
      "validation Loss: 0.3002 Acc: 0.7088\n",
      "training time: 0.0h 11m 42s\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "training Loss: 0.0315 Acc: 0.7704\n",
      "validation Loss: 0.2407 Acc: 0.7820\n",
      "training time: 0.0h 13m 11s\n",
      "\n",
      "Epoch 6/30\n",
      "----------\n",
      "training Loss: 0.0301 Acc: 0.7820\n",
      "validation Loss: 0.2055 Acc: 0.8238\n",
      "training time: 0.0h 14m 38s\n",
      "\n",
      "Epoch 7/30\n",
      "----------\n",
      "training Loss: 0.0284 Acc: 0.7956\n",
      "validation Loss: 0.1924 Acc: 0.8379\n",
      "training time: 0.0h 16m 4s\n",
      "\n",
      "Epoch 8/30\n",
      "----------\n",
      "training Loss: 0.0280 Acc: 0.7967\n",
      "validation Loss: 0.1879 Acc: 0.8343\n",
      "training time: 0.0h 17m 30s\n",
      "\n",
      "Epoch 9/30\n",
      "----------\n",
      "training Loss: 0.0269 Acc: 0.8047\n",
      "validation Loss: 0.1891 Acc: 0.8335\n",
      "training time: 0.0h 18m 57s\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "training Loss: 0.0260 Acc: 0.8113\n",
      "validation Loss: 0.1800 Acc: 0.8375\n",
      "training time: 0.0h 20m 24s\n",
      "\n",
      "Epoch 11/30\n",
      "----------\n",
      "training Loss: 0.0253 Acc: 0.8167\n",
      "validation Loss: 0.1658 Acc: 0.8612\n",
      "training time: 0.0h 21m 50s\n",
      "\n",
      "Epoch 12/30\n",
      "----------\n",
      "training Loss: 0.0245 Acc: 0.8230\n",
      "validation Loss: 0.1613 Acc: 0.8651\n",
      "training time: 0.0h 23m 19s\n",
      "\n",
      "Epoch 13/30\n",
      "----------\n",
      "training Loss: 0.0244 Acc: 0.8226\n",
      "validation Loss: 0.1388 Acc: 0.8821\n",
      "training time: 0.0h 24m 46s\n",
      "\n",
      "Epoch 14/30\n",
      "----------\n",
      "training Loss: 0.0233 Acc: 0.8312\n",
      "validation Loss: 0.1574 Acc: 0.8664\n",
      "training time: 0.0h 26m 14s\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "training Loss: 0.0234 Acc: 0.8296\n",
      "validation Loss: 0.1698 Acc: 0.8513\n",
      "training time: 0.0h 27m 40s\n",
      "\n",
      "Epoch 16/30\n",
      "----------\n",
      "training Loss: 0.0227 Acc: 0.8341\n",
      "validation Loss: 0.1567 Acc: 0.8686\n",
      "training time: 0.0h 29m 6s\n",
      "\n",
      "Epoch 17/30\n",
      "----------\n",
      "training Loss: 0.0222 Acc: 0.8388\n",
      "validation Loss: 0.1461 Acc: 0.8731\n",
      "training time: 0.0h 30m 32s\n",
      "\n",
      "Epoch 18/30\n",
      "----------\n",
      "training Loss: 0.0219 Acc: 0.8401\n",
      "validation Loss: 0.1577 Acc: 0.8673\n",
      "training time: 0.0h 32m 2s\n",
      "\n",
      "Epoch 19/30\n",
      "----------\n",
      "training Loss: 0.0217 Acc: 0.8426\n",
      "validation Loss: 0.1411 Acc: 0.8794\n",
      "training time: 0.0h 33m 29s\n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "training Loss: 0.0213 Acc: 0.8459\n",
      "validation Loss: 0.1488 Acc: 0.8721\n",
      "training time: 0.0h 34m 56s\n",
      "\n",
      "Epoch 21/30\n",
      "----------\n",
      "training Loss: 0.0209 Acc: 0.8474\n",
      "validation Loss: 0.1451 Acc: 0.8820\n",
      "training time: 0.0h 36m 23s\n",
      "\n",
      "Epoch 22/30\n",
      "----------\n",
      "training Loss: 0.0208 Acc: 0.8488\n",
      "validation Loss: 0.1413 Acc: 0.8819\n",
      "training time: 0.0h 37m 50s\n",
      "\n",
      "Epoch 23/30\n",
      "----------\n",
      "training Loss: 0.0206 Acc: 0.8497\n",
      "validation Loss: 0.1355 Acc: 0.8881\n",
      "training time: 0.0h 39m 16s\n",
      "\n",
      "Epoch 24/30\n",
      "----------\n",
      "training Loss: 0.0203 Acc: 0.8528\n",
      "validation Loss: 0.1551 Acc: 0.8673\n",
      "training time: 0.0h 40m 43s\n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "training Loss: 0.0201 Acc: 0.8545\n",
      "validation Loss: 0.1391 Acc: 0.8843\n",
      "training time: 0.0h 42m 11s\n",
      "\n",
      "Epoch 26/30\n",
      "----------\n",
      "training Loss: 0.0196 Acc: 0.8577\n",
      "validation Loss: 0.1381 Acc: 0.8856\n",
      "training time: 0.0h 43m 38s\n",
      "\n",
      "Epoch 27/30\n",
      "----------\n",
      "training Loss: 0.0193 Acc: 0.8606\n",
      "validation Loss: 0.1407 Acc: 0.8807\n",
      "training time: 0.0h 45m 4s\n",
      "\n",
      "Epoch 28/30\n",
      "----------\n",
      "training Loss: 0.0196 Acc: 0.8572\n",
      "validation Loss: 0.1448 Acc: 0.8783\n",
      "training time: 0.0h 46m 31s\n",
      "\n",
      "Epoch 29/30\n",
      "----------\n",
      "training Loss: 0.0192 Acc: 0.8594\n",
      "validation Loss: 0.1345 Acc: 0.8899\n",
      "training time: 0.0h 47m 57s\n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "training Loss: 0.0188 Acc: 0.8610\n",
      "validation Loss: 0.1240 Acc: 0.8956\n",
      "training time: 0.0h 49m 25s\n",
      "\n",
      "Training complete in 0.0h 49m 25s\n",
      "\n",
      "training model3\n",
      "\n",
      "Epoch 1/30\n",
      "----------\n",
      "training Loss: 0.0490 Acc: 0.4917\n",
      "validation Loss: 0.5790 Acc: 0.5123\n",
      "training time: 0.0h 4m 40s\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "training Loss: 0.0390 Acc: 0.5835\n",
      "validation Loss: 0.3880 Acc: 0.6726\n",
      "training time: 0.0h 6m 42s\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "training Loss: 0.0360 Acc: 0.6140\n",
      "validation Loss: 0.3715 Acc: 0.6640\n",
      "training time: 0.0h 8m 11s\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n",
      "training Loss: 0.0340 Acc: 0.6322\n",
      "validation Loss: 0.4376 Acc: 0.5242\n",
      "training time: 0.0h 9m 36s\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "training Loss: 0.0333 Acc: 0.6388\n",
      "validation Loss: 0.3456 Acc: 0.6930\n",
      "training time: 0.0h 10m 59s\n",
      "\n",
      "Epoch 6/30\n",
      "----------\n",
      "training Loss: 0.0320 Acc: 0.6502\n",
      "validation Loss: 0.3677 Acc: 0.6594\n",
      "training time: 0.0h 12m 24s\n",
      "\n",
      "Epoch 7/30\n",
      "----------\n",
      "training Loss: 0.0317 Acc: 0.6531\n",
      "validation Loss: 0.3228 Acc: 0.7163\n",
      "training time: 0.0h 13m 48s\n",
      "\n",
      "Epoch 8/30\n",
      "----------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training Loss: 0.0308 Acc: 0.6618\n",
      "validation Loss: 0.3233 Acc: 0.7151\n",
      "training time: 0.0h 15m 14s\n",
      "\n",
      "Epoch 9/30\n",
      "----------\n",
      "training Loss: 0.0301 Acc: 0.6694\n",
      "validation Loss: 0.3263 Acc: 0.7263\n",
      "training time: 0.0h 16m 37s\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "training Loss: 0.0294 Acc: 0.6741\n",
      "validation Loss: 0.3228 Acc: 0.7014\n",
      "training time: 0.0h 18m 3s\n",
      "\n",
      "Epoch 11/30\n",
      "----------\n",
      "training Loss: 0.0291 Acc: 0.6782\n",
      "validation Loss: 0.2781 Acc: 0.7544\n",
      "training time: 0.0h 19m 28s\n",
      "\n",
      "Epoch 12/30\n",
      "----------\n",
      "training Loss: 0.0288 Acc: 0.6818\n",
      "validation Loss: 0.2853 Acc: 0.7460\n",
      "training time: 0.0h 20m 52s\n",
      "\n",
      "Epoch 13/30\n",
      "----------\n",
      "training Loss: 0.0278 Acc: 0.6896\n",
      "validation Loss: 0.2957 Acc: 0.7442\n",
      "training time: 0.0h 22m 16s\n",
      "\n",
      "Epoch 14/30\n",
      "----------\n",
      "training Loss: 0.0277 Acc: 0.6911\n",
      "validation Loss: 0.3169 Acc: 0.6914\n",
      "training time: 0.0h 23m 39s\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "training Loss: 0.0273 Acc: 0.6934\n",
      "validation Loss: 0.2971 Acc: 0.7174\n",
      "training time: 0.0h 25m 3s\n",
      "\n",
      "Epoch 16/30\n",
      "----------\n",
      "training Loss: 0.0274 Acc: 0.6947\n",
      "validation Loss: 0.2940 Acc: 0.7236\n",
      "training time: 0.0h 26m 28s\n",
      "\n",
      "Epoch 17/30\n",
      "----------\n",
      "training Loss: 0.0272 Acc: 0.6976\n",
      "validation Loss: 0.2639 Acc: 0.7719\n",
      "training time: 0.0h 27m 52s\n",
      "\n",
      "Epoch 18/30\n",
      "----------\n",
      "training Loss: 0.0266 Acc: 0.7042\n",
      "validation Loss: 0.2673 Acc: 0.7698\n",
      "training time: 0.0h 29m 14s\n",
      "\n",
      "Epoch 19/30\n",
      "----------\n",
      "training Loss: 0.0263 Acc: 0.7043\n",
      "validation Loss: 0.2758 Acc: 0.7687\n",
      "training time: 0.0h 30m 40s\n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "training Loss: 0.0263 Acc: 0.7060\n",
      "validation Loss: 0.2793 Acc: 0.7493\n",
      "training time: 0.0h 32m 5s\n",
      "\n",
      "Epoch 21/30\n",
      "----------\n",
      "training Loss: 0.0258 Acc: 0.7109\n",
      "validation Loss: 0.2659 Acc: 0.7575\n",
      "training time: 0.0h 33m 28s\n",
      "\n",
      "Epoch 22/30\n",
      "----------\n",
      "training Loss: 0.0255 Acc: 0.7125\n",
      "validation Loss: 0.2663 Acc: 0.7553\n",
      "training time: 0.0h 34m 51s\n",
      "\n",
      "Epoch 23/30\n",
      "----------\n",
      "training Loss: 0.0254 Acc: 0.7143\n",
      "validation Loss: 0.2641 Acc: 0.7657\n",
      "training time: 0.0h 36m 15s\n",
      "\n",
      "Epoch 24/30\n",
      "----------\n",
      "training Loss: 0.0250 Acc: 0.7167\n",
      "validation Loss: 0.2631 Acc: 0.7625\n",
      "training time: 0.0h 37m 39s\n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "training Loss: 0.0249 Acc: 0.7191\n",
      "validation Loss: 0.2638 Acc: 0.7631\n",
      "training time: 0.0h 39m 3s\n",
      "\n",
      "Epoch 26/30\n",
      "----------\n",
      "training Loss: 0.0245 Acc: 0.7206\n",
      "validation Loss: 0.2603 Acc: 0.7723\n",
      "training time: 0.0h 40m 26s\n",
      "\n",
      "Epoch 27/30\n",
      "----------\n",
      "training Loss: 0.0245 Acc: 0.7212\n",
      "validation Loss: 0.2481 Acc: 0.7781\n",
      "training time: 0.0h 41m 51s\n",
      "\n",
      "Epoch 28/30\n",
      "----------\n",
      "training Loss: 0.0243 Acc: 0.7227\n",
      "validation Loss: 0.2491 Acc: 0.7735\n",
      "training time: 0.0h 43m 14s\n",
      "\n",
      "Epoch 29/30\n",
      "----------\n",
      "training Loss: 0.0243 Acc: 0.7251\n",
      "validation Loss: 0.2531 Acc: 0.7751\n",
      "training time: 0.0h 44m 36s\n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "training Loss: 0.0243 Acc: 0.7255\n",
      "validation Loss: 0.2556 Acc: 0.7622\n",
      "training time: 0.0h 45m 59s\n",
      "\n",
      "Training complete in 0.0h 45m 59s\n",
      "\n",
      "training model4\n",
      "\n",
      "Epoch 1/30\n",
      "----------\n",
      "training Loss: 0.0436 Acc: 0.5411\n",
      "validation Loss: 0.3316 Acc: 0.7363\n",
      "training time: 0.0h 5m 8s\n",
      "\n",
      "Epoch 2/30\n",
      "----------\n",
      "training Loss: 0.0323 Acc: 0.6583\n",
      "validation Loss: 0.2995 Acc: 0.7502\n",
      "training time: 0.0h 7m 30s\n",
      "\n",
      "Epoch 3/30\n",
      "----------\n",
      "training Loss: 0.0285 Acc: 0.6974\n",
      "validation Loss: 0.2450 Acc: 0.7949\n",
      "training time: 0.0h 9m 4s\n",
      "\n",
      "Epoch 4/30\n",
      "----------\n",
      "training Loss: 0.0263 Acc: 0.7217\n",
      "validation Loss: 0.2547 Acc: 0.7954\n",
      "training time: 0.0h 10m 31s\n",
      "\n",
      "Epoch 5/30\n",
      "----------\n",
      "training Loss: 0.0251 Acc: 0.7343\n",
      "validation Loss: 0.2845 Acc: 0.7678\n",
      "training time: 0.0h 11m 57s\n",
      "\n",
      "Epoch 6/30\n",
      "----------\n",
      "training Loss: 0.0234 Acc: 0.7516\n",
      "validation Loss: 0.2353 Acc: 0.7975\n",
      "training time: 0.0h 13m 23s\n",
      "\n",
      "Epoch 7/30\n",
      "----------\n",
      "training Loss: 0.0231 Acc: 0.7581\n",
      "validation Loss: 0.2154 Acc: 0.8241\n",
      "training time: 0.0h 14m 49s\n",
      "\n",
      "Epoch 8/30\n",
      "----------\n",
      "training Loss: 0.0218 Acc: 0.7666\n",
      "validation Loss: 0.1967 Acc: 0.8337\n",
      "training time: 0.0h 16m 15s\n",
      "\n",
      "Epoch 9/30\n",
      "----------\n",
      "training Loss: 0.0216 Acc: 0.7724\n",
      "validation Loss: 0.2101 Acc: 0.8150\n",
      "training time: 0.0h 17m 42s\n",
      "\n",
      "Epoch 10/30\n",
      "----------\n",
      "training Loss: 0.0206 Acc: 0.7810\n",
      "validation Loss: 0.1664 Acc: 0.8621\n",
      "training time: 0.0h 19m 8s\n",
      "\n",
      "Epoch 11/30\n",
      "----------\n",
      "training Loss: 0.0199 Acc: 0.7870\n",
      "validation Loss: 0.1867 Acc: 0.8478\n",
      "training time: 0.0h 20m 35s\n",
      "\n",
      "Epoch 12/30\n",
      "----------\n",
      "training Loss: 0.0195 Acc: 0.7921\n",
      "validation Loss: 0.1795 Acc: 0.8465\n",
      "training time: 0.0h 22m 2s\n",
      "\n",
      "Epoch 13/30\n",
      "----------\n",
      "training Loss: 0.0193 Acc: 0.7957\n",
      "validation Loss: 0.1829 Acc: 0.8478\n",
      "training time: 0.0h 23m 29s\n",
      "\n",
      "Epoch 14/30\n",
      "----------\n",
      "training Loss: 0.0189 Acc: 0.8000\n",
      "validation Loss: 0.1870 Acc: 0.8362\n",
      "training time: 0.0h 24m 55s\n",
      "\n",
      "Epoch 15/30\n",
      "----------\n",
      "training Loss: 0.0184 Acc: 0.8031\n",
      "validation Loss: 0.1737 Acc: 0.8510\n",
      "training time: 0.0h 26m 21s\n",
      "\n",
      "Epoch 16/30\n",
      "----------\n",
      "training Loss: 0.0178 Acc: 0.8091\n",
      "validation Loss: 0.1695 Acc: 0.8550\n",
      "training time: 0.0h 27m 47s\n",
      "\n",
      "Epoch 17/30\n",
      "----------\n",
      "training Loss: 0.0174 Acc: 0.8125\n",
      "validation Loss: 0.1706 Acc: 0.8566\n",
      "training time: 0.0h 29m 12s\n",
      "\n",
      "Epoch 18/30\n",
      "----------\n",
      "training Loss: 0.0173 Acc: 0.8135\n",
      "validation Loss: 0.1828 Acc: 0.8483\n",
      "training time: 0.0h 30m 37s\n",
      "\n",
      "Epoch 19/30\n",
      "----------\n",
      "training Loss: 0.0168 Acc: 0.8162\n",
      "validation Loss: 0.1614 Acc: 0.8649\n",
      "training time: 0.0h 32m 4s\n",
      "\n",
      "Epoch 20/30\n",
      "----------\n",
      "training Loss: 0.0170 Acc: 0.8169\n",
      "validation Loss: 0.1600 Acc: 0.8629\n",
      "training time: 0.0h 33m 31s\n",
      "\n",
      "Epoch 21/30\n",
      "----------\n",
      "training Loss: 0.0167 Acc: 0.8210\n",
      "validation Loss: 0.1513 Acc: 0.8695\n",
      "training time: 0.0h 34m 56s\n",
      "\n",
      "Epoch 22/30\n",
      "----------\n",
      "training Loss: 0.0166 Acc: 0.8190\n",
      "validation Loss: 0.1798 Acc: 0.8363\n",
      "training time: 0.0h 36m 24s\n",
      "\n",
      "Epoch 23/30\n",
      "----------\n",
      "training Loss: 0.0163 Acc: 0.8247\n",
      "validation Loss: 0.1495 Acc: 0.8710\n",
      "training time: 0.0h 37m 51s\n",
      "\n",
      "Epoch 24/30\n",
      "----------\n",
      "training Loss: 0.0159 Acc: 0.8258\n",
      "validation Loss: 0.1482 Acc: 0.8721\n",
      "training time: 0.0h 39m 16s\n",
      "\n",
      "Epoch 25/30\n",
      "----------\n",
      "training Loss: 0.0157 Acc: 0.8284\n",
      "validation Loss: 0.1514 Acc: 0.8712\n",
      "training time: 0.0h 40m 41s\n",
      "\n",
      "Epoch 26/30\n",
      "----------\n",
      "training Loss: 0.0154 Acc: 0.8321\n",
      "validation Loss: 0.1365 Acc: 0.8859\n",
      "training time: 0.0h 42m 7s\n",
      "\n",
      "Epoch 27/30\n",
      "----------\n",
      "training Loss: 0.0154 Acc: 0.8332\n",
      "validation Loss: 0.1487 Acc: 0.8705\n",
      "training time: 0.0h 43m 34s\n",
      "\n",
      "Epoch 28/30\n",
      "----------\n",
      "training Loss: 0.0154 Acc: 0.8339\n",
      "validation Loss: 0.1520 Acc: 0.8673\n",
      "training time: 0.0h 45m 2s\n",
      "\n",
      "Epoch 29/30\n",
      "----------\n",
      "training Loss: 0.0152 Acc: 0.8338\n",
      "validation Loss: 0.1598 Acc: 0.8615\n",
      "training time: 0.0h 46m 31s\n",
      "\n",
      "Epoch 30/30\n",
      "----------\n",
      "training Loss: 0.0149 Acc: 0.8369\n",
      "validation Loss: 0.1541 Acc: 0.8651\n",
      "training time: 0.0h 47m 57s\n",
      "\n",
      "Training complete in 0.0h 47m 57s\n",
      "\n",
      "training model5\n",
      "\n",
      "Epoch 1/30\n",
      "----------\n",
      "Traceback (most recent call last):\n",
      "  File \"train_sealnet.py\", line 245, in <module>\n",
      "    main()\n",
      "  File \"train_sealnet.py\", line 241, in main\n",
      "    num_epochs=hyperparameters[args.hyperparameter_set]['epochs'])\n",
      "  File \"train_sealnet.py\", line 158, in train_model\n",
      "    outputs = model(inputs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/bento/PycharmProjects/Seals_branches/Master/Seals/custom_architectures/wide_resnet.py\", line 92, in forward\n",
      "    out = F.avg_pool2d(out, 7)\n",
      "RuntimeError: Given input size: (64x4x4). Calculated output size: (64x0x0). Output size is too small at /opt/conda/conda-bld/pytorch_1524590031827/work/aten/src/THCUNN/generic/SpatialAveragePooling.cu:63\n",
      "\n",
      "training model6\n",
      "\n",
      "Epoch 1/30\n",
      "----------\n",
      "Traceback (most recent call last):\n",
      "  File \"train_sealnet.py\", line 245, in <module>\n",
      "    main()\n",
      "  File \"train_sealnet.py\", line 241, in main\n",
      "    num_epochs=hyperparameters[args.hyperparameter_set]['epochs'])\n",
      "  File \"train_sealnet.py\", line 158, in train_model\n",
      "    outputs = model(inputs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/bento/PycharmProjects/Seals_branches/Master/Seals/custom_architectures/wide_resnet.py\", line 92, in forward\n",
      "    out = F.avg_pool2d(out, 7)\n",
      "RuntimeError: Given input size: (64x4x4). Calculated output size: (64x0x0). Output size is too small at /opt/conda/conda-bld/pytorch_1524590031827/work/aten/src/THCUNN/generic/SpatialAveragePooling.cu:63\n",
      "\n",
      "training model7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/30\n",
      "----------\n",
      "Traceback (most recent call last):\n",
      "  File \"train_sealnet.py\", line 245, in <module>\n",
      "    main()\n",
      "  File \"train_sealnet.py\", line 241, in main\n",
      "    num_epochs=hyperparameters[args.hyperparameter_set]['epochs'])\n",
      "  File \"train_sealnet.py\", line 158, in train_model\n",
      "    outputs = model(inputs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/bento/PycharmProjects/Seals_branches/Master/Seals/custom_architectures/wide_resnet.py\", line 92, in forward\n",
      "    out = F.avg_pool2d(out, 7)\n",
      "RuntimeError: Given input size: (64x4x4). Calculated output size: (64x0x0). Output size is too small at /opt/conda/conda-bld/pytorch_1524590031827/work/aten/src/THCUNN/generic/SpatialAveragePooling.cu:63\n",
      "\n",
      "training model8\n",
      "\n",
      "Epoch 1/30\n",
      "----------\n",
      "Traceback (most recent call last):\n",
      "  File \"train_sealnet.py\", line 245, in <module>\n",
      "    main()\n",
      "  File \"train_sealnet.py\", line 241, in main\n",
      "    num_epochs=hyperparameters[args.hyperparameter_set]['epochs'])\n",
      "  File \"train_sealnet.py\", line 158, in train_model\n",
      "    outputs = model(inputs)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 491, in __call__\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"/home/bento/PycharmProjects/Seals_branches/Master/Seals/custom_architectures/wide_resnet.py\", line 92, in forward\n",
      "    out = F.avg_pool2d(out, 7)\n",
      "RuntimeError: Given input size: (64x4x4). Calculated output size: (64x0x0). Output size is too small at /opt/conda/conda-bld/pytorch_1524590031827/work/aten/src/THCUNN/generic/SpatialAveragePooling.cu:63\n"
     ]
    }
   ],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_single.iterrows():\n",
    "    t_dir, arch, hyp_st, cv_wgt, out = row[1]['training_dir'], row[1]['model_architecture'], row[1]['hyperparameter_set'], row[1]['cv_weights'], row[1]['output_name']\n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./saved_models/single_seal/{}/'.format(out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch --hyperparameter_set=$hyp_st --cv_weights=$cv_wgt --output_name=$out\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation - haulout level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the models we just trained to get measurements of precision and recall for all positive classes. For every model combination we trained, *validate_sealnet.py* will run a full validation round and write given label/correct label pairs to a .csv file. The resulting .csv file is then imported by an R script, *plot_confusion_matrix.R*, which saves a confusion matrix figure and a .csv spreadsheet with precision and recall for all classes of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DataFrame to combine all metrics \n",
    "comb_prec_recall = pd.DataFrame()\n",
    "\n",
    "# iterate over trained models\n",
    "for row in combinations_haul.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st, out = row[1]['training_dir'], row[1]['model_architecture'], row[1]['hyperparameter_set'], row[1]['output_name']\n",
    "    \n",
    "    # check if model file is available\n",
    "    if \"{}.tar\".format(out) not in os.listdir('./saved_models/haulout/{}/'.format(out)): \n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        print()\n",
    "        !echo validating $out\n",
    "        print()\n",
    "        \n",
    "        # run validation\n",
    "        !python validate_sealnet_haulout.py --training_dir=$t_dir --model_architecture=$arch --hyperparameter_set=$hyp_st --model_name=$out\n",
    "        \n",
    "        # extract performance\n",
    "        !Rscript plot_confusion_matrix.R $out\n",
    "        \n",
    "        # accumulate performance scores\n",
    "        comb_prec_recall = comb_prec_recall.append(pd.read_csv('./saved_models/haulout/{}/{}_haul_prec_recall.csv'.format(out, out)))\n",
    "    \n",
    "    \n",
    "# Write combined metrics to csv\n",
    "comb_prec_recall.to_csv('./saved_models/haulout/pooled_haul_prec_recall.csv')\n",
    "\n",
    "# Plot combined metrics\n",
    "!Rscript plot_comparison.R\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation - scene level\n",
    "\n",
    "The first step to validate models at the scene level is to tile out rasters to model input sizes. The following cell will search for all rasters in a dir, check if they are present in the scene_bank (see  *training_set_generation.ipynb*), store an affine matrix for that scene to go from the tile's index in the scene to projected coordinates and create tiles with the correct dimensions(including multi-scale models) for all model architectures in the 'combinations' DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load scene bank\n",
    "scene_bank = pd.read_csv('./training_sets/seals_scene_bank.csv')\n",
    "\n",
    "# point to raster dir\n",
    "raster_dir = '/home/bento/imagery'\n",
    "\n",
    "# create output folder\n",
    "%mkdir './tiled_images'\n",
    "\n",
    "# store data transforms\n",
    "affine_transforms = {}\n",
    "\n",
    "# loop through rasters creating tiled versions\n",
    "print('\\nTiling rasters:')\n",
    "for path, _, files in os.walk(raster_dir):\n",
    "    # filter rasters to include only ones present in the scene bank\n",
    "    files = [file for file in files if file in pd.unique(scene_bank['scene'])]\n",
    "    for count, filename in enumerate(files):\n",
    "        filename_lower = filename.lower()\n",
    "        input_path = (os.path.join(path, filename))\n",
    "        # extract data transform to go from raster index to ESPG3031 coordinates\n",
    "        with rasterio.open(input_path) as src:\n",
    "            affine_transforms[filename] = [src.transform[1], src.transform[2], src.transform[0], src.transform[4], src.transform[5], src.transform[3]]\n",
    "        # loop over models\n",
    "        for row in combinations_haul.iterrows():\n",
    "            # get model input size\n",
    "            input_size = model_archs[row[1]['model_architecture']]\n",
    "            ts_scales = training_sets[row[1]['training_dir']]['scale_bands']\n",
    "            scales = [str(int(input_size * scale / ts_scales[0])) for scale in ts_scales]\n",
    "            scale_bands = reduce(lambda x,y: x + '_' + y, scales)\n",
    "            output_folder = './tiled_images/{}/{}'.format(input_path, scale_bands)\n",
    "            # create a tiled out image for that model if it doesn't exist\n",
    "            if os.path.exists(output_folder):\n",
    "                print('{} is already tiled'.format(filename))\n",
    "                continue\n",
    "            !python tile_raster.py --scale_bands=$scale_bands  --input_image=$input_path --output_folder='./tiled_images'\n",
    "        print('\\nProcessed {} out of {} rasters'.format(count + 1, len(files)))\n",
    "\n",
    "# write data transforms to csv\n",
    "affine_transforms = pd.DataFrame(affine_transforms)\n",
    "affine_transforms.to_csv('affine_transforms.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With rasters tiled to the correct dimensions we can now run validation at the scene level. This procedure will work as follows: for each model, classify all images tiled in the previous step and save classification results to a pandas DataFrame. Results will be compared with the scene_bank to measure precision and recall at detecting which scenes contain positive entries (i.e. seals, penguins or even both, depending on the scene_bank being used). Locations marked as having seal haulouts will be used to detect individual seals and validate models at the seal level. To get validation results, run the following cell: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A\n",
      "\n",
      "Classifying tiled scenes with model1:\n",
      "\n",
      "Classified 1 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20151030210819_104001001351DB00_15OCT30210819-P1BS-500658695070_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20151030210819_104001001351DB00_15OCT30210819-P1BS-500658695070_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 2 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20151022204340_10400100127E2F00_15OCT22204340-P1BS-500652476030_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20151022204340_10400100127E2F00_15OCT22204340-P1BS-500652476030_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 3 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20151008072633_1040010012A79A00_15OCT08072633-P1BS-500652459050_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20151008072633_1040010012A79A00_15OCT08072633-P1BS-500652459050_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 4 out of 35 tiled scenes\n",
      "\n",
      "Classified 5 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20141107053013_10400100046B2800_14NOV07053013-P1BS-500268574010_01_P006_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20141107053013_10400100046B2800_14NOV07053013-P1BS-500268574010_01_P006_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 6 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20170125165806_1040010028CD9C00_17JAN25165806-P1BS-057089610010_01_P002_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20170125165806_1040010028CD9C00_17JAN25165806-P1BS-057089610010_01_P002_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 7 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20170125165806_1040010028CD9C00_17JAN25165806-P1BS-057089610010_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20170125165806_1040010028CD9C00_17JAN25165806-P1BS-057089610010_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 8 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20151126133728_1040010013346700_15NOV26133728-P1BS-500666441080_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20151126133728_1040010013346700_15NOV26133728-P1BS-500666441080_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 9 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20170217064537_10400100297FEA00_17FEB17064537-P1BS-057107305010_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20170217064537_10400100297FEA00_17FEB17064537-P1BS-057107305010_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 10 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20151005050119_1040010011172D00_15OCT05050119-P1BS-500638962090_01_P005_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20151005050119_1040010011172D00_15OCT05050119-P1BS-500638962090_01_P005_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 11 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20151005095617_1040010012382500_15OCT05095617-P1BS-500652418050_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20151005095617_1040010012382500_15OCT05095617-P1BS-500652418050_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 12 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20160227062810_10400100191DC900_16FEB27062810-P1BS-500638658040_01_P002_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20160227062810_10400100191DC900_16FEB27062810-P1BS-500638658040_01_P002_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 13 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20160204214026_104001001842DA00_16FEB04214026-P1BS-500638664060_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20160204214026_104001001842DA00_16FEB04214026-P1BS-500638664060_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 14 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20160227062822_10400100181F9B00_16FEB27062822-P1BS-500638715080_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20160227062822_10400100181F9B00_16FEB27062822-P1BS-500638715080_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 15 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20141030211711_10400100036F5800_14OCT30211711-P1BS-500231417160_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20141030211711_10400100036F5800_14OCT30211711-P1BS-500231417160_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 16 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20141110205238_1040010004751700_14NOV10205238-P1BS-500231412090_01_P003_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20141110205238_1040010004751700_14NOV10205238-P1BS-500231412090_01_P003_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 17 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20151009060344_1040010011162500_15OCT09060344-P1BS-500652417060_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20151009060344_1040010011162500_15OCT09060344-P1BS-500652417060_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 18 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20160307021814_1040010018046800_16MAR07021814-P1BS-500687218040_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20160307021814_1040010018046800_16MAR07021814-P1BS-500687218040_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 19 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20141020184444_10400100031B9000_14OCT20184444-P1BS-500258392140_01_P008_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20141020184444_10400100031B9000_14OCT20184444-P1BS-500258392140_01_P008_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 20 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20160210052144_10400100184CC500_16FEB10052144-P1BS-500687302080_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20160210052144_10400100184CC500_16FEB10052144-P1BS-500687302080_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 21 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20160225140323_10400100196BE200_16FEB25140323-P1BS-500638709010_01_P009_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20160225140323_10400100196BE200_16FEB25140323-P1BS-500638709010_01_P009_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 22 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20151105045105_104001001323F200_15NOV05045105-P1BS-500658675020_01_P002_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20151105045105_104001001323F200_15NOV05045105-P1BS-500658675020_01_P002_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 23 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20141120214545_1040010005B62F00_14NOV20214545-P1BS-500258392060_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20141120214545_1040010005B62F00_14NOV20214545-P1BS-500258392060_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 24 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20151024193848_1040010013779B00_15OCT24193848-P1BS-500656046010_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20151024193848_1040010013779B00_15OCT24193848-P1BS-500656046010_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 25 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found 0 files in subfolders of: ./tiled_images/WV03_20151021104725_10400100124A0000_15OCT21104725-P1BS-500652455030_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             raise(RuntimeError(\"Found 0 files in subfolders of: \" + root + \"\\n\"\n\u001b[0;32m---> 79\u001b[0;31m                                \"Supported extensions are: \" + \",\".join(extensions)))\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Found 0 files in subfolders of: ./tiled_images/WV03_20151021104725_10400100124A0000_15OCT21104725-P1BS-500652455030_01_P001_u08rf3031.tif/224_224_224\nSupported extensions are: .jpg,.jpeg,.png,.ppm,.bmp,.pgm,.tif"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classified 26 out of 35 tiled scenes\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './tiled_images/move_files.sh/224_224_224'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/PycharmProjects/Seals_branches/Master/Seals/predict_sealnet_batches.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;31m# create dataloader instance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mImageFolder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_transforms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size_test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0mnum_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhyperparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_set\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_workers_train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './tiled_images/move_files.sh/224_224_224'"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot convert the series to <class 'int'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8ce386de1ebc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m                 \u001b[0mdetected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;31m# if scene is occupied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscene_bank\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mscene_bank\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'scene'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mscene\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'present'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdetected\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m                 \u001b[0mmodel_stats\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'TP'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         raise TypeError(\"cannot convert the series to \"\n\u001b[0;32m---> 97\u001b[0;31m                         \"{0}\".format(str(converter)))\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot convert the series to <class 'int'>"
     ]
    }
   ],
   "source": [
    "# select scene bank\n",
    "bank = \"seals_scene_bank.csv\" # for penguins: bank = penguins_scene_bank.csv\n",
    "\n",
    "# load scene bank\n",
    "scene_bank = pd.read_csv('./training_sets/{}'.format(bank))\n",
    "positive_classes = [label for label in scene_bank.columns if label in class_names]\n",
    "\n",
    "# store precision and recall\n",
    "comb_prec_rec = pd.DataFrame()\n",
    "\n",
    "# loop through models\n",
    "for row in combinations_haul.iterrows():\n",
    "    # store model classifications to get seal locations for seal level validation\n",
    "    val_scene_model = pd.DataFrame()\n",
    "    \n",
    "    # get model settings\n",
    "    arch = row[1]['model_architecture']\n",
    "    model_name = row[1]['output_name']\n",
    "    t_dir = row[1]['training_dir']\n",
    "    hyp_st = row[1]['hyperparameter_set']\n",
    "    \n",
    "    # check if model was already processed\n",
    "    if \"{}_scene_val.csv\".format(model_name) in os.listdir('./saved_models/haulout/{}/'.format(model_name)): \n",
    "        continue\n",
    "    \n",
    "    # get scale bands\n",
    "    input_size = model_archs[arch]['input_size']\n",
    "    ts_scales = training_sets[t_dir]['scale_bands']\n",
    "    scales = [str(int(input_size * scale / ts_scales[0])) for scale in ts_scales]\n",
    "    scale_bands = reduce(lambda x,y: x + '_' + y, scales)\n",
    "    \n",
    "    # count true positives, true negatives, false positives, false negatives\n",
    "    model_stats = {'TP': 0, 'TN': 0, 'FP': 0, 'FN': 0}\n",
    "    \n",
    "    # loop through tiled rasters\n",
    "    scenes = os.listdir('./tiled_images')\n",
    "    print('\\nClassifying tiled scenes with {}:'.format(model_name))\n",
    "    for count, scene in enumerate(scenes):\n",
    "        input_folder = './tiled_images/{}/{}'.format(scene, scale_bands)\n",
    "        %run predict_sealnet_batches.py --training_dir=$t_dir --hyperparameter_set=$hyp_st --model_architecture=$arch --model_name=$model_name --data_dir=$input_folder --affine_transforms='affine_transforms.csv' --scene=$scene        \n",
    "        val_scene_model = val_scene_model.append(pd.read_csv('./saved_models/haulout/{}/{}_scene_val_tmp.csv'.format(model_name, model_name)), ignore_index=True)\n",
    "        # record if scene is TP, TN, FP or FN\n",
    "        detected = False\n",
    "        for label in positive_classes:\n",
    "            if label in val_scene_model['label']:\n",
    "                detected = True\n",
    "        # if scene is occupied\n",
    "        if int(scene_bank.loc[scene_bank['scene'] == scene]['present']):\n",
    "            if detected:\n",
    "                model_stats['TP'] += 1\n",
    "            else:\n",
    "                model_stats['FN'] += 1\n",
    "        # if scene is empty\n",
    "        else:\n",
    "            if detected:\n",
    "                model_stats['FP'] += 1\n",
    "            else:\n",
    "                model_stats['TN'] += 1\n",
    "    \n",
    "        print('\\nClassified {} out of {} tiled scenes'.format(count + 1, len(scenes)))\n",
    "    \n",
    "    # record precision and recall for model\n",
    "    precision = TP / (TP + FP)\n",
    "    recall = TP / (TP + FN)\n",
    "    comb_prec_rec.append({'model': model_name, 'precision': precision, 'recall': recall})\n",
    "    \n",
    "    # save final classifications to csv and cleanup temporary stats\n",
    "    val_scene_model.to_csv('./saved_models/haulout/{}/{}_scene_val.csv'.format(model_name, model_name))\n",
    "    !rm $('./saved_models/haulout/{}/{}_scene_val_tmp.csv'.format(model_name, model_name))\n",
    "    \n",
    "# save combined precision recall and combine classifications\n",
    "comb_prec_rec.to_csv('./saved_models/haulout/pooled_scene_prec_recall.csv')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation -- single seal level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%mkdir classified_images\n",
    "for x in class_names:\n",
    "    dir = './classified_images/' + x\n",
    "    %mkdir -v $dir\n",
    "    \n",
    "%run predict_sealnet.py \"training_set_vanilla\" \"NasnetA\" \"model5\" \"./to_classify\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
