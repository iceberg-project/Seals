{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seal Detection Pipeline\n",
    "---\n",
    "\n",
    "This jupyter notebook will go through assembling the main components of a complete pipeline for counting seals in high-resolution satellite imagery (figure 1, steps 3 and 4) and show some experimental results with different pipeline designs. The ultimate goal of this pipeline is to perform a pan-Antarctic pack-ice seal census. ** Running this code will require input satellite imagery and at least one GPU with >8GB of memory **\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"jupyter_notebook_images/Base Pipeline.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "---\n",
    "* [Getting started](#intro)\n",
    "    * [Setup](#setup)\n",
    "    * [Visualize training set](#vis_imgs)\n",
    "* [Pipeline 1 - Seal haulout detector](#1)\n",
    "    * [Training](#1T)\n",
    "    * [Validation](#1V)\n",
    "    * [Ablation experiment](#1A)\n",
    "* [Pipeline 1.1 - Seal haulout detector + count](#1.1)\n",
    "    * [Training](#1.1T)\n",
    "    * [Validation](#1.1V)\n",
    "    * [Ablation experiment / testing](#1.1A)\n",
    "* [Pipeline 1.2 - Seal haulout detector + single seal detector](#1.2)\n",
    "    * [Training](#1.2T)\n",
    "    * [Validation](#1.2V)\n",
    "    * [Testing](#1.2A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started<a name=\"intro\"></a>\n",
    "---\n",
    "\n",
    "If you followed the *training_set_generation* jupyter notebook (also present in this repo), you should have training sets generated and hyperparameter sets to try out, and be ready to search for a best performing seal detection pipeline.  Output files in this repository are organized as follows: *'./{dest_folder}/{pipeline}/{model_settings}/{model_settings}_{file}'*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment<a name=\"setup\"></a>\n",
    "\n",
    "Before training and validating model/hyperparameter combinations inside the pipelines, we need to load the required python modules and a few global variables. Running this script will also display a list of training classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib as mpl\n",
    "from functools import reduce\n",
    "from utils.model_library import * \n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi']= 400\n",
    "\n",
    "# destination folder for saved models and model stats\n",
    "dest_folder = 'saved_models'\n",
    "\n",
    "# save class names\n",
    "class_names = sorted([subdir for subdir in os.listdir('./training_sets/training_set_vanilla/training')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training images (Optional)<a name=\"vis_imgs\"></a>\n",
    "\n",
    "To get a better sense for what the training set is like, the next cell will display a few random images from the training classes. Displayed images are extracted from a pool of ~70000 training images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store images\n",
    "images = []\n",
    "\n",
    "# loop over labels\n",
    "for label in class_names:\n",
    "    for path, _, files in os.walk('./training_sets/training_set_vanilla/training/{}'.format(label)):\n",
    "        files = np.random.choice(files, 5)\n",
    "        for filename in files:\n",
    "            images.append(np.asarray(Image.open(os.path.join(path, filename))))\n",
    "\n",
    "images = np.array(images)\n",
    "\n",
    "# display images \n",
    "ncols=len(class_names)\n",
    "nindex, height, width, intensity = images.shape\n",
    "nrows = nindex // ncols\n",
    "# check if rows and columns can fit the number of images\n",
    "assert nindex == nrows * ncols\n",
    "result = (images.reshape(nrows, ncols, height, width, intensity)\n",
    "          .swapaxes(1,2)\n",
    "          .reshape(height*nrows, width*ncols, intensity))\n",
    "\n",
    "plt.imshow(result)\n",
    "cur_axes = plt.gca()\n",
    "cur_axes.axes.get_xaxis().set_visible(False)\n",
    "cur_axes.axes.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1 - Heatmap models <a name=\"1\"></a>\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"1T\"></a>\n",
    "\n",
    "The first step to find a best performing model is to train different model setups using our training set. To keep track of which combinations we have tried, how well they performed and the specifics of each model setup, we will store results in folders (under './{dest_folder}') named after each specific model combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# switch pipeline\n",
    "pipeline = 'Heatmap'\n",
    "\n",
    "# generate model combinations\n",
    "combinations_1 = {'model_architecture': ['Unet'],\n",
    "                  'training_dir': ['training_set_vanilla'],\n",
    "                  'hyperparameter_set': ['E']}\n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_1 = pd.DataFrame(combinations_1)\n",
    "                    \n",
    "# create folders for resulting files\n",
    "for row in combinations_1.iterrows():\n",
    "    mdl = row[1]['model_architecture'] + '_ts-' + row[1]['training_dir'].split('_')[-1]              \n",
    "    if not os.path.exists(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)):\n",
    "        os.makedirs(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then provide model combinations created above as arguments to the training script, *train_sealnet.py*. A list of required arguments can be displayed by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run train_sealnet.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training Unet_ts-vanilla\n",
      "\n",
      "Epoch 1/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.17458381634927667\n",
      "count loss: 6657.09390962841\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.027488740813612706\n",
      "count loss: 555.4578534004345\n",
      "occupancy loss: 0.0\n",
      "training time: 0.0h 37m 21s\n",
      "\n",
      "Epoch 2/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.018631256396823302\n",
      "count loss: 227.32557576500676\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.010977884428580937\n",
      "count loss: 63.05856414103473\n",
      "occupancy loss: 0.0\n",
      "training time: 1.0h 14m 22s\n",
      "\n",
      "Epoch 3/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.012972626867151116\n",
      "count loss: 74.9885837019747\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.014404363794392033\n",
      "count loss: 45.37391026394929\n",
      "occupancy loss: 0.0\n",
      "training time: 1.0h 51m 51s\n",
      "\n",
      "Epoch 4/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.011121060806777421\n",
      "count loss: 57.260663389702856\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.004026610648244039\n",
      "count loss: 28.759098939440673\n",
      "occupancy loss: 0.0\n",
      "training time: 2.0h 29m 6s\n",
      "\n",
      "Epoch 5/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.009867003854386805\n",
      "count loss: 47.621263654642796\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.0172972159540669\n",
      "count loss: 38.74677428539041\n",
      "occupancy loss: 0.0\n",
      "training time: 3.0h 6m 20s\n",
      "\n",
      "Epoch 6/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.008778181349215132\n",
      "count loss: 44.73425834349169\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.0460337154912339\n",
      "count loss: 26.027060030859342\n",
      "occupancy loss: 0.0\n",
      "training time: 3.0h 43m 19s\n",
      "\n",
      "Epoch 7/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.00995470910275246\n",
      "count loss: 44.42889229435463\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.015314686815820098\n",
      "count loss: 71.71428094164841\n",
      "occupancy loss: 0.0\n",
      "training time: 4.0h 20m 50s\n",
      "\n",
      "Epoch 8/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.00861015084759284\n",
      "count loss: 42.085023275086364\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.008994985002501366\n",
      "count loss: 33.733882601326556\n",
      "occupancy loss: 0.0\n",
      "training time: 4.0h 58m 50s\n",
      "\n",
      "Epoch 9/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.008581045769316745\n",
      "count loss: 39.805987237559854\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.03145708117688296\n",
      "count loss: 18.365681697441968\n",
      "occupancy loss: 0.0\n",
      "training time: 5.0h 36m 20s\n",
      "\n",
      "Epoch 10/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.007276317980335988\n",
      "count loss: 37.813976571182046\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.00438078019321478\n",
      "count loss: 24.692733923612256\n",
      "occupancy loss: 0.0\n",
      "training time: 6.0h 14m 16s\n",
      "\n",
      "Epoch 11/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.008185845398375217\n",
      "count loss: 36.82946734932525\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.031073671935695497\n",
      "count loss: 24.36796771525848\n",
      "occupancy loss: 0.0\n",
      "training time: 6.0h 53m 24s\n",
      "\n",
      "Epoch 12/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.007938898436420927\n",
      "count loss: 37.13488571530179\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.0068881642085587115\n",
      "count loss: 28.954885008285437\n",
      "occupancy loss: 0.0\n",
      "training time: 7.0h 31m 4s\n",
      "\n",
      "Epoch 13/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.007360987196360626\n",
      "count loss: 34.09824225108239\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.02534198983859439\n",
      "count loss: 30.850690617779048\n",
      "occupancy loss: 0.0\n",
      "training time: 8.0h 9m 55s\n",
      "\n",
      "Epoch 14/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.006270702299873694\n",
      "count loss: 31.999432838202246\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.036705659127514376\n",
      "count loss: 27.2024698510442\n",
      "occupancy loss: 0.0\n",
      "training time: 8.0h 48m 1s\n",
      "\n",
      "Epoch 15/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.006788554786860852\n",
      "count loss: 31.475736911692234\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.007581910406430515\n",
      "count loss: 16.848123333468518\n",
      "occupancy loss: 0.0\n",
      "training time: 9.0h 25m 54s\n",
      "\n",
      "Epoch 16/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.00629718518459968\n",
      "count loss: 30.92696969362564\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.039911071439144026\n",
      "count loss: 17.054402389708784\n",
      "occupancy loss: 0.0\n",
      "training time: 10.0h 3m 34s\n",
      "\n",
      "Epoch 17/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.006447910968763827\n",
      "count loss: 31.575715362749616\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.04935861948724243\n",
      "count loss: 14.336847621705338\n",
      "occupancy loss: 0.0\n",
      "training time: 10.0h 40m 56s\n",
      "\n",
      "Epoch 18/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.006242462888362186\n",
      "count loss: 30.54109415613989\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.010383564161266536\n",
      "count loss: 74.05653708070419\n",
      "occupancy loss: 0.0\n",
      "training time: 11.0h 18m 21s\n",
      "\n",
      "Epoch 19/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.006514082407487034\n",
      "count loss: 30.089700457564263\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.03164452564677063\n",
      "count loss: 18.704634279348607\n",
      "occupancy loss: 0.0\n",
      "training time: 11.0h 55m 40s\n",
      "\n",
      "Epoch 20/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.0054891403469899906\n",
      "count loss: 27.20828554386832\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.01897174737723291\n",
      "count loss: 15.963330570283055\n",
      "occupancy loss: 0.0\n",
      "training time: 12.0h 32m 48s\n",
      "\n",
      "Epoch 21/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.004429684499492407\n",
      "count loss: 23.796466778795356\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.03383659767276712\n",
      "count loss: 16.248982393998133\n",
      "occupancy loss: 0.0\n",
      "training time: 13.0h 9m 45s\n",
      "\n",
      "Epoch 22/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.004244628049713054\n",
      "count loss: 21.799582119882473\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.03186529979863051\n",
      "count loss: 14.394856526234637\n",
      "occupancy loss: 0.0\n",
      "training time: 13.0h 46m 40s\n",
      "\n",
      "Epoch 23/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.004480766569593467\n",
      "count loss: 20.382517467775106\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.03383483835355244\n",
      "count loss: 13.299211937488382\n",
      "occupancy loss: 0.0\n",
      "training time: 14.0h 24m 0s\n",
      "\n",
      "Epoch 24/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.004425149642033542\n",
      "count loss: 21.38443264717949\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.04433467410267547\n",
      "count loss: 11.863488139235375\n",
      "occupancy loss: 0.0\n",
      "training time: 15.0h 0m 49s\n",
      "\n",
      "Epoch 25/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.003951633237787999\n",
      "count loss: 19.692706564796406\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.01486710556291102\n",
      "count loss: 15.08351602991906\n",
      "occupancy loss: 0.0\n",
      "training time: 15.0h 37m 46s\n",
      "\n",
      "Epoch 26/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.003940772544644432\n",
      "count loss: 19.505967571679555\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.017285225813011947\n",
      "count loss: 11.396293335890398\n",
      "occupancy loss: 0.0\n",
      "training time: 16.0h 15m 26s\n",
      "\n",
      "Epoch 27/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.004445179542815257\n",
      "count loss: 20.97156609862049\n",
      "occupancy loss: 0.0\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.03263179619495787\n",
      "count loss: 10.01411398942251\n",
      "occupancy loss: 0.0\n",
      "training time: 16.0h 53m 24s\n",
      "\n",
      "Epoch 28/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_1.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                             --hyperparameter_set=$hyp_st \\\n",
    "                             --output_name=$out --dest_folder=$dest_folder\n",
    "                              \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"1V\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 2.1 - Heatmap + count<a name=\"1.1\"></a>\n",
    "---\n",
    "\n",
    "Here we will generate seal counting CNNs, train them and validate them. Seal counting CNNs will be trained to minimize the mean squared error (MSE) between predicted counts and ground-truth counts. Though they will be trained and validated separately from the haul out detector (Pipeline 1), these approaches will be tested on top of the haul out detector and as standalones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"1.1T\"></a>\n",
    "\n",
    "Similar to the previous pipeline, we will store results in folders (under './saved_models') named after each specific model combination for bookkeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d0f8fc021ef0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# read as a DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mcombinations_11\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombinations_11\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# switch pipeline\n",
    "pipeline = 'Heatmap-cnt'\n",
    "\n",
    "# generate model combinations\n",
    "combinations_21 = {'model_architecture': ['UnetCntWRN'],\n",
    "                   'training_dir': ['training_set_vanilla'],\n",
    "                   'hyperparameter_set': ['E']}       \n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_21 = pd.DataFrame(combinations_21)\n",
    "                    \n",
    "\n",
    "# create folders for resulting files\n",
    "for row in combinations_21.iterrows():\n",
    "    mdl = row[1]['model_architecture'] + '_ts-' + row[1]['training_dir'].split('_')[-1]               \n",
    "    if not os.path.exists(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)):\n",
    "        os.makedirs(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a counting model, model combinations created above are used as argument to to a new training script, *train_sealnet_count.py*, which uses MSE loss. It accepts the same arguments as the previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training WideResnetCount_ts-vanilla\n",
      "\n",
      "Epoch 1/5\n",
      "----------\n",
      "Traceback (most recent call last):\n",
      "  File \"train_sealnet_count.py\", line 236, in <module>\n",
      "    main()\n",
      "  File \"train_sealnet_count.py\", line 232, in main\n",
      "    num_epochs=hyperparameters[args.hyperparameter_set]['epochs'])\n",
      "  File \"train_sealnet_count.py\", line 146, in train_model\n",
      "    for data in dataloaders[phase]:\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 286, in __next__\n",
      "    return self._process_next_batch(batch)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 307, in _process_next_batch\n",
      "    raise batch.exc_type(batch.exc_msg)\n",
      "OSError: Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/bento/Seals/utils/dataloaders/data_loader_train_count.py\", line 109, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/bento/Seals/utils/dataloaders/data_loader_train_count.py\", line 165, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/bento/Seals/utils/dataloaders/data_loader_train_count.py\", line 147, in pil_loader\n",
      "    img = Image.open(f)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 2590, in open\n",
      "    % (filename if filename else fp))\n",
      "OSError: cannot identify image file <_io.BufferedReader name='./training_sets/training_set_vanilla/training/crabeater/2792.jpg'>\n",
      "\n",
      "\n",
      "training Resnet34count_ts-vanilla\n",
      "\n",
      "Epoch 1/5\n",
      "----------\n",
      "\n",
      "training Resnet18count_ts-vanilla\n",
      "\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train_sealnet_count.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/__init__.py\", line 276, in <module>\n",
      "    import torch.nn\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/nn/modules/__init__.py\", line 1, in <module>\n",
      "    from .module import Module\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 6, in <module>\n",
      "    from ..backends.thnn import backend as thnn_backend\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/nn/backends/thnn.py\", line 41, in <module>\n",
      "    _initialize_backend()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/nn/backends/thnn.py\", line 23, in _initialize_backend\n",
      "    from .._functions.rnn import RNN, \\\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 3, in <module>\n",
      "    import torch.backends.cudnn as cudnn\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/backends/cudnn/__init__.py\", line 169, in <module>\n",
      "    class CuDNNHandle:\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'child' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Vanilla Pexpect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mflush\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_poll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_poll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    302\u001b[0m         self.ptyproc = self._spawnpty(self.args, env=self.env,\n\u001b[0;32m--> 303\u001b[0;31m                                      cwd=self.cwd, **kwargs)\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;34m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mptyprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPtyProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ptyprocess/ptyprocess.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexec_err_pipe_write\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mexec_err_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexec_err_pipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexec_err_pipe_read\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-128fa26ef1f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'echo training $out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2259\u001b[0m         \u001b[0;31m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m         \u001b[0;31m# Instead, we store the exit_code in user_ns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2261\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msystem_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# (the character is known as ETX for 'End of Text', see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# curses.ascii.ETX).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0;31m# Read and print any more output the program might produce on its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;31m# way out.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'child' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_21.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                             --hyperparameter_set=$hyp_st --output_name=$out  \\\n",
    "                             --dest_folder=$dest_folder\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"1.1V\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 2.2 - Heatmap + occupancy <a name=\"1.2\"></a>\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"1.2T\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# switch pipeline\n",
    "pipeline = 'Heatmap-occ'\n",
    "\n",
    "# generate model combinations\n",
    "combinations_22 = {'model_architecture': ['UnetOccDense'],\n",
    "                   'training_dir': ['training_set_vanilla'],\n",
    "                   'hyperparameter_set': ['E'] }       \n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_22 = pd.DataFrame(combinations_22)\n",
    "                    \n",
    "\n",
    "# create folders for resulting files\n",
    "for row in combinations_22.iterrows():\n",
    "    mdl = row[1]['model_architecture'] + '_ts-' + row[1]['training_dir'].split('_')[-1]                  \n",
    "    if not os.path.exists(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)):\n",
    "        os.makedirs(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training UnetDet3Dense_ts-vanilla\n",
      "\n",
      "Epoch 1/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "train_sealnet_det3_L1.py:277: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  nn.utils.clip_grad_norm(model.parameters(), 0.5)\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss count: 0.004639265537261963\n",
      "   BCE loss: 0.007423256039619446\n",
      "   Occupancy loss: 0.012063369750976563\n",
      "   Total loss: 0.024125891327857973\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss count: 0.46058855775372054\n",
      "   BCE loss: 0.4915643134346409\n",
      "   Occupancy loss: 1.0132792065089702\n",
      "   Total loss: 1.9654320776973315\n",
      "\n",
      " 400 training iterations\n",
      "   Hubber loss count: 0.5155673709519228\n",
      "   BCE loss: 0.46340175502996145\n",
      "   Occupancy loss: 1.1233477759469919\n",
      "   Total loss: 2.102316901928876\n",
      "\n",
      " 600 training iterations\n",
      "   Hubber loss count: 0.49676646453236\n",
      "   BCE loss: 0.40183933784053594\n",
      "   Occupancy loss: 1.091930503409359\n",
      "   Total loss: 1.990536305782255\n",
      "\n",
      " 800 training iterations\n",
      "   Hubber loss count: 0.5332616397003311\n",
      "   BCE loss: 0.34538389958644206\n",
      "   Occupancy loss: 1.048728074146482\n",
      "   Total loss: 1.927373613433255\n",
      "\n",
      " 1000 training iterations\n",
      "   Hubber loss count: 0.5215187553986647\n",
      "   BCE loss: 0.29958791115996897\n",
      "   Occupancy loss: 0.9001582342013482\n",
      "   Total loss: 1.7212649007599818\n",
      "\n",
      " 1200 training iterations\n",
      "   Hubber loss count: 0.5322021527824253\n",
      "   BCE loss: 0.26139650722342495\n",
      "   Occupancy loss: 0.9082966945690748\n",
      "   Total loss: 1.701895354574925\n",
      "\n",
      " 1400 training iterations\n",
      "   Hubber loss count: 0.47402825258217324\n",
      "   BCE loss: 0.21518394490671192\n",
      "   Occupancy loss: 0.8306718859871305\n",
      "   Total loss: 1.5198840834760157\n",
      "\n",
      " 1600 training iterations\n",
      "   Hubber loss count: 0.542887005292686\n",
      "   BCE loss: 0.1853994339348101\n",
      "   Occupancy loss: 0.8184558064703926\n",
      "   Total loss: 1.5467422456978888\n",
      "\n",
      " 1800 training iterations\n",
      "   Hubber loss count: 0.48440941867602355\n",
      "   BCE loss: 0.15966538928620477\n",
      "   Occupancy loss: 0.7816775531866839\n",
      "   Total loss: 1.4257523611489122\n",
      "\n",
      " 2000 training iterations\n",
      "   Hubber loss count: 0.4340340490198609\n",
      "   BCE loss: 0.13162487579029025\n",
      "   Occupancy loss: 0.7102709054923809\n",
      "   Total loss: 1.2759298303025322\n",
      "\n",
      " 2200 training iterations\n",
      "   Hubber loss count: 0.423590002945607\n",
      "   BCE loss: 0.11702474407455715\n",
      "   Occupancy loss: 0.6455503826058632\n",
      "   Total loss: 1.1861651296260272\n",
      "\n",
      " 2400 training iterations\n",
      "   Hubber loss count: 0.42966722628192416\n",
      "   BCE loss: 0.10373844842334203\n",
      "   Occupancy loss: 0.6619075536978595\n",
      "   Total loss: 1.1953132284031258\n",
      "\n",
      " 2600 training iterations\n",
      "   Hubber loss count: 0.4379555635089536\n",
      "   BCE loss: 0.09210477769994055\n",
      "   Occupancy loss: 0.6489093346237538\n",
      "   Total loss: 1.178969675832648\n",
      "\n",
      " 2800 training iterations\n",
      "   Hubber loss count: 0.4607419054257841\n",
      "   BCE loss: 0.086557701259769\n",
      "   Occupancy loss: 0.6479593633814359\n",
      "   Total loss: 1.195258970066989\n",
      "\n",
      " 3000 training iterations\n",
      "   Hubber loss count: 0.37515953613854464\n",
      "   BCE loss: 0.0689613257780592\n",
      "   Occupancy loss: 0.6088204029080697\n",
      "   Total loss: 1.0529412648246734\n",
      "\n",
      " 3200 training iterations\n",
      "   Hubber loss count: 0.43452924106938734\n",
      "   BCE loss: 0.06431739233460666\n",
      "   Occupancy loss: 0.6269121904990693\n",
      "   Total loss: 1.1257588239030634\n",
      "\n",
      " 3400 training iterations\n",
      "   Hubber loss count: 0.3705442364473035\n",
      "   BCE loss: 0.05996053714738236\n",
      "   Occupancy loss: 0.6322299321489999\n",
      "   Total loss: 1.0627347057436858\n",
      "\n",
      " 3600 training iterations\n",
      "   Hubber loss count: 0.39852432160737433\n",
      "   BCE loss: 0.055048301218578836\n",
      "   Occupancy loss: 0.5846062367562243\n",
      "   Total loss: 1.0381788595821775\n",
      "\n",
      " 3800 training iterations\n",
      "   Hubber loss count: 0.35868656099016755\n",
      "   BCE loss: 0.04834314639744772\n",
      "   Occupancy loss: 0.5593409002407939\n",
      "   Total loss: 0.9663706076284091\n",
      "\n",
      " 4000 training iterations\n",
      "   Hubber loss count: 0.368847681650601\n",
      "   BCE loss: 0.04048045329786076\n",
      "   Occupancy loss: 0.5247662978694952\n",
      "   Total loss: 0.934094432817957\n",
      "\n",
      " 4200 training iterations\n",
      "   Hubber loss count: 0.3800022717898335\n",
      "   BCE loss: 0.042825569110815295\n",
      "   Occupancy loss: 0.6101465942214382\n",
      "   Total loss: 1.032974435122087\n",
      "\n",
      " 4400 training iterations\n",
      "   Hubber loss count: 0.3546038870842048\n",
      "   BCE loss: 0.03793025881491395\n",
      "   Occupancy loss: 0.523389307102674\n",
      "   Total loss: 0.9159234530017928\n",
      "\n",
      " 4600 training iterations\n",
      "   Hubber loss count: 0.3608976133189447\n",
      "   BCE loss: 0.033521955270891995\n",
      "   Occupancy loss: 0.5879917724678152\n",
      "   Total loss: 0.9824113410576519\n",
      "\n",
      " 4800 training iterations\n",
      "   Hubber loss count: 0.3625451490534232\n",
      "   BCE loss: 0.035895077634745545\n",
      "   Occupancy loss: 0.5668310472877036\n",
      "   Total loss: 0.9652712739758724\n",
      "training count loss: 0.4388\n",
      "training heatmap loss: 0.1635\n",
      "training occupancy loss: 0.7400\n",
      "\n",
      "validation \n",
      "\n",
      "\n",
      " 0 validation iterations\n",
      "   Hubber loss count: 0.35305105824889593\n",
      "   BCE loss: 0.03842858150006242\n",
      "   Occupancy loss: 0.5846153904506208\n",
      "   Total loss: 0.9760950301995792\n",
      "\n",
      " 200 validation iterations\n",
      "   Hubber loss count: 0.08620885656575866\n",
      "   BCE loss: 0.016403830792707883\n",
      "   Occupancy loss: 0.10880656476984747\n",
      "   Total loss: 0.21141925212831403\n",
      "validation count loss: 0.1690\n",
      "validation heatmap loss: 0.0214\n",
      "validation occupancy loss: 0.3047\n",
      "training time: 1.0h 1m 37s\n",
      "\n",
      "Epoch 2/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss count: 0.3469746208103155\n",
      "   BCE loss: 0.03327768127542932\n",
      "   Occupancy loss: 0.6871672543330878\n",
      "   Total loss: 1.0674195564188325\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss count: 0.34155579799705355\n",
      "   BCE loss: 0.03027447598606023\n",
      "   Occupancy loss: 0.5576755342151077\n",
      "   Total loss: 0.9295058081982215\n",
      "\n",
      " 400 training iterations\n",
      "   Hubber loss count: 0.3511959652303873\n",
      "   BCE loss: 0.030174629666304086\n",
      "   Occupancy loss: 0.522945160363991\n",
      "   Total loss: 0.9043157552606824\n",
      "\n",
      " 600 training iterations\n",
      "   Hubber loss count: 0.34152971128300835\n",
      "   BCE loss: 0.031804419991187396\n",
      "   Occupancy loss: 0.49329280147763105\n",
      "   Total loss: 0.8666269327518268\n",
      "\n",
      " 800 training iterations\n",
      "   Hubber loss count: 0.32405416625111344\n",
      "   BCE loss: 0.026669930093855203\n",
      "   Occupancy loss: 0.4724296323652879\n",
      "   Total loss: 0.8231537287102566\n",
      "\n",
      " 1000 training iterations\n",
      "   Hubber loss count: 0.32891242873960486\n",
      "   BCE loss: 0.03126482922694682\n",
      "   Occupancy loss: 0.4787772745416304\n",
      "   Total loss: 0.8389545325081821\n",
      "\n",
      " 1200 training iterations\n",
      "   Hubber loss count: 0.30902532203317623\n",
      "   BCE loss: 0.029317170893339205\n",
      "   Occupancy loss: 0.47671496615304015\n",
      "   Total loss: 0.8150574590795556\n",
      "\n",
      " 1400 training iterations\n",
      "   Hubber loss count: 0.3197276825729887\n",
      "   BCE loss: 0.02896617267785579\n",
      "   Occupancy loss: 0.5560641425907699\n",
      "   Total loss: 0.9047579978416144\n",
      "\n",
      " 1600 training iterations\n",
      "   Hubber loss count: 0.3070969171051102\n",
      "   BCE loss: 0.027796085481155045\n",
      "   Occupancy loss: 0.4859107394315327\n",
      "   Total loss: 0.8208037420177979\n",
      "\n",
      " 1800 training iterations\n",
      "   Hubber loss count: 0.34481388911625177\n",
      "   BCE loss: 0.02782129807777797\n",
      "   Occupancy loss: 0.5048643792313796\n",
      "   Total loss: 0.8774995664254093\n",
      "\n",
      " 2000 training iterations\n",
      "   Hubber loss count: 0.31250035471616894\n",
      "   BCE loss: 0.026100702572298816\n",
      "   Occupancy loss: 0.4628156090886665\n",
      "   Total loss: 0.8014166663771343\n",
      "\n",
      " 2200 training iterations\n",
      "   Hubber loss count: 0.29655128924844765\n",
      "   BCE loss: 0.02284767165998967\n",
      "   Occupancy loss: 0.47392881750622673\n",
      "   Total loss: 0.7933277784146641\n",
      "\n",
      " 2400 training iterations\n",
      "   Hubber loss count: 0.2989415637066074\n",
      "   BCE loss: 0.028714923494292128\n",
      "   Occupancy loss: 0.4753123478786632\n",
      "   Total loss: 0.8029688350795627\n",
      "\n",
      " 2600 training iterations\n",
      "   Hubber loss count: 0.30940468267774035\n",
      "   BCE loss: 0.02185459712609861\n",
      "   Occupancy loss: 0.4743870355985907\n",
      "   Total loss: 0.8056463154024296\n",
      "\n",
      " 2800 training iterations\n",
      "   Hubber loss count: 0.32420632485335144\n",
      "   BCE loss: 0.02120400577525404\n",
      "   Occupancy loss: 0.5170069056719706\n",
      "   Total loss: 0.8624172363005761\n",
      "\n",
      " 3000 training iterations\n",
      "   Hubber loss count: 0.3012150326760046\n",
      "   BCE loss: 0.019474408725028546\n",
      "   Occupancy loss: 0.470769971356104\n",
      "   Total loss: 0.791459412757137\n",
      "\n",
      " 3200 training iterations\n",
      "   Hubber loss count: 0.3105103909998255\n",
      "   BCE loss: 0.02314056654515647\n",
      "   Occupancy loss: 0.4890872764651305\n",
      "   Total loss: 0.8227382340101125\n",
      "\n",
      " 3400 training iterations\n",
      "   Hubber loss count: 0.2992792211943523\n",
      "   BCE loss: 0.020833414269036402\n",
      "   Occupancy loss: 0.3728442119075044\n",
      "   Total loss: 0.6929568473708931\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3600 training iterations\n",
      "   Hubber loss count: 0.30132660598096356\n",
      "   BCE loss: 0.028681097493423995\n",
      "   Occupancy loss: 0.4220473834941301\n",
      "   Total loss: 0.7520550869685176\n",
      "\n",
      " 3800 training iterations\n",
      "   Hubber loss count: 0.2952399036530147\n",
      "   BCE loss: 0.02837485776713508\n",
      "   Occupancy loss: 0.47557263313517384\n",
      "   Total loss: 0.7991873945553236\n",
      "\n",
      " 4000 training iterations\n",
      "   Hubber loss count: 0.32084811742928393\n",
      "   BCE loss: 0.019696483773277745\n",
      "   Occupancy loss: 0.4697285172171301\n",
      "   Total loss: 0.8102731184196919\n",
      "\n",
      " 4200 training iterations\n",
      "   Hubber loss count: 0.30123744468762553\n",
      "   BCE loss: 0.02039626359103926\n",
      "   Occupancy loss: 0.44979210712164097\n",
      "   Total loss: 0.7714258154003057\n",
      "\n",
      " 4400 training iterations\n",
      "   Hubber loss count: 0.31543570453389985\n",
      "   BCE loss: 0.018183666267340894\n",
      "   Occupancy loss: 0.43530722732607857\n",
      "   Total loss: 0.7689265981273193\n",
      "\n",
      " 4600 training iterations\n",
      "   Hubber loss count: 0.2618691881790725\n",
      "   BCE loss: 0.01865463956480329\n",
      "   Occupancy loss: 0.33834419556353995\n",
      "   Total loss: 0.6188680233074157\n",
      "\n",
      " 4800 training iterations\n",
      "   Hubber loss count: 0.27387029979772115\n",
      "   BCE loss: 0.020098097234535962\n",
      "   Occupancy loss: 0.43805668726282493\n",
      "   Total loss: 0.7320250842950821\n",
      "training count loss: 0.3126\n",
      "training heatmap loss: 0.0248\n",
      "training occupancy loss: 0.4799\n",
      "\n",
      "validation \n",
      "\n",
      "\n",
      " 0 validation iterations\n",
      "   Hubber loss count: 0.27591229487868535\n",
      "   BCE loss: 0.019820534347539203\n",
      "   Occupancy loss: 0.4428207551148566\n",
      "   Total loss: 0.7385535843410811\n",
      "\n",
      " 200 validation iterations\n",
      "   Hubber loss count: 0.06543986821145768\n",
      "   BCE loss: 0.005518101136635777\n",
      "   Occupancy loss: 0.07630589566383301\n",
      "   Total loss: 0.14726386501192648\n",
      "validation count loss: 0.1487\n",
      "validation heatmap loss: 0.0262\n",
      "validation occupancy loss: 0.4525\n",
      "training time: 1.0h 59m 13s\n",
      "\n",
      "Epoch 3/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss count: 0.30250053336450183\n",
      "   BCE loss: 0.06288033199752456\n",
      "   Occupancy loss: 1.1789219069422028\n",
      "   Total loss: 1.544302772304229\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss count: 0.2837373210366766\n",
      "   BCE loss: 0.0249139695626944\n",
      "   Occupancy loss: 0.5834962947076152\n",
      "   Total loss: 0.8921475853069862\n",
      "\n",
      " 400 training iterations\n",
      "   Hubber loss count: 0.30559806669474854\n",
      "   BCE loss: 0.020433095264199348\n",
      "   Occupancy loss: 0.3816053095537871\n",
      "   Total loss: 0.707636471512735\n",
      "\n",
      " 600 training iterations\n",
      "   Hubber loss count: 0.2476694820520098\n",
      "   BCE loss: 0.021117363784803746\n",
      "   Occupancy loss: 0.5118788972659901\n",
      "   Total loss: 0.7806657431028037\n",
      "\n",
      " 800 training iterations\n",
      "   Hubber loss count: 0.2713725025096377\n",
      "   BCE loss: 0.01644980152559255\n",
      "   Occupancy loss: 0.413708632009755\n",
      "   Total loss: 0.7015309360449853\n",
      "\n",
      " 1000 training iterations\n",
      "   Hubber loss count: 0.26194355395735525\n",
      "   BCE loss: 0.016688118465775448\n",
      "   Occupancy loss: 0.39247708709401496\n",
      "   Total loss: 0.6711087595171457\n",
      "\n",
      " 1200 training iterations\n",
      "   Hubber loss count: 0.24723659763834915\n",
      "   BCE loss: 0.01868178932458479\n",
      "   Occupancy loss: 0.38111733570917045\n",
      "   Total loss: 0.6470357226721044\n",
      "\n",
      " 1400 training iterations\n",
      "   Hubber loss count: 0.26503282038636855\n",
      "   BCE loss: 0.012499788513231868\n",
      "   Occupancy loss: 0.38934580377386935\n",
      "   Total loss: 0.6668784126734697\n",
      "\n",
      " 1600 training iterations\n",
      "   Hubber loss count: 0.2869732410423595\n",
      "   BCE loss: 0.01630085841738155\n",
      "   Occupancy loss: 0.43754242942564436\n",
      "   Total loss: 0.7408165288853854\n",
      "\n",
      " 1800 training iterations\n",
      "   Hubber loss count: 0.2866189072132601\n",
      "   BCE loss: 0.01703794227031444\n",
      "   Occupancy loss: 0.4500370974214812\n",
      "   Total loss: 0.7536939469050558\n",
      "\n",
      " 2000 training iterations\n",
      "   Hubber loss count: 0.24416498873020595\n",
      "   BCE loss: 0.01808067397243927\n",
      "   Occupancy loss: 0.3201559425031284\n",
      "   Total loss: 0.5824016052057737\n",
      "\n",
      " 2200 training iterations\n",
      "   Hubber loss count: 0.2325360740772133\n",
      "   BCE loss: 0.024486444128658334\n",
      "   Occupancy loss: 0.49265870065984757\n",
      "   Total loss: 0.7496812188657191\n",
      "\n",
      " 2400 training iterations\n",
      "   Hubber loss count: 0.24958442754788598\n",
      "   BCE loss: 0.018977435861375225\n",
      "   Occupancy loss: 0.36271235806997104\n",
      "   Total loss: 0.6312742214792322\n",
      "\n",
      " 2600 training iterations\n",
      "   Hubber loss count: 0.2330628520224515\n",
      "   BCE loss: 0.021028089439413766\n",
      "   Occupancy loss: 0.4999040030089869\n",
      "   Total loss: 0.7539949444708522\n",
      "\n",
      " 2800 training iterations\n",
      "   Hubber loss count: 0.23202917731489678\n",
      "   BCE loss: 0.020200999160127773\n",
      "   Occupancy loss: 0.40781094446663374\n",
      "   Total loss: 0.6600411209416583\n",
      "\n",
      " 3000 training iterations\n",
      "   Hubber loss count: 0.23776920647275812\n",
      "   BCE loss: 0.019257060362237347\n",
      "   Occupancy loss: 0.4002208724809973\n",
      "   Total loss: 0.6572471393159928\n",
      "\n",
      " 3200 training iterations\n",
      "   Hubber loss count: 0.24569283648626264\n",
      "   BCE loss: 0.01559741397238123\n",
      "   Occupancy loss: 0.30578880762546584\n",
      "   Total loss: 0.5670790580841096\n",
      "\n",
      " 3400 training iterations\n",
      "   Hubber loss count: 0.23779514937550003\n",
      "   BCE loss: 0.013983896825552698\n",
      "   Occupancy loss: 0.3497246511211214\n",
      "   Total loss: 0.6015036973221741\n",
      "\n",
      " 3600 training iterations\n",
      "   Hubber loss count: 0.2621407195637734\n",
      "   BCE loss: 0.018864184098361814\n",
      "   Occupancy loss: 0.423169877200517\n",
      "   Total loss: 0.7041747808626522\n",
      "\n",
      " 3800 training iterations\n",
      "   Hubber loss count: 0.24153201053882095\n",
      "   BCE loss: 0.014872176847671814\n",
      "   Occupancy loss: 0.3436165925842551\n",
      "   Total loss: 0.6000207799707479\n",
      "\n",
      " 4000 training iterations\n",
      "   Hubber loss count: 0.22708536843399824\n",
      "   BCE loss: 0.0115220256016844\n",
      "   Occupancy loss: 0.3257776441875332\n",
      "   Total loss: 0.5643850382232158\n",
      "\n",
      " 4200 training iterations\n",
      "   Hubber loss count: 0.22822049272458456\n",
      "   BCE loss: 0.01740210375106262\n",
      "   Occupancy loss: 0.3685599287772882\n",
      "   Total loss: 0.6141825252529354\n",
      "\n",
      " 4400 training iterations\n",
      "   Hubber loss count: 0.20709031463114913\n",
      "   BCE loss: 0.01480459884911956\n",
      "   Occupancy loss: 0.44006556447489403\n",
      "   Total loss: 0.6619604779551627\n",
      "\n",
      " 4600 training iterations\n",
      "   Hubber loss count: 0.2372571033486012\n",
      "   BCE loss: 0.016674041280825214\n",
      "   Occupancy loss: 0.4153651330352661\n",
      "   Total loss: 0.6692962776646925\n",
      "\n",
      " 4800 training iterations\n",
      "   Hubber loss count: 0.23742003383587193\n",
      "   BCE loss: 0.01541972756501219\n",
      "   Occupancy loss: 0.4215160237303018\n",
      "   Total loss: 0.6743557851311859\n",
      "training count loss: 0.2489\n",
      "training heatmap loss: 0.0176\n",
      "training occupancy loss: 0.3998\n",
      "\n",
      "validation \n",
      "\n",
      "\n",
      " 0 validation iterations\n",
      "   Hubber loss count: 0.2498668147592578\n",
      "   BCE loss: 0.01503432625955273\n",
      "   Occupancy loss: 0.35940960760504104\n",
      "   Total loss: 0.6243107486238515\n",
      "\n",
      " 200 validation iterations\n",
      "   Hubber loss count: 0.06839781703769905\n",
      "   BCE loss: 0.0029278027502483033\n",
      "   Occupancy loss: 0.056620652980652234\n",
      "   Total loss: 0.1279462727685996\n",
      "validation count loss: 0.1730\n",
      "validation heatmap loss: 0.0087\n",
      "validation occupancy loss: 0.2126\n",
      "training time: 2.0h 58m 26s\n",
      "\n",
      "Epoch 4/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss count: 0.2814712541066991\n",
      "   BCE loss: 0.0207684263573668\n",
      "   Occupancy loss: 0.5798893423811546\n",
      "   Total loss: 0.8821290228452205\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss count: 0.23252443059411632\n",
      "   BCE loss: 0.010991916580126524\n",
      "   Occupancy loss: 0.34819639769505356\n",
      "   Total loss: 0.5917127448692964\n",
      "\n",
      " 400 training iterations\n",
      "   Hubber loss count: 0.23997219213167167\n",
      "   BCE loss: 0.01741737718255171\n",
      "   Occupancy loss: 0.41815388964697997\n",
      "   Total loss: 0.6755434589612034\n",
      "\n",
      " 600 training iterations\n",
      "   Hubber loss count: 0.22957518072533464\n",
      "   BCE loss: 0.019193576956504255\n",
      "   Occupancy loss: 0.3828878982301277\n",
      "   Total loss: 0.6316566559119666\n",
      "\n",
      " 800 training iterations\n",
      "   Hubber loss count: 0.22205492438309313\n",
      "   BCE loss: 0.016681544790145872\n",
      "   Occupancy loss: 0.34088453409767155\n",
      "   Total loss: 0.5796210032709106\n",
      "\n",
      " 1000 training iterations\n",
      "   Hubber loss count: 0.24655286928478623\n",
      "   BCE loss: 0.024256453003766713\n",
      "   Occupancy loss: 0.45116313253981416\n",
      "   Total loss: 0.721972454828367\n",
      "\n",
      " 1200 training iterations\n",
      "   Hubber loss count: 0.21970238326964553\n",
      "   BCE loss: 0.014050200911998944\n",
      "   Occupancy loss: 0.3510419341279661\n",
      "   Total loss: 0.5847945183096106\n",
      "\n",
      " 1400 training iterations\n",
      "   Hubber loss count: 0.2276731660905352\n",
      "   BCE loss: 0.014584563121064877\n",
      "   Occupancy loss: 0.4249103687352045\n",
      "   Total loss: 0.6671680979468045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1600 training iterations\n",
      "   Hubber loss count: 0.2151911576951662\n",
      "   BCE loss: 0.016426464103831882\n",
      "   Occupancy loss: 0.36938606342764496\n",
      "   Total loss: 0.601003685226643\n",
      "\n",
      " 1800 training iterations\n",
      "   Hubber loss count: 0.20847495741369618\n",
      "   BCE loss: 0.01490282903570713\n",
      "   Occupancy loss: 0.32843506848727166\n",
      "   Total loss: 0.551812854936675\n",
      "\n",
      " 2000 training iterations\n",
      "   Hubber loss count: 0.22850349172211484\n",
      "   BCE loss: 0.01675819189278722\n",
      "   Occupancy loss: 0.3222187733613602\n",
      "   Total loss: 0.5674804569762623\n",
      "\n",
      " 2200 training iterations\n",
      "   Hubber loss count: 0.23399687675788805\n",
      "   BCE loss: 0.01673563770783961\n",
      "   Occupancy loss: 0.5221277261669933\n",
      "   Total loss: 0.772860240632721\n",
      "\n",
      " 2400 training iterations\n",
      "   Hubber loss count: 0.2222157274211162\n",
      "   BCE loss: 0.012384946096502997\n",
      "   Occupancy loss: 0.46295713656184295\n",
      "   Total loss: 0.6975578100794622\n",
      "\n",
      " 2600 training iterations\n",
      "   Hubber loss count: 0.2067359816446629\n",
      "   BCE loss: 0.01624083960899883\n",
      "   Occupancy loss: 0.27637971045834414\n",
      "   Total loss: 0.4993565317120059\n",
      "\n",
      " 2800 training iterations\n",
      "   Hubber loss count: 0.23140567374650908\n",
      "   BCE loss: 0.014622913886552244\n",
      "   Occupancy loss: 0.45516245800840655\n",
      "   Total loss: 0.7011910456414678\n",
      "\n",
      " 3000 training iterations\n",
      "   Hubber loss count: 0.22179771983080213\n",
      "   BCE loss: 0.01331615967832801\n",
      "   Occupancy loss: 0.412045102823468\n",
      "   Total loss: 0.6471589823325982\n",
      "\n",
      " 3200 training iterations\n",
      "   Hubber loss count: 0.21024663609164973\n",
      "   BCE loss: 0.015142571129351756\n",
      "   Occupancy loss: 0.4129966493159924\n",
      "   Total loss: 0.6383858565369939\n",
      "\n",
      " 3400 training iterations\n",
      "   Hubber loss count: 0.20004685418969348\n",
      "   BCE loss: 0.01404678657080624\n",
      "   Occupancy loss: 0.32925242054732917\n",
      "   Total loss: 0.5433460613078289\n",
      "\n",
      " 3600 training iterations\n",
      "   Hubber loss count: 0.1963505344410583\n",
      "   BCE loss: 0.012432390288571453\n",
      "   Occupancy loss: 0.4037791460702078\n",
      "   Total loss: 0.6125620707998376\n",
      "\n",
      " 3800 training iterations\n",
      "   Hubber loss count: 0.2412442345286873\n",
      "   BCE loss: 0.01794751153687614\n",
      "   Occupancy loss: 0.36449732075315117\n",
      "   Total loss: 0.6236890668187146\n",
      "\n",
      " 4000 training iterations\n",
      "   Hubber loss count: 0.2118268682321065\n",
      "   BCE loss: 0.01458215178011866\n",
      "   Occupancy loss: 0.36350164002255475\n",
      "   Total loss: 0.5899106600347799\n",
      "\n",
      " 4200 training iterations\n",
      "   Hubber loss count: 0.22089788635777247\n",
      "   BCE loss: 0.014600507410020499\n",
      "   Occupancy loss: 0.30709596868662703\n",
      "   Total loss: 0.54259436245442\n",
      "\n",
      " 4400 training iterations\n",
      "   Hubber loss count: 0.1823259442363248\n",
      "   BCE loss: 0.015480495034806746\n",
      "   Occupancy loss: 0.35695890889903814\n",
      "   Total loss: 0.5547653481701698\n",
      "\n",
      " 4600 training iterations\n",
      "   Hubber loss count: 0.19339537719865488\n",
      "   BCE loss: 0.015314799000767904\n",
      "   Occupancy loss: 0.35883186231673414\n",
      "   Total loss: 0.5675420385161569\n",
      "\n",
      " 4800 training iterations\n",
      "   Hubber loss count: 0.23757900986587926\n",
      "   BCE loss: 0.015564947234970124\n",
      "   Occupancy loss: 0.4053752497002262\n",
      "   Total loss: 0.6585192068010757\n",
      "training count loss: 0.2196\n",
      "training heatmap loss: 0.0156\n",
      "training occupancy loss: 0.3730\n",
      "\n",
      "validation \n",
      "\n",
      "\n",
      " 0 validation iterations\n",
      "   Hubber loss count: 0.22650374711668744\n",
      "   BCE loss: 0.013240006241760026\n",
      "   Occupancy loss: 0.37370654519268837\n",
      "   Total loss: 0.6134502985511359\n",
      "\n",
      " 200 validation iterations\n",
      "   Hubber loss count: 0.04495070091044901\n",
      "   BCE loss: 0.002530067573975622\n",
      "   Occupancy loss: 0.05483377395926482\n",
      "   Total loss: 0.10231454244368945\n",
      "validation count loss: 0.1135\n",
      "validation heatmap loss: 0.0074\n",
      "validation occupancy loss: 0.1618\n",
      "training time: 3.0h 56m 56s\n",
      "\n",
      "Epoch 5/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss count: 0.2519605578353069\n",
      "   BCE loss: 0.01745332780314903\n",
      "   Occupancy loss: 0.4075499225577003\n",
      "   Total loss: 0.6769638081961562\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss count: 0.21706056232896348\n",
      "   BCE loss: 0.013333614615333151\n",
      "   Occupancy loss: 0.4099818203313278\n",
      "   Total loss: 0.6403759972756244\n",
      "\n",
      " 400 training iterations\n",
      "   Hubber loss count: 0.1867712367688255\n",
      "   BCE loss: 0.014766527918140822\n",
      "   Occupancy loss: 0.32721449536767555\n",
      "   Total loss: 0.5287522600546419\n",
      "\n",
      " 600 training iterations\n",
      "   Hubber loss count: 0.19516019958909833\n",
      "   BCE loss: 0.010901685583664935\n",
      "   Occupancy loss: 0.3043367300076306\n",
      "   Total loss: 0.5103986151803939\n",
      "\n",
      " 800 training iterations\n",
      "   Hubber loss count: 0.17873682264520402\n",
      "   BCE loss: 0.01146555492811362\n",
      "   Occupancy loss: 0.34288052466463964\n",
      "   Total loss: 0.5330829022379573\n",
      "\n",
      " 1000 training iterations\n",
      "   Hubber loss count: 0.18301162454676456\n",
      "   BCE loss: 0.01387873457686823\n",
      "   Occupancy loss: 0.319644205895855\n",
      "   Total loss: 0.5165345650194878\n",
      "\n",
      " 1200 training iterations\n",
      "   Hubber loss count: 0.1889887964497807\n",
      "   BCE loss: 0.014244387449449385\n",
      "   Occupancy loss: 0.3882625675661504\n",
      "   Total loss: 0.5914957514653805\n",
      "\n",
      " 1400 training iterations\n",
      "   Hubber loss count: 0.18988821199873526\n",
      "   BCE loss: 0.014233736270967563\n",
      "   Occupancy loss: 0.30057741713048547\n",
      "   Total loss: 0.5046993654001883\n",
      "\n",
      " 1600 training iterations\n",
      "   Hubber loss count: 0.19187340868708475\n",
      "   BCE loss: 0.025257202337229098\n",
      "   Occupancy loss: 0.37449089263964636\n",
      "   Total loss: 0.5916215036639603\n",
      "\n",
      " 1800 training iterations\n",
      "   Hubber loss count: 0.2028415422811888\n",
      "   BCE loss: 0.014994816488289814\n",
      "   Occupancy loss: 0.3569561755680873\n",
      "   Total loss: 0.5747925343375659\n",
      "\n",
      " 2000 training iterations\n",
      "   Hubber loss count: 0.17431175383440267\n",
      "   BCE loss: 0.01284744993667548\n",
      "   Occupancy loss: 0.36152127098233655\n",
      "   Total loss: 0.5486804747534146\n",
      "\n",
      " 2200 training iterations\n",
      "   Hubber loss count: 0.19934298433098693\n",
      "   BCE loss: 0.014064549434397649\n",
      "   Occupancy loss: 0.351434464071404\n",
      "   Total loss: 0.5648419978367886\n",
      "\n",
      " 2400 training iterations\n",
      "   Hubber loss count: 0.1938986258035983\n",
      "   BCE loss: 0.013911775230715896\n",
      "   Occupancy loss: 0.33503395829989574\n",
      "   Total loss: 0.5428443593342099\n",
      "\n",
      " 2600 training iterations\n",
      "   Hubber loss count: 0.19414288915548453\n",
      "   BCE loss: 0.01914454760527765\n",
      "   Occupancy loss: 0.3050481510442709\n",
      "   Total loss: 0.5183355878050331\n",
      "\n",
      " 2800 training iterations\n",
      "   Hubber loss count: 0.18581705104410332\n",
      "   BCE loss: 0.01605499056753453\n",
      "   Occupancy loss: 0.33555472315309653\n",
      "   Total loss: 0.5374267647647344\n",
      "\n",
      " 3000 training iterations\n",
      "   Hubber loss count: 0.18653394085392447\n",
      "   BCE loss: 0.015067049061503636\n",
      "   Occupancy loss: 0.31714000096357503\n",
      "   Total loss: 0.5187409908790032\n",
      "\n",
      " 3200 training iterations\n",
      "   Hubber loss count: 0.22020008995668344\n",
      "   BCE loss: 0.01217881025079327\n",
      "   Occupancy loss: 0.25970087570464334\n",
      "   Total loss: 0.49207977591212004\n",
      "\n",
      " 3400 training iterations\n",
      "   Hubber loss count: 0.1874013026484646\n",
      "   BCE loss: 0.013505957004476005\n",
      "   Occupancy loss: 0.332197922838301\n",
      "   Total loss: 0.5331051824912416\n",
      "\n",
      " 3600 training iterations\n",
      "   Hubber loss count: 0.16948265087424688\n",
      "   BCE loss: 0.012667883673536178\n",
      "   Occupancy loss: 0.328230357732197\n",
      "   Total loss: 0.5103808922799801\n",
      "\n",
      " 3800 training iterations\n",
      "   Hubber loss count: 0.19805124360359122\n",
      "   BCE loss: 0.01349695093540184\n",
      "   Occupancy loss: 0.2712658896558513\n",
      "   Total loss: 0.4828140841948444\n",
      "\n",
      " 4000 training iterations\n",
      "   Hubber loss count: 0.1802083220138733\n",
      "   BCE loss: 0.01057724952107091\n",
      "   Occupancy loss: 0.3056735443838192\n",
      "   Total loss: 0.49645911591876346\n",
      "\n",
      " 4200 training iterations\n",
      "   Hubber loss count: 0.18069634926583134\n",
      "   BCE loss: 0.015839660658229587\n",
      "   Occupancy loss: 0.37184243754866614\n",
      "   Total loss: 0.568378447472727\n",
      "\n",
      " 4400 training iterations\n",
      "   Hubber loss count: 0.2029409140747158\n",
      "   BCE loss: 0.01511751244609166\n",
      "   Occupancy loss: 0.3090751035009491\n",
      "   Total loss: 0.5271335300217566\n",
      "\n",
      " 4600 training iterations\n",
      "   Hubber loss count: 0.1782551842812048\n",
      "   BCE loss: 0.0131942221250863\n",
      "   Occupancy loss: 0.34147599627404224\n",
      "   Total loss: 0.5329254026803334\n",
      "\n",
      " 4800 training iterations\n",
      "   Hubber loss count: 0.17889591349994804\n",
      "   BCE loss: 0.017308315413789598\n",
      "   Occupancy loss: 0.27999362886479434\n",
      "   Total loss: 0.47619785777853196\n",
      "training count loss: 0.1905\n",
      "training heatmap loss: 0.0146\n",
      "training occupancy loss: 0.3376\n",
      "\n",
      "validation \n",
      "\n",
      "\n",
      " 0 validation iterations\n",
      "   Hubber loss count: 0.19103282527216775\n",
      "   BCE loss: 0.018243273548135434\n",
      "   Occupancy loss: 0.3463001789317946\n",
      "   Total loss: 0.5555762777520978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 200 validation iterations\n",
      "   Hubber loss count: 0.03616781456433476\n",
      "   BCE loss: 0.0030098976596767946\n",
      "   Occupancy loss: 0.05497546314769307\n",
      "   Total loss: 0.09415317537170462\n",
      "validation count loss: 0.1056\n",
      "validation heatmap loss: 0.0182\n",
      "validation occupancy loss: 0.2257\n",
      "training time: 4.0h 54m 20s\n",
      "\n",
      "Epoch 6/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss count: 0.2574152492827723\n",
      "   BCE loss: 0.04727245893592292\n",
      "   Occupancy loss: 0.5441172270774356\n",
      "   Total loss: 0.8488049352961309\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss count: 0.2024905059830723\n",
      "   BCE loss: 0.020374773639134813\n",
      "   Occupancy loss: 0.36073574602262065\n",
      "   Total loss: 0.5836010256448277\n",
      "\n",
      " 400 training iterations\n",
      "   Hubber loss count: 0.20838511570680002\n",
      "   BCE loss: 0.01392510463383745\n",
      "   Occupancy loss: 0.3455769969121465\n",
      "   Total loss: 0.567887217252784\n",
      "\n",
      " 600 training iterations\n",
      "   Hubber loss count: 0.18901092903207853\n",
      "   BCE loss: 0.013914913931440422\n",
      "   Occupancy loss: 0.2931544673007899\n",
      "   Total loss: 0.4960803102643089\n",
      "\n",
      " 800 training iterations\n",
      "   Hubber loss count: 0.1447024743969662\n",
      "   BCE loss: 0.013751728075599889\n",
      "   Occupancy loss: 0.33291364338986423\n",
      "   Total loss: 0.4913678458624303\n",
      "\n",
      " 1000 training iterations\n",
      "   Hubber loss count: 0.14540959380913007\n",
      "   BCE loss: 0.012669460372299704\n",
      "   Occupancy loss: 0.32434538114965894\n",
      "   Total loss: 0.4824244353310887\n",
      "\n",
      " 1200 training iterations\n",
      "   Hubber loss count: 0.15473455203432274\n",
      "   BCE loss: 0.014250940577645992\n",
      "   Occupancy loss: 0.4051558831172406\n",
      "   Total loss: 0.5741413757292093\n",
      "\n",
      " 1400 training iterations\n",
      "   Hubber loss count: 0.16967468703628388\n",
      "   BCE loss: 0.011356581125158508\n",
      "   Occupancy loss: 0.31644148759130125\n",
      "   Total loss: 0.49747275575274363\n",
      "\n",
      " 1600 training iterations\n",
      "   Hubber loss count: 0.15549910361285746\n",
      "   BCE loss: 0.01688943080205868\n",
      "   Occupancy loss: 0.28980552590488784\n",
      "   Total loss: 0.462194060319804\n",
      "\n",
      " 1800 training iterations\n",
      "   Hubber loss count: 0.19725182678713774\n",
      "   BCE loss: 0.011761694929280964\n",
      "   Occupancy loss: 0.2296301578778742\n",
      "   Total loss: 0.4386436795942929\n",
      "\n",
      " 2000 training iterations\n",
      "   Hubber loss count: 0.19182584354693488\n",
      "   BCE loss: 0.011531153771334155\n",
      "   Occupancy loss: 0.33137202513929326\n",
      "   Total loss: 0.5347290224575623\n",
      "\n",
      " 2200 training iterations\n",
      "   Hubber loss count: 0.17116505030083828\n",
      "   BCE loss: 0.010665228406274084\n",
      "   Occupancy loss: 0.30216532553102615\n",
      "   Total loss: 0.48399560423813853\n",
      "\n",
      " 2400 training iterations\n",
      "   Hubber loss count: 0.1764539144193492\n",
      "   BCE loss: 0.013860908105180326\n",
      "   Occupancy loss: 0.32452222413480536\n",
      "   Total loss: 0.5148370466593348\n",
      "\n",
      " 2600 training iterations\n",
      "   Hubber loss count: 0.196394565587853\n",
      "   BCE loss: 0.014682811850610553\n",
      "   Occupancy loss: 0.3019476894371663\n",
      "   Total loss: 0.5130250668756299\n",
      "\n",
      " 2800 training iterations\n",
      "   Hubber loss count: 0.19140649242363178\n",
      "   BCE loss: 0.012698720930017308\n",
      "   Occupancy loss: 0.24697016141535638\n",
      "   Total loss: 0.4510753747690055\n",
      "\n",
      " 3000 training iterations\n",
      "   Hubber loss count: 0.15992810712239455\n",
      "   BCE loss: 0.010845582276809883\n",
      "   Occupancy loss: 0.35819022090144126\n",
      "   Total loss: 0.5289639103006457\n",
      "\n",
      " 3200 training iterations\n",
      "   Hubber loss count: 0.16543009151312496\n",
      "   BCE loss: 0.010483442914163098\n",
      "   Occupancy loss: 0.27408459922453704\n",
      "   Total loss: 0.44999813365182506\n",
      "\n",
      " 3400 training iterations\n",
      "   Hubber loss count: 0.16464028780514403\n",
      "   BCE loss: 0.015122304727722583\n",
      "   Occupancy loss: 0.274258865025242\n",
      "   Total loss: 0.45402145755810863\n",
      "\n",
      " 3600 training iterations\n",
      "   Hubber loss count: 0.1653548045259367\n",
      "   BCE loss: 0.013610501415970738\n",
      "   Occupancy loss: 0.27748351445674296\n",
      "   Total loss: 0.4564488203986504\n",
      "\n",
      " 3800 training iterations\n",
      "   Hubber loss count: 0.18181197619843187\n",
      "   BCE loss: 0.01743442459135537\n",
      "   Occupancy loss: 0.3075056269362916\n",
      "   Total loss: 0.5067520277260789\n",
      "\n",
      " 4000 training iterations\n",
      "   Hubber loss count: 0.16559443969291918\n",
      "   BCE loss: 0.018234579535582458\n",
      "   Occupancy loss: 0.32863396009706203\n",
      "   Total loss: 0.5124629793255637\n",
      "\n",
      " 4200 training iterations\n",
      "   Hubber loss count: 0.18558757927722502\n",
      "   BCE loss: 0.017847423949979197\n",
      "   Occupancy loss: 0.39199165324862323\n",
      "   Total loss: 0.5954266564758275\n",
      "\n",
      " 4400 training iterations\n",
      "   Hubber loss count: 0.1574203728058355\n",
      "   BCE loss: 0.00916874200154917\n",
      "   Occupancy loss: 0.2583173213774674\n",
      "   Total loss: 0.424906436184852\n",
      "\n",
      " 4600 training iterations\n",
      "   Hubber loss count: 0.19035318753449937\n",
      "   BCE loss: 0.012736053662642155\n",
      "   Occupancy loss: 0.24705586248028757\n",
      "   Total loss: 0.4501451036774291\n",
      "\n",
      " 4800 training iterations\n",
      "   Hubber loss count: 0.18950721783972685\n",
      "   BCE loss: 0.015519195172124415\n",
      "   Occupancy loss: 0.2352841191779447\n",
      "   Total loss: 0.440310532189796\n",
      "training count loss: 0.1743\n",
      "training heatmap loss: 0.0133\n",
      "training occupancy loss: 0.3059\n",
      "\n",
      "validation \n",
      "\n",
      "\n",
      " 0 validation iterations\n",
      "   Hubber loss count: 0.18436009087301658\n",
      "   BCE loss: 0.01569114829587093\n",
      "   Occupancy loss: 0.2612007835590705\n",
      "   Total loss: 0.461252022727958\n",
      "\n",
      " 200 validation iterations\n",
      "   Hubber loss count: 0.035413445336477514\n",
      "   BCE loss: 0.003785230461391714\n",
      "   Occupancy loss: 0.04078250155379195\n",
      "   Total loss: 0.07998117735166119\n",
      "validation count loss: 0.0723\n",
      "validation heatmap loss: 0.0072\n",
      "validation occupancy loss: 0.1464\n",
      "training time: 5.0h 54m 42s\n",
      "\n",
      "Epoch 7/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss count: 0.15120394120996872\n",
      "   BCE loss: 0.012025463282119753\n",
      "   Occupancy loss: 0.385437118052973\n",
      "   Total loss: 0.5486665225450614\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss count: 0.15789214052754527\n",
      "   BCE loss: 0.00968583253110138\n",
      "   Occupancy loss: 0.3011663558058042\n",
      "   Total loss: 0.46874432886445083\n"
     ]
    }
   ],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_22.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        #continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                             --hyperparameter_set=$hyp_st --output_name=$out  \\\n",
    "                             --dest_folder=$dest_folder\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"1.2V\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 3 - Heatmap + Count + Occupancy<a name=\"1.3T\"></a>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# switch pipeline\n",
    "pipeline = 'Heatmap-Cnt-Occ'\n",
    "\n",
    "# generate model combinations\n",
    "combinations_3 = {'model_architecture': ['UnetCntWRNOccDense'],\n",
    "                   'training_dir': ['training_set_vanilla'],\n",
    "                   'hyperparameter_set': ['E'] }       \n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_3 = pd.DataFrame(combinations_3)\n",
    "                    \n",
    "\n",
    "# create folders for resulting files\n",
    "for row in combinations_3.iterrows():\n",
    "    mdl = row[1]['model_architecture'] + '_ts-' + row[1]['training_dir'].split('_')[-1]                  \n",
    "    if not os.path.exists(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)):\n",
    "        os.makedirs(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_22.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        #continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                             --hyperparameter_set=$hyp_st --output_name=$out  \\\n",
    "                             --dest_folder=$dest_folder\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
