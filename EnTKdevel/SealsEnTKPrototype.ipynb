{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnTK Seals Pipeline notebook.\n",
    "\n",
    "This notebook provides a prototypical implementation of the Seal use case, as it is shown in the [Seal Execution Model](https://docs.google.com/document/d/1E79LfwXG1ZJ1fTQiGsDvSggE6BJSDEqAyHKcCxjqgoY/edit?ts=5af5d13e). Each cell of the notebook creates a necesary component of the pipeline definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from radical.entk import Pipeline, Stage, Task, AppManager, ResourceManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Definition\n",
    "\n",
    "The next cell defines the prototype pipeline for the Seal Use case. The pipeline has two stages, and each stage has a single task.\n",
    "The first stage executes the prediction and the second the detection.\n",
    "\n",
    "What needs to be added is [Stage number 0](https://docs.google.com/document/d/1E79LfwXG1ZJ1fTQiGsDvSggE6BJSDEqAyHKcCxjqgoY/edit?ts=5af5d13e) and the last stage that aggregates the results. Also, the single task in both cases should be broken to multiple tasks based on the number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pipeline(name, stages):  #generate the pipeline of prediction and blob detection\n",
    "\n",
    "    # Create a Pipeline object\n",
    "    p = Pipeline()\n",
    "    p.name = name\n",
    "\n",
    "    for s_cnt in range(stages):\n",
    "\n",
    "\n",
    "        if(s_cnt==0):\n",
    "            # Create a Stage object\n",
    "            s0 = Stage()\n",
    "            s0.name = 'Stage %s'%s_cnt\n",
    "            # Create Task 1, training\n",
    "            t1 = Task()\n",
    "            t1.name = 'Predictor'\n",
    "            t1.pre_exec = ['module load psc_path/1.1',\n",
    "                           'module load slurm/default',\n",
    "                           'module load intel/17.4',\n",
    "                           'module load python3',\n",
    "                           'module load cuda',\n",
    "                           'mkdir -p classified_images/crabeater',\n",
    "                           'mkdir -p classified_images/weddel',\n",
    "                           'mkdir -p classified_images/pack-ice',\n",
    "                           'mkdir -p classified_images/other',\n",
    "                           'source <path_to_env>/pytorchCuda/bin/activate'\n",
    "                          ]\n",
    "            t1.executable = 'python3'   # Assign executable to the task   \n",
    "            # Assign arguments for the task executable\n",
    "            t1.arguments = ['pt_predict.py','-class_names','crabeater','weddel','pack-ice','other']\n",
    "            t1.link_input_data = ['<path_to_model>/nn_model.pth.tar',\n",
    "                                  '<path_to_training_set>/nn_images',\n",
    "                                  '<path_to_test_set>/test_images'\n",
    "                                  ]\n",
    "            t1.upload_input_data = ['pt_predict.py','sealnet_nas_scalable.py']\n",
    "            t1.cpu_reqs = {'processes': 1,'threads_per_process': 1, 'thread_type': 'OpenMP'}\n",
    "            t1.gpu_reqs = {'processes': 1,'threads_per_process': 1, 'thread_type': 'OpenMP'}\n",
    "        \n",
    "            s0.add_tasks(t1)    \n",
    "            # Add Stage to the Pipeline\n",
    "            p.add_stages(s0)\n",
    "        else:\n",
    "            # Create a Stage object\n",
    "            s1 = Stage()\n",
    "            s1.name = 'Stage %s'%s_cnt\n",
    "            # Create Task 2,\n",
    "            t2 = Task()\n",
    "            t2.pre_exec = ['module load psc_path/1.1',\n",
    "                           'module load slurm/default',\n",
    "                           'module load intel/17.4',\n",
    "                           'module load python3',\n",
    "                           'module load cuda',\n",
    "                           'module load opencv',\n",
    "                           'source <path_to_env>/pytorchCuda/bin/activate',\n",
    "                           'mkdir -p blob_detected'\n",
    "                         ]\n",
    "            t2.name = 'Blob_detector'         \n",
    "            t2.executable = ['python3']   # Assign executable to the task   \n",
    "            # Assign arguments for the task executable\n",
    "            t2.arguments = ['blob_detector.py']\n",
    "            t2.upload_input_data = ['blob_detector.py']\n",
    "            t2.link_input_data = ['$Pipeline_%s_Stage_%s_Task_%s/classified_images'%(p.uid, s0.uid, t1.uid)]\n",
    "            t2.download_output_data = ['blob_detected/'] #Download resuting images \n",
    "            t2.cpu_reqs = {'processes': 1,'threads_per_process': 1, 'thread_type': 'OpenMP'}\n",
    "            t2.gpu_reqs = {'processes': 1, 'threads_per_process': 1, 'thread_type': 'OpenMP'}\n",
    "            s1.add_tasks(t2)\n",
    "            # Add Stage to the Pipeline\n",
    "            p.add_stages(s1)\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline generation\n",
    "\n",
    " The pipeline is define and now we create an object. Although now it only has 2 stages think that the number of stages might change based on specifics of the application. Thus, allowing the number of stages to be an input shows that possible extension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = generate_pipeline(name='Pipeline 1', stages=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource description and acquisition\n",
    "\n",
    "We define a dictionary with the following values:\n",
    "```\n",
    "{'resource': the resource to execute the pipeline, e.g. 'xsede.bridges' for Bridges,\n",
    " 'walltime': The amount of time the resources are needed,\n",
    " 'cpus': Number of CPUs needed,\n",
    " 'gpus' : Number of GPUs needed,\n",
    " 'schema' : Way to access the resource without a password. We reccomend gsissh. ,\n",
    " 'project': Project to charge,\n",
    " 'queue' : The queue you submit for example GPU-small\n",
    "    }\n",
    "```\n",
    "\n",
    "After the dictionary is created we acquire the resources by creating a `ResourceManager`\n",
    "\n",
    "---\n",
    "Instructions how to install gsissh on Ubuntu can be found [here](https://github.com/vivek-bala/docs/blob/master/misc/gsissh_setup_stampede_ubuntu_xenial.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {'resource': 'xsede.bridges',\n",
    "             'walltime': 30,\n",
    "             'cpus': 12,\n",
    "             'gpus' : 2,\n",
    "             'schema' : 'gsisshh',\n",
    "             'project': '',\n",
    "             'queue' : 'GPU-small'\n",
    "    }\n",
    "    \n",
    "# Create Resource Manager\n",
    "rman = ResourceManager(res_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "In order to execute the pipeline we create an application manager and assign to it th resource manager previously created. We also assign the generated pipeline.\n",
    "\n",
    "Finally, we request from the application manager to run the application and we wait for it to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Application Manager\n",
    "appman = AppManager(port=32773)\n",
    "\n",
    "# Assign resource manager to the Application Manager\n",
    "appman.resource_manager = rman\n",
    "\n",
    "# Assign the workflow as a set of Pipelines to the Application Manager\n",
    "appman.assign_workflow(set([p]))\n",
    "\n",
    "# Run the Application Manager\n",
    "appman.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
