{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EnTK Seals Pipeline notebook.\n",
    "\n",
    "This notebook provides a prototypical implementation of the Seal use case, as it is shown in the [Seal Execution Model](https://docs.google.com/document/d/1E79LfwXG1ZJ1fTQiGsDvSggE6BJSDEqAyHKcCxjqgoY/edit?ts=5af5d13e). Each cell of the notebook creates a necesary component of the pipeline definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from radical.entk import Pipeline, Stage, Task, AppManager, ResourceManager"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Definition\n",
    "\n",
    "The next cell defines the prototype pipeline for the Seal Use case. The pipeline has two stages, and each stage has a single task.\n",
    "The first stage executes the prediction and the second the detection.\n",
    "\n",
    "What needs to be added is [Stage number 0](https://docs.google.com/document/d/1E79LfwXG1ZJ1fTQiGsDvSggE6BJSDEqAyHKcCxjqgoY/edit?ts=5af5d13e) and the last stage that aggregates the results. Also, the single task in both cases should be broken to multiple tasks based on the number of images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_pipeline(name, stages):  #generate the pipeline of prediction and blob detection\n",
    "\n",
    "    # Create a Pipeline object\n",
    "    p = Pipeline()\n",
    "    p.name = name\n",
    "\n",
    "    for s_cnt in range(stages):\n",
    "\n",
    "\n",
    "        if(s_cnt==0):\n",
    "            # Create a Stage object\n",
    "            s0 = Stage()\n",
    "            s0.name = 'Stage %s'%s_cnt\n",
    "            # Create Task 1, training\n",
    "            t1 = Task()\n",
    "            t1.name = 'Predictor'\n",
    "            t1.pre_exec = ['module load psc_path/1.1',\n",
    "                           'module load slurm/default',\n",
    "                           'module load intel/17.4',\n",
    "                           'module load python3',\n",
    "                           'module load cuda',\n",
    "                           'mkdir -p classified_images/crabeater',\n",
    "                           'mkdir -p classified_images/weddel',\n",
    "                           'mkdir -p classified_images/pack-ice',\n",
    "                           'mkdir -p classified_images/other',\n",
    "                           'source /pylon5/mc3bggp/paraskev/pytorchCuda/bin/activate'\n",
    "                          ]\n",
    "            t1.executable = 'python3'   # Assign executable to the task   \n",
    "            # Assign arguments for the task executable\n",
    "            t1.arguments = ['pt_predict.py','-class_names','crabeater','weddel','pack-ice','other']\n",
    "            t1.link_input_data = ['/pylon5/mc3bggp/paraskev/seal_test/nn_model.pth.tar',\n",
    "                                  '/pylon5/mc3bggp/paraskev/nn_images',\n",
    "                                  '/pylon5/mc3bggp/paraskev/seal_test/test_images'\n",
    "                                  ]\n",
    "            t1.upload_input_data = ['pt_predict.py','sealnet_nas_scalable.py']\n",
    "            t1.cpu_reqs = {'processes': 1,'threads_per_process': 1, 'thread_type': 'OpenMP'}\n",
    "            t1.gpu_reqs = {'processes': 1,'threads_per_process': 1, 'thread_type': 'OpenMP'}\n",
    "        \n",
    "            s0.add_tasks(t1)    \n",
    "            # Add Stage to the Pipeline\n",
    "            p.add_stages(s0)\n",
    "        else:\n",
    "            # Create a Stage object\n",
    "            s1 = Stage()\n",
    "            s1.name = 'Stage %s'%s_cnt\n",
    "            # Create Task 2,\n",
    "            t2 = Task()\n",
    "            t2.pre_exec = ['module load psc_path/1.1',\n",
    "                           'module load slurm/default',\n",
    "                           'module load intel/17.4',\n",
    "                           'module load python3',\n",
    "                           'module load cuda',\n",
    "                           'module load opencv',\n",
    "                           'source /pylon5/mc3bggp/paraskev/pytorchCuda/bin/activate',\n",
    "                           'mkdir -p blob_detected'\n",
    "                         ]\n",
    "            t2.name = 'Blob_detector'         \n",
    "            t2.executable = ['python3']   # Assign executable to the task   \n",
    "            # Assign arguments for the task executable\n",
    "            t2.arguments = ['blob_detector.py']\n",
    "            t2.upload_input_data = ['blob_detector.py']\n",
    "            t2.link_input_data = ['$Pipeline_%s_Stage_%s_Task_%s/classified_images'%(p.uid, s0.uid, t1.uid)]\n",
    "            t2.download_output_data = ['blob_detected/'] #Download resuting images \n",
    "            t2.cpu_reqs = {'processes': 1,'threads_per_process': 1, 'thread_type': 'OpenMP'}\n",
    "            t2.gpu_reqs = {'processes': 1, 'threads_per_process': 1, 'thread_type': 'OpenMP'}\n",
    "            s1.add_tasks(t2)\n",
    "            # Add Stage to the Pipeline\n",
    "            p.add_stages(s1)\n",
    "\n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline generation\n",
    "\n",
    " The pipeline is define and now we create an object. Although now it only has 2 stages think that the number of stages might change based on specifics of the application. Thus, allowing the number of stages to be an input shows that possible extension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = generate_pipeline(name='Pipeline 1', stages=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resource description and acquisition\n",
    "\n",
    "We define a dictionary with the following values:\n",
    "```\n",
    "{'resource': the resource to execute the pipeline, e.g. 'xsede.bridges' for Bridges,\n",
    " 'walltime': The amount of time the resources are needed,\n",
    " 'cpus': Number of CPUs needed,\n",
    " 'gpus' : Number of GPUs needed,\n",
    " 'schema' : Way to access the resource without a password. We reccomend gsissh. ,\n",
    " 'project': Project to charge,\n",
    " 'queue' : The queue you submit for example GPU-small\n",
    "    }\n",
    "```\n",
    "\n",
    "After the dictionary is created we acquire the resources by creating a `ResourceManager`\n",
    "\n",
    "---\n",
    "Instructions how to install gsissh on Ubuntu can be found [here](https://github.com/vivek-bala/docs/blob/master/misc/gsissh_setup_stampede_ubuntu_xenial.sh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dict = {'resource': 'xsede.bridges',\n",
    "             'walltime': 30,\n",
    "             'cpus': 12,\n",
    "             'gpus' : 2,\n",
    "             'schema' : 'gsisshh',\n",
    "             'project': 'mc3bggp',\n",
    "             'queue' : 'GPU-small'\n",
    "    }\n",
    "    \n",
    "# Create Resource Manager\n",
    "rman = ResourceManager(res_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution\n",
    "\n",
    "In order to execute the pipeline we create an application manager and assign to it th resource manager previously created. We also assign the generated pipeline.\n",
    "\n",
    "Finally, we request from the application manager to run the application and we wait for it to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[94mnew session: \u001b[39m\u001b[0m[rp.session.js-157-203.jetstream-cloud.org.iparask.017696.0004]\u001b[39m\u001b[0m\u001b[94m   \\\n",
      "database   : \u001b[39m\u001b[0m[mongodb://giannis:giannis@149.165.168.81:32768/entk]\u001b[39m\u001b[0m\u001b[92m            ok\n",
      "\u001b[39m\u001b[0m\u001b[94mcreate pilot manager\u001b[39m\u001b[0m\u001b[92m                                                          ok\n",
      "\u001b[39m\u001b[0m\u001b[94msubmit 1 pilot(s)\n",
      "        \u001b[39m\u001b[0m.\u001b[39m\u001b[0m\u001b[92m                                                                     ok\n",
      "\u001b[39m\u001b[0mSyncing task radical.entk.task.0000 with state SCHEDULING\n",
      "Synced task radical.entk.task.0000 with state SCHEDULING\n",
      "Syncing task radical.entk.task.0000 with state SCHEDULED\n",
      "Synced task radical.entk.task.0000 with state SCHEDULED\n",
      "\u001b[94mcreate unit manager\u001b[39m\u001b[0m\u001b[92m                                                           ok\n",
      "\u001b[94madd 1 pilot(s)\u001b[39m\u001b[0m\u001b[39m\u001b[0m\u001b[92m                                                                ok\n",
      "\u001b[39m\u001b[0m\u001b[94msubmit 1 unit(s)\n",
      "        \u001b[39m\u001b[0m.\u001b[39m\u001b[0m\u001b[92m                                                                     ok\n",
      "\u001b[39m\u001b[0mSyncing task radical.entk.task.0000 with state SUBMITTING\n",
      "Synced task radical.entk.task.0000 with state SUBMITTING\n",
      "Syncing task radical.entk.task.0000 with state SUBMITTED\n",
      "Synced task radical.entk.task.0000 with state SUBMITTED\n",
      "\u001b[94msubmit 1 unit(s)\n",
      "        \u001b[39m\u001b[0m.\u001b[39m\u001b[0m\u001b[92m                                                                     ok\n",
      "\u001b[39m\u001b[0mSyncing task radical.entk.task.0000 with state EXECUTED\n",
      "Synced task radical.entk.task.0000 with state EXECUTED\n",
      "Syncing task radical.entk.task.0000 with state DEQUEUEING\n",
      "Synced task radical.entk.task.0000 with state DEQUEUEING\n",
      "Syncing task radical.entk.task.0000 with state DEQUEUED\n",
      "Synced task radical.entk.task.0000 with state DEQUEUED\n",
      "Syncing task radical.entk.task.0000 with state DONE\n",
      "Synced task radical.entk.task.0000 with state DONE\n",
      "Syncing task radical.entk.task.0001 with state SCHEDULING\n",
      "Synced task radical.entk.task.0001 with state SCHEDULING\n",
      "Syncing task radical.entk.task.0001 with state SUBMITTING\n",
      "Synced task radical.entk.task.0001 with state SUBMITTING\n",
      "Syncing task radical.entk.task.0001 with state SCHEDULED\n",
      "Synced task radical.entk.task.0001 with state SCHEDULED\n",
      "Syncing task radical.entk.task.0001 with state SUBMITTED\n",
      "Synced task radical.entk.task.0001 with state SUBMITTED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-14 08:32:55,922: radical.entk.wfprocessor.0000: wfprocessor                     : dequeue-thread : ERROR   : Unable to receive message from completed queue: \n",
      "2018-06-14 08:32:55,930: radical.entk.wfprocessor.0000: wfprocessor                     : dequeue-thread : ERROR   : Error in dequeue-thread: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/home/iparask/miniconda2/envs/EnTKGPU/lib/python2.7/site-packages/radical/entk/appman/wfprocessor.py\", line 476, in _dequeue\n",
      "    method_frame, header_frame, body = mq_channel.basic_get(queue=self._completed_queue[0])\n",
      "  File \"/home/iparask/miniconda2/envs/EnTKGPU/lib/python2.7/site-packages/pika/adapters/blocking_connection.py\", line 2032, in basic_get\n",
      "    no_ack=no_ack)\n",
      "  File \"/home/iparask/miniconda2/envs/EnTKGPU/lib/python2.7/site-packages/pika/channel.py\", line 360, in basic_get\n",
      "    self._validate_channel_and_callback(callback)\n",
      "  File \"/home/iparask/miniconda2/envs/EnTKGPU/lib/python2.7/site-packages/pika/channel.py\", line 1362, in _validate_channel_and_callback\n",
      "    raise exceptions.ChannelClosed()\n",
      "ChannelClosed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread dequeue-thread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/iparask/miniconda2/envs/EnTKGPU/lib/python2.7/threading.py\", line 801, in __bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/home/iparask/miniconda2/envs/EnTKGPU/lib/python2.7/threading.py\", line 754, in run\n",
      "    self.__target(*self.__args, **self.__kwargs)\n",
      "  File \"/home/iparask/miniconda2/envs/EnTKGPU/lib/python2.7/site-packages/radical/entk/appman/wfprocessor.py\", line 596, in _dequeue\n",
      "    raise Error(text=ex)\n",
      "Error: Error: \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syncing task radical.entk.task.0001 with state EXECUTED\n",
      "Synced task radical.entk.task.0001 with state EXECUTED\n",
      "Syncing task radical.entk.task.0001 with state DEQUEUEING\n",
      "Synced task radical.entk.task.0001 with state DEQUEUEING\n",
      "Syncing task radical.entk.task.0001 with state DEQUEUED\n",
      "Synced task radical.entk.task.0001 with state DEQUEUED\n",
      "Syncing task radical.entk.task.0001 with state DONE\n",
      "Synced task radical.entk.task.0001 with state DONE\n",
      "\u001b[94mwait for 1 pilot(s)\n",
      "        \u001b[39m\u001b[0mO\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0|\u001b[39m\u001b[0/\u001b[39m\u001b[0-\u001b[39m\u001b[0\\\u001b[39m\u001b[0 \u001b[39m\u001b[0m\u001b[92m                                                                     ok\n",
      "\u001b[39m\u001b[0m\u001b[94mclosing session rp.session.js-157-203.jetstream-cloud.org.iparask.017696.0004\u001b[39m\u001b[0m\u001b[94m  \\\n",
      "close pilot manager\u001b[39m\u001b[0m\u001b[94m                                                            \\\n",
      "wait for 1 pilot(s)\n",
      "        \u001b[39m\u001b[0mO\u001b[39m\u001b[0 \u001b[39m\u001b[0m\u001b[93m                                                                timeout\n",
      "\u001b[39m\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-06-14 08:33:28,443: radical.entk.resource_manager.0000: MainProcess                     : pmgr.0000.subscriber._state_sub_cb: ERROR   : Pilot has completed\n"
     ]
    }
   ],
   "source": [
    "# Create Application Manager\n",
    "appman = AppManager(port=32773)\n",
    "\n",
    "# Assign resource manager to the Application Manager\n",
    "appman.resource_manager = rman\n",
    "\n",
    "# Assign the workflow as a set of Pipelines to the Application Manager\n",
    "appman.assign_workflow(set([p]))\n",
    "\n",
    "# Run the Application Manager\n",
    "appman.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
