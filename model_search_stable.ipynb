{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seal Detection Pipeline\n",
    "---\n",
    "\n",
    "This jupyter notebook will go through assembling the main components of a complete pipeline for counting seals in high-resolution satellite imagery (figure 1, steps 3 and 4) and show some experimental results with different pipeline designs. The ultimate goal of this pipeline is to perform a pan-Antarctic pack-ice seal census. ** Running this code will require input satellite imagery and at least one GPU with >8GB of memory **\n",
    "\n",
    "<br>\n",
    "\n",
    "<img src=\"jupyter_notebook_images/Base Pipeline.png\">\n",
    "\n",
    "<br>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "---\n",
    "* [Getting started](#intro)\n",
    "    * [Setup](#setup)\n",
    "    * [Visualize training set](#vis_imgs)\n",
    "* [Pipeline 1 - Seal haulout detector](#1)\n",
    "    * [Training](#1T)\n",
    "    * [Validation](#1V)\n",
    "    * [Ablation experiment](#1A)\n",
    "* [Pipeline 1.1 - Seal haulout detector + count](#1.1)\n",
    "    * [Training](#1.1T)\n",
    "    * [Validation](#1.1V)\n",
    "    * [Ablation experiment / testing](#1.1A)\n",
    "* [Pipeline 1.2 - Seal haulout detector + single seal detector](#1.2)\n",
    "    * [Training](#1.2T)\n",
    "    * [Validation](#1.2V)\n",
    "    * [Testing](#1.2A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started<a name=\"intro\"></a>\n",
    "---\n",
    "\n",
    "If you followed the *training_set_generation* jupyter notebook (also present in this repo), you should have training sets generated and hyperparameter sets to try out, and be ready to search for a best performing seal detection pipeline.  Output files in this repository are organized as follows: *'./{dest_folder}/{pipeline}/{model_settings}/{model_settings}_{file}'*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment<a name=\"setup\"></a>\n",
    "\n",
    "Before training and validating model/hyperparameter combinations inside the pipelines, we need to load the required python modules and a few global variables. Running this script will also display a list of training classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib as mpl\n",
    "from functools import reduce\n",
    "from utils.model_library import * \n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi']= 400\n",
    "\n",
    "# destination folder for saved models and model stats\n",
    "dest_folder = 'saved_models_stable'\n",
    "\n",
    "# save class names\n",
    "class_names = sorted([subdir for subdir in os.listdir('./training_sets/training_set_vanilla/training')])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training images (Optional)<a name=\"vis_imgs\"></a>\n",
    "\n",
    "To get a better sense for what the training set is like, the next cell will display a few random images from the training classes. Displayed images are extracted from a pool of ~70000 training images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store images\n",
    "images = []\n",
    "\n",
    "# loop over labels\n",
    "for label in class_names:\n",
    "    for path, _, files in os.walk('./training_sets/training_set_vanilla/training/{}'.format(label)):\n",
    "        files = np.random.choice(files, 5)\n",
    "        for filename in files:\n",
    "            images.append(np.asarray(Image.open(os.path.join(path, filename))))\n",
    "\n",
    "images = np.array(images)\n",
    "\n",
    "# display images \n",
    "ncols=len(class_names)\n",
    "nindex, height, width, intensity = images.shape\n",
    "nrows = nindex // ncols\n",
    "# check if rows and columns can fit the number of images\n",
    "assert nindex == nrows * ncols\n",
    "result = (images.reshape(nrows, ncols, height, width, intensity)\n",
    "          .swapaxes(1,2)\n",
    "          .reshape(height*nrows, width*ncols, intensity))\n",
    "\n",
    "plt.imshow(result)\n",
    "cur_axes = plt.gca()\n",
    "cur_axes.axes.get_xaxis().set_visible(False)\n",
    "cur_axes.axes.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1 - haulout detector<a name=\"1\"></a>\n",
    "---\n",
    "\n",
    "The simplest pipeline we can use is one that just uses an object classification step to find seal haulouts or penguins colonies. The obvious downside for this approach is that we often have more than one seal in a haulout, which is hardly usefull if we are looking for a count. However, we will use this as a 'pre-preocessing' step, where we narrow down the totality of patches to the subset where the haulout detection CNN flagged groups of seals. To validate the usefulness of this preprocessing step we can compare results obtained with the full pipeline (i.e. haul out detector + count) to one that simply tries to count on all tiles. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"1T\"></a>\n",
    "\n",
    "The first step to find a best performing model is to train different model setups using our training set. To keep track of which combinations we have tried, how well they performed and the specifics of each model setup, we will store results in folders (under './{dest_folder}') named after each specific model combination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch pipeline\n",
    "pipeline = 'Pipeline1'\n",
    "\n",
    "# generate model combinations\n",
    "combinations_1 = {'model_architecture': ['Resnet18', 'Resnet34', 'Resnet50', 'Squeezenet11',\n",
    "                                         'Alexnet', 'NasnetA', 'Densenet121','Densenet169', \n",
    "                                         'VGG16'],\n",
    "                  'training_dir': ['training_set_vanilla'] * 9,\n",
    "                  'hyperparameter_set': ['B'] * 9,\n",
    "                  'cv_weights': ['NO'] * 9}\n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_1 = pd.DataFrame(combinations_1)\n",
    "                    \n",
    "# create folders for resulting files\n",
    "for row in combinations_1.iterrows():\n",
    "    mdl = row[1]['model_architecture'] + '_ts-' + row[1]['training_dir'].split('_')[-1]              \n",
    "    if not os.path.exists(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)):\n",
    "        os.makedirs(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then provide model combinations created above as arguments to the training script, *train_sealnet.py*. A list of required arguments can be displayed by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run train_sealnet.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over combinations\n",
    "\n",
    "for row in combinations_1.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st, cv_wgt = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                                  row[1]['hyperparameter_set'], row[1]['cv_weights']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                             --hyperparameter_set=$hyp_st --cv_weights=$cv_wgt \\\n",
    "                             --output_name=$out --pipeline=$pipeline \\\n",
    "                             --dest_folder=$dest_folder\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"1V\"></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now load the models we just trained to get measurements of precision and recall for all positive classes. For every model combination we trained, *validate_sealnet.py* will run a full validation round and write given label/correct label pairs to a .csv file. The resulting .csv file is then imported by an R script, *plot_confusion_matrix.R*, which saves a confusion matrix figure and a .csv spreadsheet with precision and recall for all classes of interest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame to combine all metrics \n",
    "comb_prec_recall = pd.DataFrame()\n",
    "pipeline = 'Pipeline1'\n",
    "\n",
    "# iterate over trained models\n",
    "for row in combinations_1.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st, cv_wgt = row[1]['training_dir'], row[1]['model_architecture'],\\\n",
    "                                  row[1]['hyperparameter_set'], row[1]['cv_weights']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model file is available\n",
    "    if \"{}.tar\".format(out) not in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} has not been trained yet'.format(out))\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        print()\n",
    "        !echo validating $out\n",
    "        print()\n",
    "        \n",
    "        #run validation\n",
    "        !python validate_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                                    --hyperparameter_set=$hyp_st --model_name=$out \\\n",
    "                                    --pipeline=$pipeline --dest_folder=$dest_folder\n",
    "        \n",
    "        # extract performance metrics and plot confusion matrix\n",
    "        !Rscript plot_confusion_matrix.R --input_file=$out --pipeline=$pipeline \\\n",
    "                                         --dest_folder=$dest_folder\n",
    "        \n",
    "        # accumulate performance scores\n",
    "        comb_prec_recall = comb_prec_recall.append(pd.read_csv('./{}/{}/{}/{}_prec_recall.csv'.format(dest_folder, pipeline, out, out)))\n",
    "    \n",
    "    \n",
    "# Write combined metrics to csv and plot combined metrics\n",
    "pooled_data_path = './{}/{}/pooled_prec_recall.csv'.format(dest_folder, pipeline)\n",
    "comb_prec_recall.to_csv(pooled_data_path)\n",
    "output_file_path = './{}/{}/comparison_plot.png'.format(dest_folder, pipeline) \n",
    "!Rscript plot_comparison.R --input_file=$pooled_data_path \\\n",
    "                           --output_file=$output_file_path \\\n",
    "                           --x='recall' --y='precision' --facet='label'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ablation experiment -- 11 classes training set vs. binary training set<a name=\"1A\"></a>\n",
    "\n",
    "Here we test how models with 11 classes compare with a model that simply classifies between 'seal' and 'not_seal'. We will train all architectures from Pipeline 1 on the binary training set and compare validation results to decide which approach is superior.  We begin by creating new model combinations which will be similar to the ones above but now using training_set_binary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch pipeline \n",
    "pipeline = 'Pipeline1'\n",
    "\n",
    "# generate model combinations -- now with training_set_binary\n",
    "combinations_1_bin = {'model_architecture': ['Resnet18', 'Resnet34', 'Resnet50', 'Squeezenet11',\n",
    "                                             'Alexnet', 'NasnetA', 'Densenet121','Densenet169', \n",
    "                                             'VGG16'],\n",
    "                      'training_dir': ['training_set_binary'] * 9,\n",
    "                      'hyperparameter_set': ['B'] * 9,\n",
    "                      'cv_weights': ['NO'] * 9}\n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_1_bin = pd.DataFrame(combinations_1_bin)\n",
    "                    \n",
    "# create folders for resulting files\n",
    "for row in combinations_1_bin.iterrows():\n",
    "    mdl = row[1]['model_architecture'] + '_ts-' + row[1]['training_dir'].split('_')[-1]             \n",
    "    if not os.path.exists(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)):\n",
    "        os.makedirs(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over combinations -- now with training_set_binary\n",
    "for row in combinations_1_bin.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st, cv_wgt = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                                  row[1]['hyperparameter_set'], row[1]['cv_weights']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                             --hyperparameter_set=$hyp_st --cv_weights=$cv_wgt \\\n",
    "                             --output_name=$out --pipeline=$pipeline \\\n",
    "                             --dest_folder=$dest_folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame to combine all metrics \n",
    "comb_prec_recall = pd.DataFrame()\n",
    "pipeline = 'Pipeline1'\n",
    "\n",
    "# iterate over trained models\n",
    "for row in combinations_1_bin.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st, cv_wgt = row[1]['training_dir'], row[1]['model_architecture'],\\\n",
    "                                  row[1]['hyperparameter_set'], row[1]['cv_weights']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model file is available\n",
    "    if \"{}.tar\".format(out) not in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} has not been trained yet'.format(out))\n",
    "        continue\n",
    "        \n",
    "    else:\n",
    "        print()\n",
    "        !echo validating $out\n",
    "        print()\n",
    "        \n",
    "        #run validation\n",
    "        !python validate_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                                    --hyperparameter_set=$hyp_st --model_name=$out \\\n",
    "                                    --pipeline=$pipeline --dest_folder=$dest_folder\n",
    "        \n",
    "        # extract performance metrics and plot confusion matrix\n",
    "        !Rscript plot_confusion_matrix.R --input_file=$out --pipeline=$pipeline \\\n",
    "                                         --dest_folder=$dest_folder\n",
    "        \n",
    "        # accumulate performance scores\n",
    "        comb_prec_recall = comb_prec_recall.append(pd.read_csv('./{}/{}/{}/{}_prec_recall.csv'.format(dest_folder, pipeline, out, out)))\n",
    "    \n",
    "    \n",
    "# Write combined metrics to csv and plot combined metrics\n",
    "pooled_data_path = './{}/{}/pooled_prec_recall.csv'.format(dest_folder, pipeline)\n",
    "comb_prec_recall.to_csv(pooled_data_path)\n",
    "output_file_path = './{}/{}/comparison_plot_bin.png'.format(dest_folder, pipeline) \n",
    "!Rscript plot_comparison.R --input_file=$pooled_data_path \\\n",
    "                           --output_file=$output_file_path \\\n",
    "                           --x='recall' --y='precision' --facet='label'\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison -- binary vs. 11 classes \n",
    "\n",
    "We can visually compare validation results from both approaches by examining comparison plots.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# increase dpi\n",
    "mpl.rcParams['figure.dpi']= 2500\n",
    "\n",
    "# read both plots \n",
    "comp11 = mpimg.imread('./{}/{}/comparison_plot.png'.format(dest_folder, pipeline))\n",
    "comp2 = mpimg.imread('./{}/{}/comparison_plot_bin.png'.format(dest_folder, pipeline))\n",
    "\n",
    "# plot them side by side\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title('11 classes')\n",
    "plt.imshow(comp11)\n",
    "\n",
    "cur_axes = plt.gca()\n",
    "cur_axes.axes.get_xaxis().set_visible(False)\n",
    "cur_axes.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(comp2)\n",
    "\n",
    "cur_axes = plt.gca()\n",
    "plt.title('2 classes')\n",
    "cur_axes.axes.get_xaxis().set_visible(False)\n",
    "cur_axes.axes.get_yaxis().set_visible(False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1.1 - haulout detector + count<a name=\"1.1\"></a>\n",
    "---\n",
    "\n",
    "Here we will generate seal counting CNNs, train them and validate them. Seal counting CNNs will be trained to minimize the mean squared error (MSE) between predicted counts and ground-truth counts. Though they will be trained and validated separately from the haul out detector (Pipeline 1), these approaches will be tested on top of the haul out detector and as standalones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"1.1T\"></a>\n",
    "\n",
    "Similar to the previous pipeline, we will store results in folders (under './saved_models') named after each specific model combination for bookkeeping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch pipeline\n",
    "pipeline = 'Pipeline1.1'\n",
    "\n",
    "# generate model combinations\n",
    "combinations_11 = {'model_architecture': ['WideResnetCount', 'Resnet34count', \\\n",
    "                                          'Resnet18count', \\\n",
    "                                          'NasnetAcount', 'CountCeption'] * 2,\n",
    "                   'training_dir': ['training_set_vanilla'] * 5 + ['training_set_binary'] * 5,\n",
    "                   'hyperparameter_set': ['D'] * 2 + ['A'] * 4 + ['B'] * 4}       \n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_11 = pd.DataFrame(combinations_11)\n",
    "                    \n",
    "\n",
    "# create folders for resulting files\n",
    "for row in combinations_11.iterrows():\n",
    "    mdl = row[1]['model_architecture'] + '_ts-' + row[1]['training_dir'].split('_')[-1]               \n",
    "    if not os.path.exists(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)):\n",
    "        os.makedirs(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a counting model, model combinations created above are used as argument to to a new training script, *train_sealnet_count.py*, which uses MSE loss. It accepts the same arguments as the previous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training WideResnetCount_ts-vanilla\n",
      "\n",
      "Epoch 1/5\n",
      "----------\n",
      "Traceback (most recent call last):\n",
      "  File \"train_sealnet_count.py\", line 236, in <module>\n",
      "    main()\n",
      "  File \"train_sealnet_count.py\", line 232, in main\n",
      "    num_epochs=hyperparameters[args.hyperparameter_set]['epochs'])\n",
      "  File \"train_sealnet_count.py\", line 146, in train_model\n",
      "    for data in dataloaders[phase]:\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 286, in __next__\n",
      "    return self._process_next_batch(batch)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 307, in _process_next_batch\n",
      "    raise batch.exc_type(batch.exc_msg)\n",
      "OSError: Traceback (most recent call last):\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n",
      "    samples = collate_fn([dataset[i] for i in batch_indices])\n",
      "  File \"/home/bento/Seals/utils/dataloaders/data_loader_train_count.py\", line 109, in __getitem__\n",
      "    sample = self.loader(path)\n",
      "  File \"/home/bento/Seals/utils/dataloaders/data_loader_train_count.py\", line 165, in default_loader\n",
      "    return pil_loader(path)\n",
      "  File \"/home/bento/Seals/utils/dataloaders/data_loader_train_count.py\", line 147, in pil_loader\n",
      "    img = Image.open(f)\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/PIL/Image.py\", line 2590, in open\n",
      "    % (filename if filename else fp))\n",
      "OSError: cannot identify image file <_io.BufferedReader name='./training_sets/training_set_vanilla/training/crabeater/2792.jpg'>\n",
      "\n",
      "\n",
      "training Resnet34count_ts-vanilla\n",
      "\n",
      "Epoch 1/5\n",
      "----------\n",
      "\n",
      "training Resnet18count_ts-vanilla\n",
      "\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"train_sealnet_count.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/__init__.py\", line 276, in <module>\n",
      "    import torch.nn\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/nn/modules/__init__.py\", line 1, in <module>\n",
      "    from .module import Module\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/nn/modules/module.py\", line 6, in <module>\n",
      "    from ..backends.thnn import backend as thnn_backend\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/nn/backends/thnn.py\", line 41, in <module>\n",
      "    _initialize_backend()\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/nn/backends/thnn.py\", line 23, in _initialize_backend\n",
      "    from .._functions.rnn import RNN, \\\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/nn/_functions/rnn.py\", line 3, in <module>\n",
      "    import torch.backends.cudnn as cudnn\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/backends/cudnn/__init__.py\", line 169, in <module>\n",
      "    class CuDNNHandle:\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'child' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    155\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m                 \u001b[0mchild\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpexpect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'-c'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Vanilla Pexpect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m             \u001b[0mflush\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, command, args, timeout, maxread, searchwindowsize, logfile, cwd, env, ignore_sighup, echo, preexec_fn, encoding, codec_errors, dimensions, use_poll)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreexec_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdimensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_poll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0muse_poll\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawn\u001b[0;34m(self, command, args, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    302\u001b[0m         self.ptyproc = self._spawnpty(self.args, env=self.env,\n\u001b[0;32m--> 303\u001b[0;31m                                      cwd=self.cwd, **kwargs)\n\u001b[0m\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pexpect/pty_spawn.py\u001b[0m in \u001b[0;36m_spawnpty\u001b[0;34m(self, args, **kwargs)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;34m'''Spawn a pty and return an instance of PtyProcess.'''\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mptyprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPtyProcess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspawn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/ptyprocess/ptyprocess.py\u001b[0m in \u001b[0;36mspawn\u001b[0;34m(cls, argv, cwd, env, echo, preexec_fn, dimensions)\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexec_err_pipe_write\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m         \u001b[0mexec_err_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexec_err_pipe_read\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexec_err_pipe_read\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-128fa26ef1f6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'echo training $out'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36msystem_piped\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m   2259\u001b[0m         \u001b[0;31m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2260\u001b[0m         \u001b[0;31m# Instead, we store the exit_code in user_ns.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2261\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_exit_code'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2263\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msystem_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/IPython/utils/_process_posix.py\u001b[0m in \u001b[0;36msystem\u001b[0;34m(self, cmd)\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0;31m# (the character is known as ETX for 'End of Text', see\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0;31m# curses.ascii.ETX).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0mchild\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msendline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m             \u001b[0;31m# Read and print any more output the program might produce on its\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m             \u001b[0;31m# way out.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'child' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_11.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet_count.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                                   --hyperparameter_set=$hyp_st --output_name=$out  \\\n",
    "                                   --pipeline=$pipeline --dest_folder=$dest_folder\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"1.1V\"></a>\n",
    "\n",
    "Validating counting models is a little bit simpler then with seal haul out models: for each model we just extract the mean squared error, running time at inference and number of model parameters. To test if classifying images before counting is helpful, performance stats during counting validation will be later compared to those where images where classified prior to counting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DataFrame to combine all metrics \n",
    "comb_mse = pd.DataFrame()\n",
    "pipeline = 'Pipeline1.1'\n",
    "\n",
    "# iterate over trained models\n",
    "for row in combinations_11.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'],\\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model file is available\n",
    "    if \"{}.tar\".format(out) not in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} has not been trained yet'.format(out))\n",
    "        continue\n",
    "    \n",
    "    else:\n",
    "        print()\n",
    "        !echo validating $out\n",
    "        print()\n",
    "        \n",
    "        #run validation\n",
    "        !python validate_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                                    --hyperparameter_set=$hyp_st --model_name=$out \\\n",
    "                                    --pipeline=$pipeline --dest_folder=$dest_folder\n",
    "        \n",
    "        # extract performance metrics and plot confusion matrix\n",
    "        !Rscript get_mse.R --input_file=$out --pipeline=$pipeline --dest_folder=$dest_folder\n",
    "    \n",
    "        # accumulate performance scores\n",
    "        comb_mse = comb_mse.append(pd.read_csv('./{}/{}/{}/{}_mse.csv'.format(dest_folder, pipeline, out, out)))\n",
    "    \n",
    "# generate counting benchmarks and add them to the combined metrics DataFrame\n",
    "!Rscript generate_benchmarks.R --input_file=$out --pipeline=$pipeline \\\n",
    "                               --dest_folder=$dest_folder\n",
    "comb_mse = comb_mse.append(pd.read_csv('./{}/{}/benchmarks.csv'.format(dest_folder, pipeline)))\n",
    "\n",
    "# write combined metrics to csv and plot combined metrics\n",
    "pooled_data_path = './{}/{}/pooled_mse.csv'.format(dest_folder, pipeline)\n",
    "comb_mse.to_csv(pooled_data_path)\n",
    "\n",
    "# total predicted vs ground-truth plot\n",
    "output_file_path = './{}/{}/comparison_final_count.png'.format(dest_folder, pipeline)\n",
    "!Rscript plot_comparison.R --input_file=$pooled_data_path \\\n",
    "                           --output_file=$output_file_path \\\n",
    "                           --x='total_predicted' --y='total_ground_truth'\n",
    "        \n",
    "# inference time vs. MSE plot\n",
    "output_file_path = './{}/{}/comparison_mse.png'.format(dest_folder, pipeline)\n",
    "!Rscript plot_comparison.R --input_file=$pooled_data_path \\\n",
    "                           --output_file=$output_file_path \\\n",
    "                           --x='running_time' --y='MSE'\n",
    "\n",
    "# precision vs. recall plot\n",
    "output_file_path = './{}/{}/comparison_prec_recall.png'.format(dest_folder, pipeline)\n",
    "!Rscript plot_comparison.R --input_file=$pooled_data_path \\\n",
    "                           --output_file=$output_file_path \\\n",
    "                           --x='recall' --y='precision'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1.2 - haulout detector + single seal detector<a name=\"1.2\"></a>\n",
    "---\n",
    "\n",
    "Pipeline 1.2 adds an individual seal detection CNN on top of the seal haul out detector (Pipeline 1. Individual seal detection CNNs will be trained to localize detection points and minimize the MSE between the number of detections and ground-truth count. In this approach, counts will be obtained by adding up the number of detections. Though they will be trained and validated separately from the haul out detector (Pipeline 1), these approaches will be tested on top of the haul out detector and as standalones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"1.2T\"></a>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch pipeline\n",
    "pipeline = 'Pipeline1.2'\n",
    "\n",
    "# generate model combinations\n",
    "combinations_12 = {'model_architecture': ['UnetDet', 'UnetDet'],\n",
    "                   'training_dir': ['training_set_binary', 'training_set_vanilla'],\n",
    "                   'hyperparameter_set': ['B', 'B']}       \n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_12 = pd.DataFrame(combinations_12)\n",
    "                    \n",
    "\n",
    "# create folders for resulting files\n",
    "for row in combinations_12.iterrows():\n",
    "    mdl = row[1]['model_architecture'] + '_ts-' + row[1]['training_dir'].split('_')[-1]                  \n",
    "    if not os.path.exists(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)):\n",
    "        os.makedirs(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training UnetDet_ts-binary\n",
      "\n",
      "Epoch 1/5\n",
      "----------\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss: 0.7242593765258789\n",
      "   Euclidean loss: 1.6564775685502544\n",
      "   BCE loss: 1.1911379098892212\n",
      "   total loss: 1.9153972864151\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss: 2.3120179176330566\n",
      "   Euclidean loss: 1.5596258375522867\n",
      "   BCE loss: 0.17397159337997437\n",
      "   total loss: 2.485989511013031\n",
      "\n",
      " 400 training iterations\n",
      "   Hubber loss: 2.015331506729126\n",
      "   Euclidean loss: 1.3903650832746606\n",
      "   BCE loss: 0.0333438515663147\n",
      "   total loss: 2.0486753582954407\n",
      "\n",
      " 600 training iterations\n",
      "   Hubber loss: 0.898402988910675\n",
      "   Euclidean loss: 1.1422966264437096\n",
      "   BCE loss: 0.010145379230380058\n",
      "   total loss: 0.9085483681410551\n",
      "\n",
      " 800 training iterations\n",
      "   Hubber loss: 1.0583045482635498\n",
      "   Euclidean loss: 0.18857554237620533\n",
      "   BCE loss: 0.005427421536296606\n",
      "   total loss: 1.0637319697998464\n",
      "\n",
      " 1000 training iterations\n",
      "   Hubber loss: 0.5775986909866333\n",
      "   Euclidean loss: 0.9949795570987857\n",
      "   BCE loss: 0.003970695659518242\n",
      "   total loss: 0.5815693866461515\n",
      "\n",
      " 1200 training iterations\n",
      "   Hubber loss: 0.3128666877746582\n",
      "   Euclidean loss: 0.5914939617697205\n",
      "   BCE loss: 0.003020708914846182\n",
      "   total loss: 0.3158873966895044\n",
      "\n",
      " 1400 training iterations\n",
      "   Hubber loss: 1.0953729152679443\n",
      "   Euclidean loss: 0.46271572956187157\n",
      "   BCE loss: 0.0023874244652688503\n",
      "   total loss: 1.0977603397332132\n",
      "\n",
      " 1600 training iterations\n",
      "   Hubber loss: 0.5454410314559937\n",
      "   Euclidean loss: 0.4050487360171171\n",
      "   BCE loss: 0.0017870455048978329\n",
      "   total loss: 0.5472280769608915\n",
      "\n",
      " 1800 training iterations\n",
      "   Hubber loss: 0.5451428890228271\n",
      "   Euclidean loss: 0.8923173810923789\n",
      "   BCE loss: 0.0014832844026386738\n",
      "   total loss: 0.5466261734254658\n",
      "\n",
      " 2000 training iterations\n",
      "   Hubber loss: 0.43127647042274475\n",
      "   Euclidean loss: 0.8463492563996291\n",
      "   BCE loss: 0.0011896021896973252\n",
      "   total loss: 0.4324660726124421\n",
      "\n",
      " 2200 training iterations\n",
      "   Hubber loss: 0.8295094966888428\n",
      "   Euclidean loss: 0.7317844440103792\n",
      "   BCE loss: 0.0009241867228411138\n",
      "   total loss: 0.8304336834116839\n",
      "\n",
      " 2400 training iterations\n",
      "   Hubber loss: 0.730016827583313\n",
      "   Euclidean loss: 0.12224792442880127\n",
      "   BCE loss: 0.0007801122847013175\n",
      "   total loss: 0.7307969398680143\n",
      "\n",
      " 2600 training iterations\n",
      "   Hubber loss: 0.28767159581184387\n",
      "   Euclidean loss: 1.12829139991518\n",
      "   BCE loss: 0.0008158884011209011\n",
      "   total loss: 0.2884874842129648\n",
      "\n",
      " 2800 training iterations\n",
      "   Hubber loss: 0.2751982808113098\n",
      "   Euclidean loss: 0.26146987379109843\n",
      "   BCE loss: 0.0005773665034212172\n",
      "   total loss: 0.27577564731473103\n",
      "\n",
      " 3000 training iterations\n",
      "   Hubber loss: 0.23907329142093658\n",
      "   Euclidean loss: 0.4915291244570908\n",
      "   BCE loss: 0.0005171512602828443\n",
      "   total loss: 0.23959044268121943\n",
      "\n",
      " 3200 training iterations\n",
      "   Hubber loss: 0.4288594126701355\n",
      "   Euclidean loss: 0.44066142229402033\n",
      "   BCE loss: 0.0004512954910751432\n",
      "   total loss: 0.42931070816121064\n",
      "\n",
      " 3400 training iterations\n",
      "   Hubber loss: 0.39457547664642334\n",
      "   Euclidean loss: 0.33588067085031914\n",
      "   BCE loss: 0.00037512919516302645\n",
      "   total loss: 0.39495060584158637\n",
      "\n",
      " 3600 training iterations\n",
      "   Hubber loss: 0.20358367264270782\n",
      "   Euclidean loss: 0.4895975718632188\n",
      "   BCE loss: 0.00036415521753951907\n",
      "   total loss: 0.20394782786024734\n",
      "\n",
      " 3800 training iterations\n",
      "   Hubber loss: 0.5100808143615723\n",
      "   Euclidean loss: 0.2885959280447693\n",
      "   BCE loss: 0.00035436335019767284\n",
      "   total loss: 0.5104351777117699\n",
      "\n",
      " 4000 training iterations\n",
      "   Hubber loss: 0.3357929587364197\n",
      "   Euclidean loss: 0.43123498972467167\n",
      "   BCE loss: 0.00032086146529763937\n",
      "   total loss: 0.3361138202017173\n",
      "\n",
      " 4200 training iterations\n",
      "   Hubber loss: 0.1990433633327484\n",
      "   Euclidean loss: 0.4519776009526275\n",
      "   BCE loss: 0.00027896161191165447\n",
      "   total loss: 0.19932232494466007\n",
      "training Loss: 0.1406\n",
      "train_sealnet_det.py:208: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "  pred_loc = [get_xy_locs(loc, max(0, int(pred_cnt[idx]))) for idx, loc in enumerate(pred_loc.numpy())]\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss: 0.04465072602033615\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.0001417148014297709\n",
      "   total loss: 0.04479244082176592\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss: 5.475402304000454e-06\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00015402342251036316\n",
      "   total loss: 0.00015949882481436362\n",
      "\n",
      " 400 training iterations\n",
      "   Hubber loss: 0.00010557857603998855\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00014905589341651648\n",
      "   total loss: 0.00025463446945650503\n",
      "\n",
      " 600 training iterations\n",
      "   Hubber loss: 3.548885433701798e-05\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.0001342528557870537\n",
      "   total loss: 0.0001697417101240717\n",
      "\n",
      " 800 training iterations\n",
      "   Hubber loss: 2.983030319213867\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00018575171998236328\n",
      "   total loss: 2.9832160709338496\n",
      "\n",
      " 1000 training iterations\n",
      "   Hubber loss: 0.0006042279419489205\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00017954981012735516\n",
      "   total loss: 0.0007837777520762756\n",
      "\n",
      " 1200 training iterations\n",
      "   Hubber loss: 7.568985165562481e-05\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00015219814667943865\n",
      "   total loss: 0.00022788799833506346\n",
      "\n",
      " 1400 training iterations\n",
      "   Hubber loss: 3.200660285074264e-05\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00015566115325782448\n",
      "   total loss: 0.00018766775610856712\n",
      "\n",
      " 1600 training iterations\n",
      "   Hubber loss: 2.585927724838257\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00026291425456292927\n",
      "   total loss: 2.5861906390928198\n",
      "\n",
      " 1800 training iterations\n",
      "   Hubber loss: 0.0005627202917821705\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00017873574688564986\n",
      "   total loss: 0.0007414560386678204\n",
      "\n",
      " 2000 training iterations\n",
      "   Hubber loss: 0.00014323565119411796\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00014805502723902464\n",
      "   total loss: 0.0002912906784331426\n",
      "\n",
      " 2200 training iterations\n",
      "   Hubber loss: 0.003041018033400178\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00014161807484924793\n",
      "   total loss: 0.003182636108249426\n",
      "\n",
      " 2400 training iterations\n",
      "   Hubber loss: 0.09449447691440582\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00016044560470618308\n",
      "   total loss: 0.094654922519112\n",
      "\n",
      " 2600 training iterations\n",
      "   Hubber loss: 5.9802299801958725e-05\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00013020406186114997\n",
      "   total loss: 0.0001900063616631087\n",
      "\n",
      " 2800 training iterations\n",
      "   Hubber loss: 0.0008411739254370332\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.0001417178864357993\n",
      "   total loss: 0.0009828918118728325\n",
      "\n",
      " 3000 training iterations\n",
      "   Hubber loss: 0.00013045362720731646\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00014371192082762718\n",
      "   total loss: 0.00027416554803494364\n",
      "\n",
      " 3200 training iterations\n",
      "   Hubber loss: 0.004759116098284721\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.0001255891693290323\n",
      "   total loss: 0.004884705267613754\n",
      "\n",
      " 3400 training iterations\n",
      "   Hubber loss: 0.0003625937970355153\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00013145789853297174\n",
      "   total loss: 0.000494051695568487\n",
      "\n",
      " 3600 training iterations\n",
      "   Hubber loss: 0.0009499291772954166\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00013356414274312556\n",
      "   total loss: 0.0010834933200385422\n",
      "\n",
      " 3800 training iterations\n",
      "   Hubber loss: 0.00018929575162474066\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00015307619469240308\n",
      "   total loss: 0.00034237194631714374\n",
      "\n",
      " 4000 training iterations\n",
      "   Hubber loss: 4.728334261017153e-06\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00015432531654369086\n",
      "   total loss: 0.000159053650804708\n",
      "\n",
      " 4200 training iterations\n",
      "   Hubber loss: 0.3813827335834503\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00015421785064972937\n",
      "   total loss: 0.38153695143410005\n",
      "\n",
      " 4400 training iterations\n",
      "   Hubber loss: 0.0008308019023388624\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00018379613175056875\n",
      "   total loss: 0.0010145980340894312\n",
      "\n",
      " 4600 training iterations\n",
      "   Hubber loss: 4.3440741137601435e-05\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.0001510770816821605\n",
      "   total loss: 0.00019451782281976193\n",
      "\n",
      " 4800 training iterations\n",
      "   Hubber loss: 0.00011746203381335363\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00014937006926629692\n",
      "   total loss: 0.00026683210307965055\n",
      "\n",
      " 5000 training iterations\n",
      "   Hubber loss: 0.00037423489266075194\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.0001419459149474278\n",
      "   total loss: 0.0005161808076081797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 5200 training iterations\n",
      "   Hubber loss: 0.00021214125445112586\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.0001485606044298038\n",
      "   total loss: 0.00036070185888092965\n",
      "\n",
      " 5400 training iterations\n",
      "   Hubber loss: 3.708488748088712e-06\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00014953743084333837\n",
      "   total loss: 0.00015324591959142708\n",
      "\n",
      " 5600 training iterations\n",
      "   Hubber loss: 0.0005204820772632957\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00014151839422993362\n",
      "   total loss: 0.0006620004714932293\n",
      "\n",
      " 5800 training iterations\n",
      "   Hubber loss: 0.0002311283751623705\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00014435594493988901\n",
      "   total loss: 0.0003754843201022595\n",
      "\n",
      " 6000 training iterations\n",
      "   Hubber loss: 0.0005760377389378846\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.0001784644409781322\n",
      "   total loss: 0.0007545021799160168\n",
      "\n",
      " 6200 training iterations\n",
      "   Hubber loss: 5.467914525070228e-05\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00014568286132998765\n",
      "   total loss: 0.00020036200658068992\n",
      "\n",
      " 6400 training iterations\n",
      "   Hubber loss: 0.0005611884407699108\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00017862326058093458\n",
      "   total loss: 0.0007398117013508454\n",
      "\n",
      " 6600 training iterations\n",
      "   Hubber loss: 0.0005791380535811186\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00017923519772011787\n",
      "   total loss: 0.0007583732513012365\n",
      "\n",
      " 6800 training iterations\n",
      "   Hubber loss: 0.00012102528853574768\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.0001447325776098296\n",
      "   total loss: 0.0002657578661455773\n",
      "\n",
      " 7000 training iterations\n",
      "   Hubber loss: 0.6112933158874512\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00015223435184452683\n",
      "   total loss: 0.6114455502392957\n",
      "\n",
      " 7200 training iterations\n",
      "   Hubber loss: 0.014846897684037685\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.0001627517631277442\n",
      "   total loss: 0.01500964944716543\n",
      "\n",
      " 7400 training iterations\n",
      "   Hubber loss: 0.0003379262052476406\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00023223516473080963\n",
      "   total loss: 0.0005701613699784502\n",
      "\n",
      " 7600 training iterations\n",
      "   Hubber loss: 1.9728994369506836\n",
      "   Euclidean loss: 0.021380899352993952\n",
      "   BCE loss: 0.0004964964464306831\n",
      "   total loss: 1.9733959333971143\n",
      "validation Loss: 0.7379\n",
      "training time: 0.0h 43m 58s\n",
      "\n",
      "Epoch 2/5\n",
      "----------\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss: 0.2604496479034424\n",
      "   Euclidean loss: 0.14012657089520933\n",
      "   BCE loss: 0.00021415160153992474\n",
      "   total loss: 0.2606637995049823\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss: 0.6552428007125854\n",
      "   Euclidean loss: 0.5140331010711755\n",
      "   BCE loss: 0.00025763572193682194\n",
      "   total loss: 0.6555004364345223\n",
      "\n",
      " 400 training iterations\n",
      "   Hubber loss: 0.33780235052108765\n",
      "   Euclidean loss: 0.35389322625277503\n",
      "   BCE loss: 0.00021393437054939568\n",
      "   total loss: 0.33801628489163704\n",
      "\n",
      " 600 training iterations\n",
      "   Hubber loss: 0.10317574441432953\n",
      "   Euclidean loss: 0.4449894305805004\n",
      "   BCE loss: 0.00018002874276135117\n",
      "   total loss: 0.10335577315709088\n",
      "\n",
      " 800 training iterations\n",
      "   Hubber loss: 0.5110330581665039\n",
      "   Euclidean loss: 0.547400743387444\n",
      "   BCE loss: 0.00027454845258034766\n",
      "   total loss: 0.5113076066190843\n",
      "\n",
      " 1000 training iterations\n",
      "   Hubber loss: 0.17011435329914093\n",
      "   Euclidean loss: 0.30471417959358793\n",
      "   BCE loss: 0.00013997932546772063\n",
      "   total loss: 0.17025433262460865\n",
      "\n",
      " 1200 training iterations\n",
      "   Hubber loss: 0.44539856910705566\n",
      "   Euclidean loss: 0.41491238466828784\n",
      "   BCE loss: 0.00018482179439160973\n",
      "   total loss: 0.4455833909014473\n",
      "\n",
      " 1400 training iterations\n",
      "   Hubber loss: 0.23818321526050568\n",
      "   Euclidean loss: 0.5157278142367546\n",
      "   BCE loss: 0.0001523022510809824\n",
      "   total loss: 0.23833551751158666\n",
      "\n",
      " 1600 training iterations\n",
      "   Hubber loss: 0.4401046633720398\n",
      "   Euclidean loss: 0.26661370054957345\n",
      "   BCE loss: 0.00016767399210948497\n",
      "   total loss: 0.4402723373641493\n",
      "\n",
      " 1800 training iterations\n",
      "   Hubber loss: 0.09791126847267151\n",
      "   Euclidean loss: 0.2670916063841237\n",
      "   BCE loss: 0.00014702715270686895\n",
      "   total loss: 0.09805829562537838\n",
      "\n",
      " 2000 training iterations\n",
      "   Hubber loss: 0.287813663482666\n",
      "   Euclidean loss: 0.481077723997198\n",
      "   BCE loss: 0.00014520568947773427\n",
      "   total loss: 0.28795886917214375\n",
      "\n",
      " 2200 training iterations\n",
      "   Hubber loss: 0.31503862142562866\n",
      "   Euclidean loss: 0.22442404274754174\n",
      "   BCE loss: 0.00012023162707919255\n",
      "   total loss: 0.31515885305270785\n",
      "\n",
      " 2400 training iterations\n",
      "   Hubber loss: 0.8955163955688477\n",
      "   Euclidean loss: 0.1565601784269089\n",
      "   BCE loss: 0.0002116878895321861\n",
      "   total loss: 0.8957280834583798\n",
      "\n",
      " 2600 training iterations\n",
      "   Hubber loss: 0.26920902729034424\n",
      "   Euclidean loss: 0.2942190817552067\n",
      "   BCE loss: 0.00014695960271637887\n",
      "   total loss: 0.2693559868930606\n",
      "\n",
      " 2800 training iterations\n",
      "   Hubber loss: 0.23416179418563843\n",
      "   Euclidean loss: 0.16090867910461817\n",
      "   BCE loss: 7.036404713289812e-05\n",
      "   total loss: 0.23423215823277133\n",
      "\n",
      " 3000 training iterations\n",
      "   Hubber loss: 0.17429178953170776\n",
      "   Euclidean loss: 0.4197091924684127\n",
      "   BCE loss: 0.00011516060476424173\n",
      "   total loss: 0.174406950136472\n",
      "\n",
      " 3200 training iterations\n",
      "   Hubber loss: 0.4437066614627838\n",
      "   Euclidean loss: 0.2634332872244476\n",
      "   BCE loss: 0.00010252150241285563\n",
      "   total loss: 0.44380918296519667\n",
      "\n",
      " 3400 training iterations\n",
      "   Hubber loss: 0.4149003326892853\n",
      "   Euclidean loss: 0.3911792982550992\n",
      "   BCE loss: 0.00015556163270957768\n",
      "   total loss: 0.41505589432199486\n",
      "\n",
      " 3600 training iterations\n",
      "   Hubber loss: 0.21193468570709229\n",
      "   Euclidean loss: 0.6797332402795909\n",
      "   BCE loss: 0.00014303546049632132\n",
      "   total loss: 0.2120777211675886\n",
      "\n",
      " 3800 training iterations\n",
      "   Hubber loss: 0.6920385360717773\n",
      "   Euclidean loss: 1.0017160599551382\n",
      "   BCE loss: 0.0002271839475724846\n",
      "   total loss: 0.6922657200193498\n",
      "\n",
      " 4000 training iterations\n",
      "   Hubber loss: 0.7469110488891602\n",
      "   Euclidean loss: 0.4145874964444001\n",
      "   BCE loss: 0.00017442267562728375\n",
      "   total loss: 0.7470854715647874\n",
      "\n",
      " 4200 training iterations\n",
      "   Hubber loss: 0.4848342537879944\n",
      "   Euclidean loss: 0.37648459002840157\n",
      "   BCE loss: 0.00016425146895926446\n",
      "   total loss: 0.48499850525695365\n",
      "training Loss: 0.0734\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss: 0.05928422510623932\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.1099837138317525e-05\n",
      "   total loss: 0.059295324943377636\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss: 0.0011963894357904792\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 9.682565178081859e-06\n",
      "   total loss: 0.001206072000968561\n",
      "\n",
      " 400 training iterations\n",
      "   Hubber loss: 0.001091041136533022\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 9.382997632201295e-06\n",
      "   total loss: 0.0011004241341652232\n",
      "\n",
      " 600 training iterations\n",
      "   Hubber loss: 0.0028827718924731016\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.1713062121998519e-05\n",
      "   total loss: 0.0028944849545951\n",
      "\n",
      " 800 training iterations\n",
      "   Hubber loss: 3.3526618480682373\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 7.655224180780351e-05\n",
      "   total loss: 3.352738400310045\n",
      "\n",
      " 1000 training iterations\n",
      "   Hubber loss: 0.0008347869152203202\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.105905721487943e-05\n",
      "   total loss: 0.0008458459724351997\n",
      "\n",
      " 1200 training iterations\n",
      "   Hubber loss: 0.0006675884360447526\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 9.148844583251048e-06\n",
      "   total loss: 0.0006767372806280036\n",
      "\n",
      " 1400 training iterations\n",
      "   Hubber loss: 0.0018589503597468138\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 9.99188887362834e-06\n",
      "   total loss: 0.0018689422486204421\n",
      "\n",
      " 1600 training iterations\n",
      "   Hubber loss: 1.1834611892700195\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 5.47290183021687e-05\n",
      "   total loss: 1.1835159182883217\n",
      "\n",
      " 1800 training iterations\n",
      "   Hubber loss: 0.0006575306761078537\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0205565558862872e-05\n",
      "   total loss: 0.0006677362416667165\n",
      "\n",
      " 2000 training iterations\n",
      "   Hubber loss: 0.0007243329309858382\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 9.541053259454202e-06\n",
      "   total loss: 0.0007338739842452924\n",
      "\n",
      " 2200 training iterations\n",
      "   Hubber loss: 0.030245337635278702\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.4812009794695769e-05\n",
      "   total loss: 0.030260149645073398\n",
      "\n",
      " 2400 training iterations\n",
      "   Hubber loss: 0.16339115798473358\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.4298913811217062e-05\n",
      "   total loss: 0.1634054568985448\n",
      "\n",
      " 2600 training iterations\n",
      "   Hubber loss: 0.0008016684441827238\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.2491751476773061e-05\n",
      "   total loss: 0.0008141601956594968\n",
      "\n",
      " 2800 training iterations\n",
      "   Hubber loss: 0.0005157013656571507\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.2065129340044223e-05\n",
      "   total loss: 0.000527766494997195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3000 training iterations\n",
      "   Hubber loss: 0.0010466458043083549\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.1982819160039071e-05\n",
      "   total loss: 0.001058628623468394\n",
      "\n",
      " 3200 training iterations\n",
      "   Hubber loss: 0.00029239783179946244\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 9.57989595917752e-06\n",
      "   total loss: 0.00030197772775863996\n",
      "\n",
      " 3400 training iterations\n",
      "   Hubber loss: 0.0017132139764726162\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 9.78340449364623e-06\n",
      "   total loss: 0.0017229973809662624\n",
      "\n",
      " 3600 training iterations\n",
      "   Hubber loss: 0.02850513905286789\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.1090661246271338e-05\n",
      "   total loss: 0.02851622971411416\n",
      "\n",
      " 3800 training iterations\n",
      "   Hubber loss: 0.0023850579746067524\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 9.991586921387352e-06\n",
      "   total loss: 0.0023950495615281397\n",
      "\n",
      " 4000 training iterations\n",
      "   Hubber loss: 0.0015332995681092143\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 9.824499102251139e-06\n",
      "   total loss: 0.0015431240672114654\n",
      "\n",
      " 4200 training iterations\n",
      "   Hubber loss: 0.1378667801618576\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.6576779671595432e-05\n",
      "   total loss: 0.1378833569415292\n",
      "\n",
      " 4400 training iterations\n",
      "   Hubber loss: 0.0008831077720969915\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.1075579095631838e-05\n",
      "   total loss: 0.0008941833511926234\n",
      "\n",
      " 4600 training iterations\n",
      "   Hubber loss: 0.0025254751089960337\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0124123036803212e-05\n",
      "   total loss: 0.002535599232032837\n",
      "\n",
      " 4800 training iterations\n",
      "   Hubber loss: 0.0010900264605879784\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 9.514200428384356e-06\n",
      "   total loss: 0.0010995406610163627\n",
      "\n",
      " 5000 training iterations\n",
      "   Hubber loss: 0.0024661007337272167\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0512904736970086e-05\n",
      "   total loss: 0.002476613638464187\n",
      "\n",
      " 5200 training iterations\n",
      "   Hubber loss: 0.002947181463241577\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0364123227191158e-05\n",
      "   total loss: 0.0029575455864687683\n",
      "\n",
      " 5400 training iterations\n",
      "   Hubber loss: 0.002195828827098012\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0009522156906314e-05\n",
      "   total loss: 0.0022058383492549183\n",
      "\n",
      " 5600 training iterations\n",
      "   Hubber loss: 0.0033676098100841045\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.05234794318676e-05\n",
      "   total loss: 0.003378133289515972\n",
      "\n",
      " 5800 training iterations\n",
      "   Hubber loss: 0.00301992311142385\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.143881854659412e-05\n",
      "   total loss: 0.003031361929970444\n",
      "\n",
      " 6000 training iterations\n",
      "   Hubber loss: 0.0006942737381905317\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0160388228541706e-05\n",
      "   total loss: 0.0007044341264190734\n",
      "\n",
      " 6200 training iterations\n",
      "   Hubber loss: 0.0016107932897284627\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 8.907521987566724e-06\n",
      "   total loss: 0.0016197008117160294\n",
      "\n",
      " 6400 training iterations\n",
      "   Hubber loss: 0.0006707696011289954\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0552055755397305e-05\n",
      "   total loss: 0.0006813216568843927\n",
      "\n",
      " 6600 training iterations\n",
      "   Hubber loss: 0.0007229997427202761\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0780401680676732e-05\n",
      "   total loss: 0.0007337801444009528\n",
      "\n",
      " 6800 training iterations\n",
      "   Hubber loss: 0.0001382991613354534\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.2700165825663134e-05\n",
      "   total loss: 0.00015099932716111653\n",
      "\n",
      " 7000 training iterations\n",
      "   Hubber loss: 0.3846815228462219\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 3.641399598564021e-05\n",
      "   total loss: 0.38471793684220756\n",
      "\n",
      " 7200 training iterations\n",
      "   Hubber loss: 0.030437802895903587\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.373533177684294e-05\n",
      "   total loss: 0.03045153822768043\n",
      "\n",
      " 7400 training iterations\n",
      "   Hubber loss: 0.0320357121527195\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.0001332676620222628\n",
      "   total loss: 0.03216897981474176\n",
      "\n",
      " 7600 training iterations\n",
      "   Hubber loss: 3.4315500259399414\n",
      "   Euclidean loss: 0.06500980056868115\n",
      "   BCE loss: 0.0004202309646643698\n",
      "   total loss: 3.4319702569046058\n",
      "validation Loss: 0.4415\n",
      "training time: 1.0h 27m 12s\n",
      "\n",
      "Epoch 3/5\n",
      "----------\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss: 0.5193980932235718\n",
      "   Euclidean loss: 0.18444082150535077\n",
      "   BCE loss: 9.009592031361535e-05\n",
      "   total loss: 0.5194881891438854\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss: 0.2864972949028015\n",
      "   Euclidean loss: 0.36649167221589435\n",
      "   BCE loss: 5.937819150858559e-05\n",
      "   total loss: 0.2865566730943101\n",
      "\n",
      " 400 training iterations\n",
      "   Hubber loss: 0.7491127848625183\n",
      "   Euclidean loss: 0.06380963578953144\n",
      "   BCE loss: 0.0001233373477589339\n",
      "   total loss: 0.7492361222102772\n",
      "\n",
      " 600 training iterations\n",
      "   Hubber loss: 0.29220813512802124\n",
      "   Euclidean loss: 0.16648036593522492\n",
      "   BCE loss: 9.640397183829919e-05\n",
      "   total loss: 0.29230453909985954\n",
      "\n",
      " 800 training iterations\n",
      "   Hubber loss: 0.06827113777399063\n",
      "   Euclidean loss: 0.299660035100778\n",
      "   BCE loss: 6.981027399888262e-05\n",
      "   total loss: 0.06834094804798951\n",
      "\n",
      " 1000 training iterations\n",
      "   Hubber loss: 0.17215478420257568\n",
      "   Euclidean loss: 0.3548433036271259\n",
      "   BCE loss: 0.00010388201917521656\n",
      "   total loss: 0.1722586662217509\n",
      "\n",
      " 1200 training iterations\n",
      "   Hubber loss: 0.4298582971096039\n",
      "   Euclidean loss: 0.8840108192993883\n",
      "   BCE loss: 0.00017837627092376351\n",
      "   total loss: 0.43003667338052765\n",
      "\n",
      " 1400 training iterations\n",
      "   Hubber loss: 0.1904098391532898\n",
      "   Euclidean loss: 0.2662112878220263\n",
      "   BCE loss: 6.799276161473244e-05\n",
      "   total loss: 0.19047783191490453\n",
      "\n",
      " 1600 training iterations\n",
      "   Hubber loss: 0.16135136783123016\n",
      "   Euclidean loss: 0.2637899897002632\n",
      "   BCE loss: 5.27654483448714e-05\n",
      "   total loss: 0.16140413327957503\n",
      "\n",
      " 1800 training iterations\n",
      "   Hubber loss: 0.10891804099082947\n",
      "   Euclidean loss: 0.38479483401753384\n",
      "   BCE loss: 6.129464600235224e-05\n",
      "   total loss: 0.10897933563683182\n",
      "\n",
      " 2000 training iterations\n",
      "   Hubber loss: 0.25208669900894165\n",
      "   Euclidean loss: 0.34190880571299453\n",
      "   BCE loss: 6.852330989204347e-05\n",
      "   total loss: 0.2521552223188337\n",
      "\n",
      " 2200 training iterations\n",
      "   Hubber loss: 0.40345820784568787\n",
      "   Euclidean loss: 0.43349678081940013\n",
      "   BCE loss: 0.00012421741848811507\n",
      "   total loss: 0.403582425264176\n",
      "\n",
      " 2400 training iterations\n",
      "   Hubber loss: 0.04263222962617874\n",
      "   Euclidean loss: 0.5033496552415287\n",
      "   BCE loss: 3.9508373447461054e-05\n",
      "   total loss: 0.0426717379996262\n",
      "\n",
      " 2600 training iterations\n",
      "   Hubber loss: 0.32477208971977234\n",
      "   Euclidean loss: 0.39274581197541586\n",
      "   BCE loss: 0.0001384816423524171\n",
      "   total loss: 0.32491057136212476\n",
      "\n",
      " 2800 training iterations\n",
      "   Hubber loss: 0.15783187747001648\n",
      "   Euclidean loss: 0.8406191240509555\n",
      "   BCE loss: 0.00011789097334258258\n",
      "   total loss: 0.15794976844335906\n",
      "\n",
      " 3000 training iterations\n",
      "   Hubber loss: 0.1976025104522705\n",
      "   Euclidean loss: 0.231772913168921\n",
      "   BCE loss: 4.7273937525460497e-05\n",
      "   total loss: 0.19764978438979597\n",
      "\n",
      " 3200 training iterations\n",
      "   Hubber loss: 0.6346721649169922\n",
      "   Euclidean loss: 0.4679203103458613\n",
      "   BCE loss: 0.00011997270485153422\n",
      "   total loss: 0.6347921376218437\n",
      "\n",
      " 3400 training iterations\n",
      "   Hubber loss: 0.4527548551559448\n",
      "   Euclidean loss: 0.44352315170633283\n",
      "   BCE loss: 0.00015108130173757672\n",
      "   total loss: 0.4529059364576824\n",
      "\n",
      " 3600 training iterations\n",
      "   Hubber loss: 0.4900185465812683\n",
      "   Euclidean loss: 0.2290646243448093\n",
      "   BCE loss: 0.00014276220463216305\n",
      "   total loss: 0.4901613087859005\n",
      "\n",
      " 3800 training iterations\n",
      "   Hubber loss: 0.4971156418323517\n",
      "   Euclidean loss: 0.4806276479202812\n",
      "   BCE loss: 0.00014108330651652068\n",
      "   total loss: 0.4972567251388682\n",
      "\n",
      " 4000 training iterations\n",
      "   Hubber loss: 0.28148353099823\n",
      "   Euclidean loss: 0.30145942199455233\n",
      "   BCE loss: 0.00014576590911019593\n",
      "   total loss: 0.2816292969073402\n",
      "\n",
      " 4200 training iterations\n",
      "   Hubber loss: 0.306833416223526\n",
      "   Euclidean loss: 0.2089725488548627\n",
      "   BCE loss: 7.591189205413684e-05\n",
      "   total loss: 0.30690932811558014\n",
      "training Loss: 0.0614\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss: 0.01942838355898857\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.6530384527868591e-06\n",
      "   total loss: 0.019430036597441358\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss: 0.013765770010650158\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 9.204380262417544e-07\n",
      "   total loss: 0.0137666904486764\n",
      "\n",
      " 400 training iterations\n",
      "   Hubber loss: 0.012822064571082592\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.2524552605318604e-06\n",
      "   total loss: 0.012824317026343124\n",
      "\n",
      " 600 training iterations\n",
      "   Hubber loss: 0.014161394909024239\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.3064175163890468e-06\n",
      "   total loss: 0.014162701326540628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 800 training iterations\n",
      "   Hubber loss: 0.11289433389902115\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 8.223415534303058e-06\n",
      "   total loss: 0.11290255731455545\n",
      "\n",
      " 1000 training iterations\n",
      "   Hubber loss: 0.013072723522782326\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 3.218685378669761e-06\n",
      "   total loss: 0.013075942208160996\n",
      "\n",
      " 1200 training iterations\n",
      "   Hubber loss: 0.012860178016126156\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.4992148180681397e-06\n",
      "   total loss: 0.012861677230944224\n",
      "\n",
      " 1400 training iterations\n",
      "   Hubber loss: 0.012913922779262066\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0292751539964229e-06\n",
      "   total loss: 0.012914952054416062\n",
      "\n",
      " 1600 training iterations\n",
      "   Hubber loss: 0.3078731894493103\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0297922017343808e-05\n",
      "   total loss: 0.30788348737132765\n",
      "\n",
      " 1800 training iterations\n",
      "   Hubber loss: 0.012925496324896812\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 4.587844614434289e-06\n",
      "   total loss: 0.012930084169511247\n",
      "\n",
      " 2000 training iterations\n",
      "   Hubber loss: 0.012850065715610981\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.8360529995552497e-06\n",
      "   total loss: 0.012852901768610536\n",
      "\n",
      " 2200 training iterations\n",
      "   Hubber loss: 0.036939095705747604\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.4835849217197392e-06\n",
      "   total loss: 0.036941579290669324\n",
      "\n",
      " 2400 training iterations\n",
      "   Hubber loss: 0.04665571078658104\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.6617924529782613e-06\n",
      "   total loss: 0.04665737257903402\n",
      "\n",
      " 2600 training iterations\n",
      "   Hubber loss: 0.009535584598779678\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0512865173950559e-06\n",
      "   total loss: 0.009536635885297073\n",
      "\n",
      " 2800 training iterations\n",
      "   Hubber loss: 0.01201180461794138\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.800417860271409e-06\n",
      "   total loss: 0.012014605035801651\n",
      "\n",
      " 3000 training iterations\n",
      "   Hubber loss: 0.012734953314065933\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 3.2273474062094465e-06\n",
      "   total loss: 0.012738180661472143\n",
      "\n",
      " 3200 training iterations\n",
      "   Hubber loss: 0.017085619270801544\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 8.670099305163603e-07\n",
      "   total loss: 0.01708648628073206\n",
      "\n",
      " 3400 training iterations\n",
      "   Hubber loss: 0.018732411786913872\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 9.769604503162554e-07\n",
      "   total loss: 0.018733388747364188\n",
      "\n",
      " 3600 training iterations\n",
      "   Hubber loss: 0.014097423292696476\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.408156663273985e-06\n",
      "   total loss: 0.01409883144935975\n",
      "\n",
      " 3800 training iterations\n",
      "   Hubber loss: 0.012873088009655476\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.1792707229906227e-06\n",
      "   total loss: 0.012875267280378466\n",
      "\n",
      " 4000 training iterations\n",
      "   Hubber loss: 0.012744610197842121\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.2904509933141526e-06\n",
      "   total loss: 0.012746900648835435\n",
      "\n",
      " 4200 training iterations\n",
      "   Hubber loss: 0.08388910442590714\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.2173371032986324e-05\n",
      "   total loss: 0.08390127779694012\n",
      "\n",
      " 4400 training iterations\n",
      "   Hubber loss: 0.01361613068729639\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 3.531016773195006e-06\n",
      "   total loss: 0.013619661704069586\n",
      "\n",
      " 4600 training iterations\n",
      "   Hubber loss: 0.012751813046634197\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.211543917634117e-06\n",
      "   total loss: 0.012753024590551831\n",
      "\n",
      " 4800 training iterations\n",
      "   Hubber loss: 0.012987426482141018\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.295606691404828e-06\n",
      "   total loss: 0.012989722088832423\n",
      "\n",
      " 5000 training iterations\n",
      "   Hubber loss: 0.013345937244594097\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 3.225629598091473e-06\n",
      "   total loss: 0.013349162874192189\n",
      "\n",
      " 5200 training iterations\n",
      "   Hubber loss: 0.012904457747936249\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.618424903426785e-06\n",
      "   total loss: 0.012906076172839676\n",
      "\n",
      " 5400 training iterations\n",
      "   Hubber loss: 0.024107903242111206\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 3.2228663258138113e-06\n",
      "   total loss: 0.02411112610843702\n",
      "\n",
      " 5600 training iterations\n",
      "   Hubber loss: 0.012775842100381851\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.8733908291324042e-06\n",
      "   total loss: 0.012777715491210984\n",
      "\n",
      " 5800 training iterations\n",
      "   Hubber loss: 0.012840718030929565\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.2035562804157962e-06\n",
      "   total loss: 0.012841921587209981\n",
      "\n",
      " 6000 training iterations\n",
      "   Hubber loss: 0.012830081395804882\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 3.931067567464197e-06\n",
      "   total loss: 0.012834012463372346\n",
      "\n",
      " 6200 training iterations\n",
      "   Hubber loss: 0.01320287212729454\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.2989545439268113e-06\n",
      "   total loss: 0.013204171081838467\n",
      "\n",
      " 6400 training iterations\n",
      "   Hubber loss: 0.012823685072362423\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 4.324243491282687e-06\n",
      "   total loss: 0.012828009315853706\n",
      "\n",
      " 6600 training iterations\n",
      "   Hubber loss: 0.01287451758980751\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 4.444848855200689e-06\n",
      "   total loss: 0.012878962438662711\n",
      "\n",
      " 6800 training iterations\n",
      "   Hubber loss: 0.0142930643633008\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 4.150806489633396e-06\n",
      "   total loss: 0.014297215169790434\n",
      "\n",
      " 7000 training iterations\n",
      "   Hubber loss: 0.10887904465198517\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 6.54769837638014e-06\n",
      "   total loss: 0.10888559235036155\n",
      "\n",
      " 7200 training iterations\n",
      "   Hubber loss: 0.01382401678711176\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.275617134728236e-06\n",
      "   total loss: 0.013826292404246487\n",
      "\n",
      " 7400 training iterations\n",
      "   Hubber loss: 0.0013581275707110763\n",
      "   Euclidean loss: 0.04756828460010884\n",
      "   BCE loss: 0.00012235164467711002\n",
      "   total loss: 0.0014804792153881863\n",
      "\n",
      " 7600 training iterations\n",
      "   Hubber loss: 0.33331719040870667\n",
      "   Euclidean loss: 0.1300330407786547\n",
      "   BCE loss: 0.0004065777757205069\n",
      "   total loss: 0.33372376818442717\n",
      "validation Loss: 0.2468\n",
      "training time: 2.0h 11m 47s\n",
      "\n",
      "Epoch 4/5\n",
      "----------\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss: 0.4161049723625183\n",
      "   Euclidean loss: 0.3448521189178608\n",
      "   BCE loss: 9.777701052371413e-05\n",
      "   total loss: 0.416202749373042\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss: 0.3836653232574463\n",
      "   Euclidean loss: 0.44787540734742814\n",
      "   BCE loss: 0.00015748641453683376\n",
      "   total loss: 0.3838228096719831\n",
      "\n",
      " 400 training iterations\n",
      "   Hubber loss: 0.17029424011707306\n",
      "   Euclidean loss: 0.19392930947786483\n",
      "   BCE loss: 9.509242227068171e-05\n",
      "   total loss: 0.17038933253934374\n",
      "\n",
      " 600 training iterations\n",
      "   Hubber loss: 0.08206421136856079\n",
      "   Euclidean loss: 0.13987482221257042\n",
      "   BCE loss: 6.350826151901856e-05\n",
      "   total loss: 0.08212771963007981\n",
      "\n",
      " 800 training iterations\n",
      "   Hubber loss: 0.17155979573726654\n",
      "   Euclidean loss: 0.35608973360724233\n",
      "   BCE loss: 3.519299934851006e-05\n",
      "   total loss: 0.17159498873661505\n",
      "\n",
      " 1000 training iterations\n",
      "   Hubber loss: 0.11412858963012695\n",
      "   Euclidean loss: 0.41467024082609577\n",
      "   BCE loss: 5.853251786902547e-05\n",
      "   total loss: 0.11418712214799598\n",
      "\n",
      " 1200 training iterations\n",
      "   Hubber loss: 0.21607637405395508\n",
      "   Euclidean loss: 0.05676328045558612\n",
      "   BCE loss: 2.111202775267884e-05\n",
      "   total loss: 0.21609748608170776\n",
      "\n",
      " 1400 training iterations\n",
      "   Hubber loss: 0.7913881540298462\n",
      "   Euclidean loss: 0.22376993390170863\n",
      "   BCE loss: 0.00011115464440081269\n",
      "   total loss: 0.791499308674247\n",
      "\n",
      " 1600 training iterations\n",
      "   Hubber loss: 0.10326816886663437\n",
      "   Euclidean loss: 0.16433870119938185\n",
      "   BCE loss: 2.7273410523775965e-05\n",
      "   total loss: 0.10329544227715814\n",
      "\n",
      " 1800 training iterations\n",
      "   Hubber loss: 0.1544801890850067\n",
      "   Euclidean loss: 0.12158073463695772\n",
      "   BCE loss: 6.798112008254975e-05\n",
      "   total loss: 0.15454817020508926\n",
      "\n",
      " 2000 training iterations\n",
      "   Hubber loss: 0.19948188960552216\n",
      "   Euclidean loss: 0.5586563907404063\n",
      "   BCE loss: 0.00010999522055499256\n",
      "   total loss: 0.19959188482607715\n",
      "\n",
      " 2200 training iterations\n",
      "   Hubber loss: 0.5210617780685425\n",
      "   Euclidean loss: 0.6258381596153815\n",
      "   BCE loss: 0.0001692295481916517\n",
      "   total loss: 0.5212310076167341\n",
      "\n",
      " 2400 training iterations\n",
      "   Hubber loss: 0.19371481239795685\n",
      "   Euclidean loss: 0.4716724380932781\n",
      "   BCE loss: 7.264839223353192e-05\n",
      "   total loss: 0.19378746079019038\n",
      "\n",
      " 2600 training iterations\n",
      "   Hubber loss: 0.1840430200099945\n",
      "   Euclidean loss: 0.222401641792845\n",
      "   BCE loss: 6.0660706367343664e-05\n",
      "   total loss: 0.18410368071636185\n",
      "\n",
      " 2800 training iterations\n",
      "   Hubber loss: 0.06587382405996323\n",
      "   Euclidean loss: 0.0965685424949238\n",
      "   BCE loss: 5.4463780543301255e-05\n",
      "   total loss: 0.06592828784050653\n",
      "\n",
      " 3000 training iterations\n",
      "   Hubber loss: 0.9407257437705994\n",
      "   Euclidean loss: 0.3414733770464719\n",
      "   BCE loss: 0.0001132516554207541\n",
      "   total loss: 0.9408389954260201\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 3200 training iterations\n",
      "   Hubber loss: 0.4388287663459778\n",
      "   Euclidean loss: 0.6122187900918922\n",
      "   BCE loss: 0.00014530037879012525\n",
      "   total loss: 0.4389740667247679\n",
      "\n",
      " 3400 training iterations\n",
      "   Hubber loss: 0.03901855647563934\n",
      "   Euclidean loss: 0.28381182664441623\n",
      "   BCE loss: 6.925973866600543e-05\n",
      "   total loss: 0.03908781621430535\n",
      "\n",
      " 3600 training iterations\n",
      "   Hubber loss: 0.32879167795181274\n",
      "   Euclidean loss: 0.34764733386719165\n",
      "   BCE loss: 0.0001843722420744598\n",
      "   total loss: 0.3289760501938872\n",
      "\n",
      " 3800 training iterations\n",
      "   Hubber loss: 0.142726868391037\n",
      "   Euclidean loss: 0.34785078340905623\n",
      "   BCE loss: 0.00011606147745624185\n",
      "   total loss: 0.14284292986849323\n",
      "\n",
      " 4000 training iterations\n",
      "   Hubber loss: 0.10890240222215652\n",
      "   Euclidean loss: 0.17873675440257802\n",
      "   BCE loss: 5.503498323378153e-05\n",
      "   total loss: 0.1089574372053903\n",
      "\n",
      " 4200 training iterations\n",
      "   Hubber loss: 0.5211203098297119\n",
      "   Euclidean loss: 0.33780703337279117\n",
      "   BCE loss: 0.00012491292727645487\n",
      "   total loss: 0.5212452227569884\n",
      "training Loss: 0.0536\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss: 0.007114403415471315\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 8.381030966120306e-06\n",
      "   total loss: 0.007122784446437436\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss: 2.2172866010805592e-05\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.2212168460100656e-07\n",
      "   total loss: 2.23949876954066e-05\n",
      "\n",
      " 400 training iterations\n",
      "   Hubber loss: 0.0008425085106864572\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0112086101798923e-06\n",
      "   total loss: 0.000843519719296637\n",
      "\n",
      " 600 training iterations\n",
      "   Hubber loss: 0.07290037721395493\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 7.625878879480297e-06\n",
      "   total loss: 0.0729080030928344\n",
      "\n",
      " 800 training iterations\n",
      "   Hubber loss: 0.05252818763256073\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.711202068894636e-05\n",
      "   total loss: 0.052555299653249676\n",
      "\n",
      " 1000 training iterations\n",
      "   Hubber loss: 0.0009541370673105121\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.3533114895381004e-07\n",
      "   total loss: 0.0009542723984594659\n",
      "\n",
      " 1200 training iterations\n",
      "   Hubber loss: 0.0004987397696822882\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.3276193417132163e-07\n",
      "   total loss: 0.0004989725316164595\n",
      "\n",
      " 1400 training iterations\n",
      "   Hubber loss: 0.00023298250744119287\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.375751932959247e-07\n",
      "   total loss: 0.0002332200826344888\n",
      "\n",
      " 1600 training iterations\n",
      "   Hubber loss: 1.6323573589324951\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.9551223633461632e-05\n",
      "   total loss: 1.6323869101561286\n",
      "\n",
      " 1800 training iterations\n",
      "   Hubber loss: 0.0009365268633700907\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0109369696920112e-07\n",
      "   total loss: 0.0009366279570670599\n",
      "\n",
      " 2000 training iterations\n",
      "   Hubber loss: 0.0007464879308827221\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.1726191334892064e-07\n",
      "   total loss: 0.0007467051927960711\n",
      "\n",
      " 2200 training iterations\n",
      "   Hubber loss: 0.0006232194136828184\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 4.064489530719584e-06\n",
      "   total loss: 0.000627283903213538\n",
      "\n",
      " 2400 training iterations\n",
      "   Hubber loss: 0.18877072632312775\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 6.483508514065761e-06\n",
      "   total loss: 0.1887772098316418\n",
      "\n",
      " 2600 training iterations\n",
      "   Hubber loss: 0.00041179361869581044\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.463268856445211e-06\n",
      "   total loss: 0.00041425688755225565\n",
      "\n",
      " 2800 training iterations\n",
      "   Hubber loss: 0.00031884340569376945\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.421311282001625e-07\n",
      "   total loss: 0.0003189855368219696\n",
      "\n",
      " 3000 training iterations\n",
      "   Hubber loss: 2.2731939679943025e-05\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 5.222180448072322e-07\n",
      "   total loss: 2.3254157724750257e-05\n",
      "\n",
      " 3200 training iterations\n",
      "   Hubber loss: 0.0015461122384294868\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 5.070939437246125e-07\n",
      "   total loss: 0.0015466193323732114\n",
      "\n",
      " 3400 training iterations\n",
      "   Hubber loss: 0.0011788747506216168\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 4.4324755776870006e-07\n",
      "   total loss: 0.0011793179981793855\n",
      "\n",
      " 3600 training iterations\n",
      "   Hubber loss: 5.910801974096103e-06\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 7.852929115870211e-07\n",
      "   total loss: 6.696094885683124e-06\n",
      "\n",
      " 3800 training iterations\n",
      "   Hubber loss: 0.0007684897282160819\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.2745538785784447e-07\n",
      "   total loss: 0.0007687171836039397\n",
      "\n",
      " 4000 training iterations\n",
      "   Hubber loss: 0.0007100931834429502\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.2312069347663055e-07\n",
      "   total loss: 0.0007103163041364269\n",
      "\n",
      " 4200 training iterations\n",
      "   Hubber loss: 0.05610429495573044\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 3.084432682953775e-05\n",
      "   total loss: 0.056135139282559976\n",
      "\n",
      " 4400 training iterations\n",
      "   Hubber loss: 0.0009127916418947279\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.9978255611240456e-07\n",
      "   total loss: 0.0009130914244508403\n",
      "\n",
      " 4600 training iterations\n",
      "   Hubber loss: 0.00011145234748255461\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.4123397679431946e-07\n",
      "   total loss: 0.00011169358145934893\n",
      "\n",
      " 4800 training iterations\n",
      "   Hubber loss: 0.0006200395291671157\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.2462458559857623e-07\n",
      "   total loss: 0.0006202641537527143\n",
      "\n",
      " 5000 training iterations\n",
      "   Hubber loss: 0.0007244052249006927\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.587062226666603e-07\n",
      "   total loss: 0.0007246639311233594\n",
      "\n",
      " 5200 training iterations\n",
      "   Hubber loss: 0.001915469765663147\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.4475255600009405e-07\n",
      "   total loss: 0.001915714518219147\n",
      "\n",
      " 5400 training iterations\n",
      "   Hubber loss: 0.0007180775282904506\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.9490269071175135e-07\n",
      "   total loss: 0.0007183724309811623\n",
      "\n",
      " 5600 training iterations\n",
      "   Hubber loss: 0.002350632566958666\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.5657703872639104e-07\n",
      "   total loss: 0.0023508891439973922\n",
      "\n",
      " 5800 training iterations\n",
      "   Hubber loss: 0.0006451127119362354\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 3.6591066532309924e-07\n",
      "   total loss: 0.0006454786226015585\n",
      "\n",
      " 6000 training iterations\n",
      "   Hubber loss: 0.0009214510209858418\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 9.106654630386402e-08\n",
      "   total loss: 0.0009215420875321456\n",
      "\n",
      " 6200 training iterations\n",
      "   Hubber loss: 0.00022903646458871663\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 7.116762503756036e-07\n",
      "   total loss: 0.00022974814083909223\n",
      "\n",
      " 6400 training iterations\n",
      "   Hubber loss: 0.0009677057969383895\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0000320571634802e-07\n",
      "   total loss: 0.0009678058001441059\n",
      "\n",
      " 6600 training iterations\n",
      "   Hubber loss: 0.0009452257654629648\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0344458445388227e-07\n",
      "   total loss: 0.0009453292100474187\n",
      "\n",
      " 6800 training iterations\n",
      "   Hubber loss: 0.0011922449339181185\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0294091623563872e-07\n",
      "   total loss: 0.0011923478748343541\n",
      "\n",
      " 7000 training iterations\n",
      "   Hubber loss: 0.015542679466307163\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.1967563952784985e-05\n",
      "   total loss: 0.015564647030259948\n",
      "\n",
      " 7200 training iterations\n",
      "   Hubber loss: 0.001419520121999085\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 3.846246727334801e-06\n",
      "   total loss: 0.0014233663687264198\n",
      "\n",
      " 7400 training iterations\n",
      "   Hubber loss: 0.002687301719561219\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.00010963957902276888\n",
      "   total loss: 0.002796941298583988\n",
      "\n",
      " 7600 training iterations\n",
      "   Hubber loss: 0.10258768498897552\n",
      "   Euclidean loss: 0.1445933670624404\n",
      "   BCE loss: 0.0003251504967920482\n",
      "   total loss: 0.10291283548576757\n",
      "validation Loss: 0.2028\n",
      "training time: 2.0h 57m 30s\n",
      "\n",
      "Epoch 5/5\n",
      "----------\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss: 0.07644739001989365\n",
      "   Euclidean loss: 0.33540213928458307\n",
      "   BCE loss: 7.848134555388242e-05\n",
      "   total loss: 0.07652587136544753\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss: 0.49510297179222107\n",
      "   Euclidean loss: 0.5361292216295741\n",
      "   BCE loss: 0.00013304685126058757\n",
      "   total loss: 0.49523601864348166\n",
      "\n",
      " 400 training iterations\n",
      "   Hubber loss: 0.2723679542541504\n",
      "   Euclidean loss: 0.6392439450102975\n",
      "   BCE loss: 0.00012385092850308865\n",
      "   total loss: 0.2724918051826535\n",
      "\n",
      " 600 training iterations\n",
      "   Hubber loss: 0.2608788013458252\n",
      "   Euclidean loss: 0.3435855658382803\n",
      "   BCE loss: 6.50135989417322e-05\n",
      "   total loss: 0.2609438149447669\n",
      "\n",
      " 800 training iterations\n",
      "   Hubber loss: 0.07655879855155945\n",
      "   Euclidean loss: 0.28837357821379395\n",
      "   BCE loss: 0.00010303829185431823\n",
      "   total loss: 0.07666183684341377\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 1000 training iterations\n",
      "   Hubber loss: 0.35412511229515076\n",
      "   Euclidean loss: 0.30648268323463773\n",
      "   BCE loss: 9.007938206195831e-05\n",
      "   total loss: 0.3542151916772127\n",
      "\n",
      " 1200 training iterations\n",
      "   Hubber loss: 0.21008449792861938\n",
      "   Euclidean loss: 0.5085668387208913\n",
      "   BCE loss: 8.732505375519395e-05\n",
      "   total loss: 0.21017182298237458\n",
      "\n",
      " 1400 training iterations\n",
      "   Hubber loss: 0.23055928945541382\n",
      "   Euclidean loss: 0.4525804117138987\n",
      "   BCE loss: 0.00012092375982319936\n",
      "   total loss: 0.23068021321523702\n",
      "\n",
      " 1600 training iterations\n",
      "   Hubber loss: 0.18587930500507355\n",
      "   Euclidean loss: 0.2356374582785201\n",
      "   BCE loss: 9.4414601335302e-05\n",
      "   total loss: 0.18597371960640885\n",
      "\n",
      " 1800 training iterations\n",
      "   Hubber loss: 0.014428911730647087\n",
      "   Euclidean loss: 0.14406135888745852\n",
      "   BCE loss: 3.76662501366809e-05\n",
      "   total loss: 0.014466577980783768\n",
      "\n",
      " 2000 training iterations\n",
      "   Hubber loss: 0.07637704908847809\n",
      "   Euclidean loss: 0.5055226765396192\n",
      "   BCE loss: 8.618024730822071e-05\n",
      "   total loss: 0.07646322933578631\n",
      "\n",
      " 2200 training iterations\n",
      "   Hubber loss: 0.06401664763689041\n",
      "   Euclidean loss: 0.0505575726863466\n",
      "   BCE loss: 5.707212767447345e-05\n",
      "   total loss: 0.06407371976456488\n",
      "\n",
      " 2400 training iterations\n",
      "   Hubber loss: 0.04494021087884903\n",
      "   Euclidean loss: 0.4574695747476599\n",
      "   BCE loss: 9.878010314423591e-05\n",
      "   total loss: 0.045038990981993265\n",
      "\n",
      " 2600 training iterations\n",
      "   Hubber loss: 0.14564958214759827\n",
      "   Euclidean loss: 0.5023006795175209\n",
      "   BCE loss: 0.00010700296115828678\n",
      "   total loss: 0.14575658510875655\n",
      "\n",
      " 2800 training iterations\n",
      "   Hubber loss: 0.07918629050254822\n",
      "   Euclidean loss: 0.20506415564594355\n",
      "   BCE loss: 7.03272016835399e-05\n",
      "   total loss: 0.07925661770423176\n",
      "\n",
      " 3000 training iterations\n",
      "   Hubber loss: 0.006894297432154417\n",
      "   Euclidean loss: 0.2555404731622717\n",
      "   BCE loss: 4.589388845488429e-05\n",
      "   total loss: 0.006940191320609301\n",
      "\n",
      " 3200 training iterations\n",
      "   Hubber loss: 0.06432797759771347\n",
      "   Euclidean loss: 0.5395781499159888\n",
      "   BCE loss: 6.432492227759212e-05\n",
      "   total loss: 0.06439230251999106\n",
      "\n",
      " 3400 training iterations\n",
      "   Hubber loss: 0.08475494384765625\n",
      "   Euclidean loss: 0.6141227380623966\n",
      "   BCE loss: 6.980724720051512e-05\n",
      "   total loss: 0.08482475109485677\n",
      "\n",
      " 3600 training iterations\n",
      "   Hubber loss: 0.026578310877084732\n",
      "   Euclidean loss: 0.4594013364680953\n",
      "   BCE loss: 7.255126547534019e-05\n",
      "   total loss: 0.026650862142560072\n",
      "\n",
      " 3800 training iterations\n",
      "   Hubber loss: 0.27887192368507385\n",
      "   Euclidean loss: 0.5993371247711924\n",
      "   BCE loss: 0.00011055612412746996\n",
      "   total loss: 0.2789824798092013\n",
      "\n",
      " 4000 training iterations\n",
      "   Hubber loss: 0.3577984571456909\n",
      "   Euclidean loss: 0.619891703743394\n",
      "   BCE loss: 0.00013771977683063596\n",
      "   total loss: 0.35793617692252155\n",
      "\n",
      " 4200 training iterations\n",
      "   Hubber loss: 0.044021014124155045\n",
      "   Euclidean loss: 0.5845437871505149\n",
      "   BCE loss: 8.406880078837276e-05\n",
      "   total loss: 0.04410508292494342\n",
      "training Loss: 0.0484\n",
      "\n",
      " 0 training iterations\n",
      "   Hubber loss: 0.021462978795170784\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.4521798220812343e-05\n",
      "   total loss: 0.021477500593391596\n",
      "\n",
      " 200 training iterations\n",
      "   Hubber loss: 0.00014898537483531982\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0444721709745863e-07\n",
      "   total loss: 0.00014908982205241728\n",
      "\n",
      " 400 training iterations\n",
      "   Hubber loss: 0.0019599469378590584\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.6347214543420705e-07\n",
      "   total loss: 0.0019602104100044926\n",
      "\n",
      " 600 training iterations\n",
      "   Hubber loss: 0.00886434968560934\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 3.6130062653683126e-05\n",
      "   total loss: 0.008900479748263024\n",
      "\n",
      " 800 training iterations\n",
      "   Hubber loss: 0.0005032008630223572\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.6884246835834347e-05\n",
      "   total loss: 0.0005300851098581916\n",
      "\n",
      " 1000 training iterations\n",
      "   Hubber loss: 0.00199129874818027\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0046908016647649e-07\n",
      "   total loss: 0.0019913992172604367\n",
      "\n",
      " 1200 training iterations\n",
      "   Hubber loss: 0.002234428422525525\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.4282695676447474e-07\n",
      "   total loss: 0.0022346712494822896\n",
      "\n",
      " 1400 training iterations\n",
      "   Hubber loss: 0.002673967042937875\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.0666507677115078e-07\n",
      "   total loss: 0.002674073708014646\n",
      "\n",
      " 1600 training iterations\n",
      "   Hubber loss: 0.1896069198846817\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.5015371647896245e-05\n",
      "   total loss: 0.1896319352563296\n",
      "\n",
      " 1800 training iterations\n",
      "   Hubber loss: 0.00203279173001647\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 5.867455499242169e-08\n",
      "   total loss: 0.0020328504045714624\n",
      "\n",
      " 2000 training iterations\n",
      "   Hubber loss: 0.0021628965623676777\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.1388989662464155e-07\n",
      "   total loss: 0.0021630104522643023\n",
      "\n",
      " 2200 training iterations\n",
      "   Hubber loss: 0.007941337302327156\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 6.129435405455297e-06\n",
      "   total loss: 0.007947466737732611\n",
      "\n",
      " 2400 training iterations\n",
      "   Hubber loss: 0.026385333389043808\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.1279562386334874e-05\n",
      "   total loss: 0.026396612951430143\n",
      "\n",
      " 2600 training iterations\n",
      "   Hubber loss: 0.009852518327534199\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 3.850034772767685e-06\n",
      "   total loss: 0.009856368362306966\n",
      "\n",
      " 2800 training iterations\n",
      "   Hubber loss: 0.002359073143452406\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 6.814735797888716e-08\n",
      "   total loss: 0.002359141290810385\n",
      "\n",
      " 3000 training iterations\n",
      "   Hubber loss: 0.0025478110183030367\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 3.00657546858929e-07\n",
      "   total loss: 0.0025481116758498956\n",
      "\n",
      " 3200 training iterations\n",
      "   Hubber loss: 0.010422265157103539\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.264161966219035e-07\n",
      "   total loss: 0.01042239157330016\n",
      "\n",
      " 3400 training iterations\n",
      "   Hubber loss: 0.002810910576954484\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 5.449560944725818e-07\n",
      "   total loss: 0.0028114555330489566\n",
      "\n",
      " 3600 training iterations\n",
      "   Hubber loss: 0.002528336364775896\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.4855689869364141e-06\n",
      "   total loss: 0.0025298219337628325\n",
      "\n",
      " 3800 training iterations\n",
      "   Hubber loss: 0.001794090960174799\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 7.448088723549517e-08\n",
      "   total loss: 0.0017941654410620345\n",
      "\n",
      " 4000 training iterations\n",
      "   Hubber loss: 0.0021619948092848063\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 8.868367018521894e-08\n",
      "   total loss: 0.0021620834929549915\n",
      "\n",
      " 4200 training iterations\n",
      "   Hubber loss: 0.014365015551447868\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.3985047189635225e-05\n",
      "   total loss: 0.014379000598637504\n",
      "\n",
      " 4400 training iterations\n",
      "   Hubber loss: 0.0019104329403489828\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.06523430076777e-06\n",
      "   total loss: 0.0019114981746497506\n",
      "\n",
      " 4600 training iterations\n",
      "   Hubber loss: 0.005353899672627449\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.8188639216987212e-07\n",
      "   total loss: 0.005354081559019619\n",
      "\n",
      " 4800 training iterations\n",
      "   Hubber loss: 0.002019376726821065\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.1011734812882423e-07\n",
      "   total loss: 0.0020194868441691938\n",
      "\n",
      " 5000 training iterations\n",
      "   Hubber loss: 0.001592731336131692\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 2.348819378994449e-07\n",
      "   total loss: 0.0015929662180695914\n",
      "\n",
      " 5200 training iterations\n",
      "   Hubber loss: 0.0003567842941265553\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 8.771539938834394e-08\n",
      "   total loss: 0.00035687200952594367\n",
      "\n",
      " 5400 training iterations\n",
      "   Hubber loss: 0.0010927662951871753\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 9.679879298118976e-08\n",
      "   total loss: 0.0010928630939801565\n",
      "\n",
      " 5600 training iterations\n",
      "   Hubber loss: 0.0015668015694245696\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 7.077696295709757e-08\n",
      "   total loss: 0.0015668723463875267\n",
      "\n",
      " 5800 training iterations\n",
      "   Hubber loss: 0.0009990943362936378\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 3.4064962051161274e-07\n",
      "   total loss: 0.0009994349859141494\n",
      "\n",
      " 6000 training iterations\n",
      "   Hubber loss: 0.0020500877872109413\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 4.3264961391287216e-08\n",
      "   total loss: 0.0020501310521723326\n",
      "\n",
      " 6200 training iterations\n",
      "   Hubber loss: 0.001242260099388659\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 5.362870069802739e-06\n",
      "   total loss: 0.0012476229694584617\n",
      "\n",
      " 6400 training iterations\n",
      "   Hubber loss: 0.0020717703737318516\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 5.602788988312568e-08\n",
      "   total loss: 0.0020718264016217347\n",
      "\n",
      " 6600 training iterations\n",
      "   Hubber loss: 0.002036667661741376\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 5.804970726330794e-08\n",
      "   total loss: 0.0020367257114486392\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 6800 training iterations\n",
      "   Hubber loss: 0.00030907863401807845\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 7.294500846910523e-06\n",
      "   total loss: 0.00031637313486498897\n",
      "\n",
      " 7000 training iterations\n",
      "   Hubber loss: 0.05805201455950737\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 8.687548870511819e-06\n",
      "   total loss: 0.05806070210837788\n",
      "\n",
      " 7200 training iterations\n",
      "   Hubber loss: 0.03420364111661911\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 1.4586008546757512e-05\n",
      "   total loss: 0.03421822712516587\n",
      "\n",
      " 7400 training iterations\n",
      "   Hubber loss: 0.004011834505945444\n",
      "   Euclidean loss: 0.0\n",
      "   BCE loss: 0.0001287074846914038\n",
      "   total loss: 0.004140541990636848\n",
      "\n",
      " 7600 training iterations\n",
      "   Hubber loss: 0.2608555853366852\n",
      "   Euclidean loss: 0.15175308305060195\n",
      "   BCE loss: 0.00035882348311133683\n",
      "   total loss: 0.2612144088197965\n",
      "validation Loss: 0.1399\n",
      "training time: 3.0h 40m 47s\n",
      "\n",
      "Training complete in 3.0h 40m 47s\n"
     ]
    }
   ],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_12.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet_det.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                                 --hyperparameter_set=$hyp_st --output_name=$out  \\\n",
    "                                 --pipeline=$pipeline --dest_folder=$dest_folder\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"1.2V\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing -- ablation study (real scenes)<a name=\"1.2T\"></a>\n",
    "\n",
    "Here we will test the output from top performing pipelines on real imagery. The following cell loops through a set of WV03 rasters, split them into patches and tries both classifying and counting and just counting. Finally, we can compare model predictions to those obtained with human observers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13662 tiles created in 0 minutes and 23.54 seconds\n",
      "\n",
      "Predicting with NasnetAcount:\n",
      "Traceback (most recent call last):\n",
      "  File \"predict_raster_det.py\", line 279, in <module>\n",
      "    main()\n",
      "  File \"predict_raster_det.py\", line 211, in main\n",
      "    torch.load(\"./saved_models_stable/Pipeline1.1/{}/{}.tar\".format(model_name, model_name)))\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/serialization.py\", line 301, in load\n",
      "    f = open(f, 'rb')\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './saved_models_stable/Pipeline1.1/NasnetAcount_ts-binary/NasnetAcount_ts-binary.tar'\n",
      "\n",
      "13662 tiles created in 0 minutes and 22.30 seconds\n",
      "\n",
      "Predicting with CountCeption:\n",
      "Traceback (most recent call last):\n",
      "  File \"predict_raster_det.py\", line 279, in <module>\n",
      "    main()\n",
      "  File \"predict_raster_det.py\", line 211, in main\n",
      "    torch.load(\"./saved_models_stable/Pipeline1.1/{}/{}.tar\".format(model_name, model_name)))\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/serialization.py\", line 301, in load\n",
      "    f = open(f, 'rb')\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './saved_models_stable/Pipeline1.1/CountCeption_ts-binary/CountCeption_ts-binary.tar'\n",
      "\n",
      "13662 tiles created in 0 minutes and 22.43 seconds\n",
      "\n",
      "Predicting with Resnet18count:\n",
      "Traceback (most recent call last):\n",
      "  File \"predict_raster_det.py\", line 279, in <module>\n",
      "    main()\n",
      "  File \"predict_raster_det.py\", line 211, in main\n",
      "    torch.load(\"./saved_models_stable/Pipeline1.1/{}/{}.tar\".format(model_name, model_name)))\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/serialization.py\", line 301, in load\n",
      "    f = open(f, 'rb')\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './saved_models_stable/Pipeline1.1/Resnet18count_ts-binary/Resnet18count_ts-binary.tar'\n",
      "\n",
      "13662 tiles created in 0 minutes and 23.39 seconds\n",
      "\n",
      "Predicting with WideResnetCount:\n",
      "Traceback (most recent call last):\n",
      "  File \"predict_raster_det.py\", line 279, in <module>\n",
      "    main()\n",
      "  File \"predict_raster_det.py\", line 211, in main\n",
      "    torch.load(\"./saved_models_stable/Pipeline1.1/{}/{}.tar\".format(model_name, model_name)))\n",
      "  File \"/home/bento/anaconda3/lib/python3.6/site-packages/torch/serialization.py\", line 301, in load\n",
      "    f = open(f, 'rb')\n",
      "FileNotFoundError: [Errno 2] No such file or directory: './saved_models_stable/Pipeline1.1/WideResnetCount_ts-binary/WideResnetCount_ts-binary.tar'\n",
      "\n",
      "13662 tiles created in 0 minutes and 22.13 seconds\n",
      "\n",
      "Predicting with UnetDet:\n",
      "Testing complete in 0.0h 7m 40s\n",
      "    Total predicted in test.tif:  474\n"
     ]
    }
   ],
   "source": [
    "# create combinations for testing just counting\n",
    "combs_count = pd.DataFrame({'arch': ['NasnetAcount', 'CountCeption',\n",
    "                                     'Resnet18count', 'WideResnetCount'],\n",
    "                            'hyp_st': ['B'] * 2 + ['A'] * 2})\n",
    "\n",
    "# and just detecting\n",
    "combs_det = pd.DataFrame({'arch': ['UnetDet'],\n",
    "                          'hyp_st': ['B']})\n",
    "\n",
    "# iterate through test images\n",
    "test_imgs = [img for img in os.listdir('./test_scenes') if img[-4:] == '.tif']\n",
    "for img in test_imgs:\n",
    "    img_path = './test_scenes/{}'.format(img)\n",
    "    \n",
    "    # iterate through count CNN combinations\n",
    "    for row in combs_count.iterrows():\n",
    "        arch = row[1]['arch']\n",
    "        hyp_st = row[1]['hyp_st']\n",
    "        out_fldr = 'test_scenes/{}'.format(arch)\n",
    "        # get results for counting\n",
    "        !python predict_raster_det.py --input_image=$img_path \\\n",
    "                                      --class_architecture='NasnetA' --count_architecture=$arch \\\n",
    "                                      --hyperparameter_set_class='B' --hyperparameter_set_count=$hyp_st \\\n",
    "                                      --training_dir='training_set_binary' --dest_folder=$out_fldr \\\n",
    "                                      --skip_class='1'\n",
    "    \n",
    "    # iterate through detection CNN combinations\n",
    "    for row in combs_det.iterrows():\n",
    "        arch = row[1]['arch']\n",
    "        hyp_st = row[1]['hyp_st']\n",
    "        out_fldr = 'test_scenes/{}'.format(arch)\n",
    "        # get results for detection\n",
    "        !python predict_raster_det.py --input_image=$img_path \\\n",
    "                                      --class_architecture='NasnetA' --det_architecture=$arch \\\n",
    "                                      --hyperparameter_set_class='B' --hyperparameter_set_count=$hyp_st \\\n",
    "                                      --training_dir='training_set_binary' --dest_folder=$out_fldr \\\n",
    "                                      --skip_class='1'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifying + counting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create combinations for testing count + classify\n",
    "combinations_class_count = pd.DataFrame()\n",
    "combinations_class_det = pd.DataFrame()\n",
    "\n",
    "# define architectures we wish to test\n",
    "class_archs = ['NasnetA', 'Densenet169']\n",
    "count_archs = ['WideResnetCount', 'CountCeption']\n",
    "det_archs = ['UnetDet']\n",
    "t_sets = ['training_set_vanilla', 'training_set_binary']\n",
    "hyp_sets = {'NasnetA': 'B', 'Densenet169': 'B', 'CountCeption': 'B',\n",
    "            'WideResnetCount': 'A', 'UnetDet': 'B'}\n",
    "\n",
    "# add combinations \n",
    "for class_arch in class_archs:\n",
    "    for t_set in t_sets:\n",
    "        for count_arch in count_archs:\n",
    "            combinations_class_count = combinations_class_count.append(\n",
    "                pd.Series({'class_arch': class_arch, 'count_arch': count_arch, \n",
    "                           'hyp_class': hyp_sets[class_arch], \n",
    "                           'hyp_count': hyp_sets[count_arch],\n",
    "                           't_set': t_set}), ignore_index=True)\n",
    "        for det_arch in det_archs:\n",
    "            combinations_det_count = combinations_det_count.append(\n",
    "                pd.Series({'class_arch': class_arch, 'det_arch': count_arch, \n",
    "                           'hyp_class': hyp_sets[class_arch], \n",
    "                           'hyp_count': hyp_sets[count_arch],\n",
    "                           't_set': t_set}), ignore_index=True)\n",
    "        \n",
    "\n",
    "# iterate through test images\n",
    "test_imgs = [img for img in os.listdir('./test_scenes') if img[-4:] == '.tif']\n",
    "for img in test_imgs:\n",
    "    img_path = 'test_scenes/{}'.format(img)\n",
    "    # iterate through combinations -- counting\n",
    "    for row in combinations_class_count.iterrows():\n",
    "        cnt_arch = row[1]['count_arch']\n",
    "        cls_arch = row[1]['class_arch']\n",
    "        hyp_cnt = row[1]['hyp_count']\n",
    "        hyp_cls = row[1]['hyp_class']\n",
    "        t_set = row[1]['t_set']\n",
    "        # find positive classes\n",
    "        if t_set.split('_')[-1] == 'binary':\n",
    "            pos_classes = 'seal'\n",
    "        else:\n",
    "            pos_classes='crabeater_weddell'\n",
    "        out_fldr = 'test_scenes/{}-{}-{}'.format(cls_arch, cnt_arch, t_set.split('_')[-1])\n",
    "        # check if combination was already tried\n",
    "        if os.path.exists('./{}/predicted_shapefiles/{}'.format(out_fldr, img[:-4])):\n",
    "            print('Already classified {} with {}'.format(img, os.path.basename(out_fldr)))\n",
    "            continue\n",
    "        !python predict_raster_det.py --input_image=$img_path \\\n",
    "                                      --class_architecture=$cls_arch --count_architecture=$cnt_arch \\\n",
    "                                      --hyperparameter_set_class=$hyp_cls --hyperparameter_set_count=$hyp_cnt \\\n",
    "                                      --training_dir=$tset --dest_folder=$out_fldr \\\n",
    "                                      --pos_classes=$pos_classes\n",
    "    # iterate through combinations -- detecting\n",
    "    for row in combinations_class_det.iterrows():\n",
    "        det_arch = row[1]['det_arch']\n",
    "        cls_arch = row[1]['class_arch']\n",
    "        hyp_cnt = row[1]['hyp_count']\n",
    "        hyp_cls = row[1]['hyp_class']\n",
    "        t_set = row[1]['t_set']\n",
    "        # find positive classes\n",
    "        if t_set.split('_')[-1] == 'binary':\n",
    "            pos_classes = 'seal'\n",
    "        else:\n",
    "            pos_classes='crabeater_weddell'\n",
    "        out_fldr = 'test_scenes/{}-{}'.format(cls_arch, cnt_arch, t_set.split('_')[-1])\n",
    "        # check if combination was already tried\n",
    "        if os.path.exists('./{}/predicted_shapefiles/{}'.format(out_fldr, img[:-4])):\n",
    "            print('Already classified {} with {}'.format(img, os.path.basename(out_fldr)))\n",
    "            continue\n",
    "        !python predict_raster_det.py --input_image=$img_path \\\n",
    "                                      --class_architecture=$cls_arch --det_architecture=$det_arch \\\n",
    "                                      --hyperparameter_set_class=$hyp_cls --hyperparameter_set_count=$hyp_cnt \\\n",
    "                                      --training_dir=$tset --dest_folder=$out_fldr \\\n",
    "                                      --pos_classes=$pos_classes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
