{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SealNet Detection Pipeline -- Training / Validation\n",
    "---\n",
    "\n",
    "This jupyter notebook will go through training CNN within the SealNet pipeline. Requires a training set generated on the [training_set_generation](https://github.com/iceberg-project/Seals/blob/paper/SealNet_code/training_set_generation.ipynb) notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "---\n",
    "* [Getting started](#intro)\n",
    "    * [Setup](#setup)\n",
    "    * [Visualize training set](#vis_imgs)\n",
    "* [Pipeline 1 - Heatmap](#1)\n",
    "    * [Training](#1T)\n",
    "    * [Validation](#1V)\n",
    "* [Pipeline 2.1 - Heatmap + count](#2.1)\n",
    "    * [Training](#2.1T)\n",
    "    * [Validation](#2.1V)\n",
    "* [Pipeline 2.2 - Heatmap + occupancy](#2.2)\n",
    "    * [Training](#2.2T)\n",
    "    * [Validation](#2.2V)\n",
    "* [Pipeline 3 - Heatmap + count + occupancy](#3)\n",
    "    * [Training](#3T)\n",
    "    * [Validation](#3V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started<a name=\"intro\"></a>\n",
    "---\n",
    "\n",
    "If you followed the *training_set_generation* jupyter notebook (also present in this repo), you should have training sets generated and hyperparameter sets to try out, and be ready to search for a best performing seal detection pipeline.  Output files in this repository are organized as follows: *'./{dest_folder}/{pipeline}/{model_settings}/{model_settings}_{file}'*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment<a name=\"setup\"></a>\n",
    "\n",
    "Before training and validating model/hyperparameter combinations inside the pipelines, we need to load the required python modules and a few global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib as mpl\n",
    "from functools import reduce\n",
    "from utils.model_library import * \n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi']= 400\n",
    "\n",
    "# destination folder for saved models and model stats\n",
    "dest_folder = 'saved_models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training images (Optional)<a name=\"vis_imgs\"></a>\n",
    "\n",
    "To get a better sense for what the training set is like, the next cell will display a few random images from the training classes. Displayed images are extracted from a pool of ~75000 training images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save class names\n",
    "class_names = sorted([subdir for subdir in os.listdir('./training_sets/training_set_vanilla/training')])\n",
    "# store images\n",
    "images = []\n",
    "\n",
    "# loop over labels\n",
    "for label in class_names:\n",
    "    for path, _, files in os.walk('./training_sets/training_set_vanilla/training/{}'.format(label)):\n",
    "        files = np.random.choice(files, 5)\n",
    "        for filename in files:\n",
    "            images.append(np.asarray(Image.open(os.path.join(path, filename))))\n",
    "\n",
    "images = np.array(images)\n",
    "\n",
    "# display images \n",
    "ncols=len(class_names)\n",
    "nindex, height, width, intensity = images.shape\n",
    "nrows = nindex // ncols\n",
    "# check if rows and columns can fit the number of images\n",
    "assert nindex == nrows * ncols\n",
    "result = (images.reshape(nrows, ncols, height, width, intensity)\n",
    "          .swapaxes(1,2)\n",
    "          .reshape(height*nrows, width*ncols, intensity))\n",
    "\n",
    "plt.imshow(result)\n",
    "cur_axes = plt.gca()\n",
    "cur_axes.axes.get_xaxis().set_visible(False)\n",
    "cur_axes.axes.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1 - Heatmap models <a name=\"1\"></a>\n",
    "---\n",
    "\n",
    "Heatmap models work by using semantic segmentation to generate a pixel-wise probability of a cell being a seal centroid. With a matrix of probabilities, we obtain a count by applying a sigmoid transform to it, thresholding and adding over all cells. Seal locations on an image are determined by finding the *n* greatest peaks of intensity in the image, where *n* is the count for that image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"1T\"></a>\n",
    "\n",
    "The following cell trains heatmap based models on a selected combination of architectures, training sets and hyperparameters. For valid entries, see \"*/utils/model_library.py*\" within the main repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch pipeline\n",
    "pipeline = 'Heatmap'\n",
    "\n",
    "# generate model combinations\n",
    "combinations_1 = {'model_architecture': ['Unet'],\n",
    "                  'training_dir': ['training_set_vanilla'],\n",
    "                  'hyperparameter_set': ['D']}\n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_1 = pd.DataFrame(combinations_1)\n",
    "                    \n",
    "# create folders for resulting files\n",
    "for row in combinations_1.iterrows():\n",
    "    mdl = row[1]['model_architecture'] + '_ts-' + row[1]['training_dir'].split('_')[-1]              \n",
    "    if not os.path.exists(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)):\n",
    "        os.makedirs(\"{}/{}/{}\".format(dest_folder, pipeline, mdl)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then provide model combinations created above as arguments to the training script, *train_sealnet.py*. A list of required arguments can be displayed by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run train_sealnet.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training Unet_ts-vanilla\n",
      "\n",
      "Epoch 1/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_1.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                             --hyperparameter_set=$hyp_st \\\n",
    "                             --output_name=$out --models_folder=$dest_folder\n",
    "                              \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"1V\"></a>\n",
    "\n",
    "This script validates models from training, generating .csvs with precision and recall on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'combinations_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-297379bfecad>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# iterate over combinations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcombinations_1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# read hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mt_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0march\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhyp_st\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training_dir'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_architecture'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'combinations_1' is not defined"
     ]
    }
   ],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_1.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is trained\n",
    "    if \"{}.tar\".format(out) not in os.listdir('{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was not trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo validating $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python validate_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                                --hyperparameter_set=$hyp_st \\\n",
    "                                --output_name=$out --models_folder=$dest_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 2.1 - Heatmap + count<a name=\"1.1\"></a>\n",
    "---\n",
    "\n",
    "This pipeline will also generate a seal intensity heatmap like the previous one, but it does not add over pixels to get a count. Instead, Heatmap + count models have a specialized branch to get a count by regression. Seal locations on an image are determined by finding the *n* greatest peaks of intensity in the image, where *n* is the count for that image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"2.1T\"></a>\n",
    "\n",
    "The following cell trains heatmap + count based models on a selected combination of architectures, training sets and hyperparameters. For valid entries, see \"*/utils/model_library.py*\" within the main repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch pipeline\n",
    "pipeline = 'Heatmap-Cnt'\n",
    "\n",
    "# generate model combinations\n",
    "combinations_21 = {'model_architecture': ['UnetCntWRN'],\n",
    "                   'training_dir': ['training_set_vanilla'],\n",
    "                   'hyperparameter_set': ['D']}       \n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_21 = pd.DataFrame(combinations_21)\n",
    "                    \n",
    "\n",
    "# create folders for resulting files\n",
    "for row in combinations_21.iterrows():\n",
    "    mdl = row[1]['model_architecture'] + '_ts-' + row[1]['training_dir'].split('_')[-1]               \n",
    "    if not os.path.exists(\"{}/{}/{}\".format(dest_folder, pipeline, mdl)):\n",
    "        os.makedirs(\"{}/{}/{}\".format(dest_folder, pipeline, mdl)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_21.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                             --hyperparameter_set=$hyp_st \\\n",
    "                             --output_name=$out --models_folder=$dest_folder\n",
    "                              \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"2.1V\"></a>\n",
    "\n",
    "This script validates models from training, generating .csvs with precision and recall on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_21.iterrows():\n",
    "    \n",
    "   # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is trained\n",
    "    if \"{}.tar\".format(out) not in os.listdir('{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was not trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo validating $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python validate_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                                --hyperparameter_set=$hyp_st \\\n",
    "                                --output_name=$out --models_folder=$dest_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 2.2 - Heatmap + occupancy <a name=\"1.2\"></a>\n",
    "---\n",
    "\n",
    "Heatmap + occupancy models, like pure Heatmap models, work by using semantic segmentation to generate a pixel-wise probability of a cell being a seal centroid. With a matrix of probabilities, we obtain a count by applying a sigmoid transform to it, thresholding and adding over all cells. After addining up cells, however, we use the output from a specialized occupancy branch -- and another threshold -- to decide if predicted counts will be set to zero (i.e. image is not occupied). Seal locations on an image are determined by finding the *n* greatest peaks of intensity in the image, where *n* is the count for that image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"2.2T\"></a>\n",
    "\n",
    "The following cell trains heatmap + occupancy based models on a selected combination of architectures, training sets and hyperparameters. For valid entries, see \"*/utils/model_library.py*\" within the main repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch pipeline\n",
    "pipeline = 'Heatmap-Occ'\n",
    "\n",
    "# generate model combinations\n",
    "combinations_22 = {'model_architecture': ['UnetOccDense'],\n",
    "                   'training_dir': ['training_set_vanilla'],\n",
    "                   'hyperparameter_set': ['D'] }       \n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_22 = pd.DataFrame(combinations_22)\n",
    "                    \n",
    "\n",
    "# create folders for resulting files\n",
    "for row in combinations_22.iterrows():\n",
    "    mdl = row[1]['model_architecture'] + '_ts-' + row[1]['training_dir'].split('_')[-1]                  \n",
    "    if not os.path.exists(\"{}/{}/{}\".format(dest_folder, pipeline, mdl)):\n",
    "        os.makedirs(\"{}/{}/{}\".format(dest_folder, pipeline, mdl)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_22.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        #continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                             --hyperparameter_set=$hyp_st --output_name=$out  \\\n",
    "                             --models_folder=$dest_folder\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"2.2V\"></a>\n",
    "\n",
    "This script validates models from training, generating .csvs with precision and recall on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_22.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is trained\n",
    "    if \"{}.tar\".format(out) not in os.listdir('{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was not trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo validating $out\n",
    "    print()\n",
    "    \n",
    "    # run validation\n",
    "    !python validate_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                                --hyperparameter_set=$hyp_st \\\n",
    "                                --output_name=$out --models_folder=$dest_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 3 - Heatmap + Count + Occupancy<a name=\"1.3T\"></a>\n",
    "\n",
    "Like all previous ones, this pipeline will also generate a seal intensity heatmap. Similar to Heatmap + count models, models on this pipeline have a specialized branch to get a count by regression. After counting, however, we use a threshold and the output from a specialized occupancy branch to decide if we will set the count to zero (i.e. image is not occupied). Seal locations on an image are determined by finding the *n* greatest peaks of intensity in the image, where *n* is the count for that image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"3T\"></a>\n",
    "\n",
    "The following cell trains heatmap + count + occupancy based models on a selected combination of architectures, training sets and hyperparameters. For valid entries, see \"*/utils/model_library.py*\" within the main repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch pipeline\n",
    "pipeline = 'Heatmap-Cnt-Occ'\n",
    "\n",
    "# generate model combinations\n",
    "combinations_3 = {'model_architecture': ['UnetCntWRNOccDense'],\n",
    "                   'training_dir': ['training_set_vanilla'],\n",
    "                   'hyperparameter_set': ['D'] }       \n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_3 = pd.DataFrame(combinations_3)\n",
    "                    \n",
    "\n",
    "# create folders for resulting files\n",
    "for row in combinations_3.iterrows():\n",
    "    mdl = row[1]['model_architecture'] + '_ts-' + row[1]['training_dir'].split('_')[-1]                  \n",
    "    if not os.path.exists(\"{}/{}/{}\".format(dest_folder, pipeline, mdl)):\n",
    "        os.makedirs(\"{}/{}/{}\".format(dest_folder, pipeline, mdl)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "training UnetCntWRNOccDense_ts-vanilla\n",
      "\n",
      "Epoch 1/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 2.3010823183406903\n",
      "count loss: 0.45830215521716555\n",
      "occupancy loss: 0.7644140137030223\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.5803011048680082\n",
      "count loss: 0.2466599441661196\n",
      "occupancy loss: 0.3527543436242526\n",
      "training time: 0.0h 15m 37s\n",
      "\n",
      "Epoch 2/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.7834362572332068\n",
      "count loss: 0.36345044802402426\n",
      "occupancy loss: 0.4758972249235825\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.33263532237665133\n",
      "count loss: 0.2201538124290163\n",
      "occupancy loss: 0.7488286396566401\n",
      "training time: 0.0h 31m 11s\n",
      "\n",
      "Epoch 3/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6611745876683544\n",
      "count loss: 0.3045333038118967\n",
      "occupancy loss: 0.36930637492276613\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.29965631509554996\n",
      "count loss: 0.1687900461457432\n",
      "occupancy loss: 0.34506139076526554\n",
      "training time: 0.0h 46m 57s\n",
      "\n",
      "Epoch 4/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6439267598760787\n",
      "count loss: 0.26024411624937704\n",
      "occupancy loss: 0.3282535105210484\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28536100884814425\n",
      "count loss: 0.17374157578493507\n",
      "occupancy loss: 0.41709155599422393\n",
      "training time: 1.0h 2m 23s\n",
      "\n",
      "Epoch 5/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6360301788245817\n",
      "count loss: 0.2278529443742669\n",
      "occupancy loss: 0.2871984934718442\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2837969725688636\n",
      "count loss: 0.15722654763474933\n",
      "occupancy loss: 0.20700921361048627\n",
      "training time: 1.0h 18m 6s\n",
      "\n",
      "Epoch 6/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6358950209101465\n",
      "count loss: 0.19787951032493\n",
      "occupancy loss: 0.2723887227555172\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2827708290001501\n",
      "count loss: 0.1540291625403581\n",
      "occupancy loss: 0.24197233732589363\n",
      "training time: 1.0h 33m 59s\n",
      "\n",
      "Epoch 7/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6450794049022431\n",
      "count loss: 0.19111239476704653\n",
      "occupancy loss: 0.24444337877339978\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2816047677565054\n",
      "count loss: 0.12129713943245653\n",
      "occupancy loss: 0.16942685359333762\n",
      "training time: 1.0h 49m 30s\n",
      "\n",
      "Epoch 8/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6296603365973339\n",
      "count loss: 0.17043126481932902\n",
      "occupancy loss: 0.2397712531940014\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2809808671880374\n",
      "count loss: 0.11894192117902701\n",
      "occupancy loss: 0.5804718941541692\n",
      "training time: 2.0h 4m 52s\n",
      "\n",
      "Epoch 9/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6215441330281681\n",
      "count loss: 0.16053500102864582\n",
      "occupancy loss: 0.22130673657467959\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28133035926432587\n",
      "count loss: 0.0937170067668262\n",
      "occupancy loss: 0.18152161231412733\n",
      "training time: 2.0h 20m 12s\n",
      "\n",
      "Epoch 10/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6281194409174953\n",
      "count loss: 0.15490411698730822\n",
      "occupancy loss: 0.22323548652343272\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28107504897176133\n",
      "count loss: 0.07730298013261132\n",
      "occupancy loss: 0.2633626948019196\n",
      "training time: 2.0h 35m 33s\n",
      "\n",
      "Epoch 11/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6284210887001718\n",
      "count loss: 0.1495216181440902\n",
      "occupancy loss: 0.20503561475485163\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2863911184213375\n",
      "count loss: 0.08621448563253227\n",
      "occupancy loss: 0.22232822754491152\n",
      "training time: 2.0h 50m 55s\n",
      "\n",
      "Epoch 12/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6326300253079165\n",
      "count loss: 0.14547609409432702\n",
      "occupancy loss: 0.2191729799093067\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2815303815259849\n",
      "count loss: 0.0798417295634726\n",
      "occupancy loss: 0.4720423125011884\n",
      "training time: 3.0h 6m 17s\n",
      "\n",
      "Epoch 13/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6454004719764291\n",
      "count loss: 0.13774656233636198\n",
      "occupancy loss: 0.194520468264181\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2816254492656306\n",
      "count loss: 0.07896968120222132\n",
      "occupancy loss: 0.1428581833947426\n",
      "training time: 3.0h 21m 40s\n",
      "\n",
      "Epoch 14/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6402757749356792\n",
      "count loss: 0.12934346346826542\n",
      "occupancy loss: 0.1982450570450574\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2808319457122316\n",
      "count loss: 0.057919156929328794\n",
      "occupancy loss: 0.13898967986207814\n",
      "training time: 3.0h 37m 2s\n",
      "\n",
      "Epoch 15/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6397154755057619\n",
      "count loss: 0.12663949951592263\n",
      "occupancy loss: 0.18996517733370646\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28048372694088325\n",
      "count loss: 0.07623685830606082\n",
      "occupancy loss: 0.10410280939696863\n",
      "training time: 3.0h 52m 23s\n",
      "\n",
      "Epoch 16/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6446869437226326\n",
      "count loss: 0.1277931205252788\n",
      "occupancy loss: 0.18329558143700594\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2808155236122185\n",
      "count loss: 0.08908813465362048\n",
      "occupancy loss: 0.07284395885065617\n",
      "training time: 4.0h 7m 49s\n",
      "\n",
      "Epoch 17/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6226626366387226\n",
      "count loss: 0.11560865492768875\n",
      "occupancy loss: 0.18302012650369537\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2803378200657804\n",
      "count loss: 0.0795368426060443\n",
      "occupancy loss: 0.08433484183616184\n",
      "training time: 4.0h 23m 13s\n",
      "\n",
      "Epoch 18/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6385067407513066\n",
      "count loss: 0.11829543724839979\n",
      "occupancy loss: 0.16704421054367585\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28085251900199965\n",
      "count loss: 0.11832944607125527\n",
      "occupancy loss: 0.1052996041689055\n",
      "training time: 4.0h 38m 38s\n",
      "\n",
      "Epoch 19/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6356263163820355\n",
      "count loss: 0.11494285216462871\n",
      "occupancy loss: 0.17584287288247713\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28041515533089\n",
      "count loss: 0.07177817707989063\n",
      "occupancy loss: 0.1695120931078064\n",
      "training time: 4.0h 54m 3s\n",
      "\n",
      "Epoch 20/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6261715880584872\n",
      "count loss: 0.11172724972481704\n",
      "occupancy loss: 0.17352989168204838\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28039867756025244\n",
      "count loss: 0.08014642075009103\n",
      "occupancy loss: 0.19159840714799956\n",
      "training time: 5.0h 9m 27s\n",
      "\n",
      "Epoch 21/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6233526465946935\n",
      "count loss: 0.1074289579583955\n",
      "occupancy loss: 0.167329791882419\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2808284942287127\n",
      "count loss: 0.13878108360976688\n",
      "occupancy loss: 0.10131566396502242\n",
      "training time: 5.0h 24m 50s\n",
      "\n",
      "Epoch 22/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6278015983167916\n",
      "count loss: 0.1038995091035035\n",
      "occupancy loss: 0.15890373927217333\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2803859152166739\n",
      "count loss: 0.06197145270475696\n",
      "occupancy loss: 0.07930603769529485\n",
      "training time: 5.0h 40m 16s\n",
      "\n",
      "Epoch 23/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6218375696975588\n",
      "count loss: 0.0998293597069914\n",
      "occupancy loss: 0.16271954636632574\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28045225187765027\n",
      "count loss: 0.05289767459014613\n",
      "occupancy loss: 0.1695619527379545\n",
      "training time: 5.0h 55m 39s\n",
      "\n",
      "Epoch 24/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6394276933933263\n",
      "count loss: 0.09922602661995535\n",
      "occupancy loss: 0.16558199100871193\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28027700443315734\n",
      "count loss: 0.06147548806946735\n",
      "occupancy loss: 0.09168990256730669\n",
      "training time: 6.0h 11m 5s\n",
      "\n",
      "Epoch 25/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6360635908619545\n",
      "count loss: 0.09787805503852626\n",
      "occupancy loss: 0.15065135845091818\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2802934618513667\n",
      "count loss: 0.04349661341831789\n",
      "occupancy loss: 0.09859332142477495\n",
      "training time: 6.0h 26m 44s\n",
      "\n",
      "Epoch 26/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.627043275301034\n",
      "count loss: 0.0962826310243009\n",
      "occupancy loss: 0.15724502428746173\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.280259150733519\n",
      "count loss: 0.0406727989302875\n",
      "occupancy loss: 0.05318212276034472\n",
      "training time: 6.0h 42m 16s\n",
      "\n",
      "Epoch 27/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6232702789393457\n",
      "count loss: 0.09163371596245162\n",
      "occupancy loss: 0.162760407643281\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.27991226500022115\n",
      "count loss: 0.06962716579640321\n",
      "occupancy loss: 0.17174959893630143\n",
      "training time: 6.0h 57m 44s\n",
      "\n",
      "Epoch 28/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6482860455050059\n",
      "count loss: 0.09456241714419983\n",
      "occupancy loss: 0.14922860295929619\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2802034930814684\n",
      "count loss: 0.06384688754460634\n",
      "occupancy loss: 0.11417995497296977\n",
      "training time: 7.0h 13m 13s\n",
      "\n",
      "Epoch 29/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6436794508207869\n",
      "count loss: 0.09158315697524198\n",
      "occupancy loss: 0.15156677192409862\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.27985690178229167\n",
      "count loss: 0.04703803595024233\n",
      "occupancy loss: 0.23240389696195649\n",
      "training time: 7.0h 28m 42s\n",
      "\n",
      "Epoch 30/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6382728466964854\n",
      "count loss: 0.09159259675090765\n",
      "occupancy loss: 0.14919579516516682\n",
      "\n",
      "validation \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heatmap loss: 0.28013965547277614\n",
      "count loss: 0.06262725355967949\n",
      "occupancy loss: 0.12958461057405313\n",
      "training time: 7.0h 44m 12s\n",
      "\n",
      "Epoch 31/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6347078735313967\n",
      "count loss: 0.08872572081749928\n",
      "occupancy loss: 0.14829044628497626\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2804313989107592\n",
      "count loss: 0.03491514248606648\n",
      "occupancy loss: 0.08339769048596676\n",
      "training time: 7.0h 59m 48s\n",
      "\n",
      "Epoch 32/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6322992704575037\n",
      "count loss: 0.08986886661849378\n",
      "occupancy loss: 0.14636566284814712\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.27996501508702726\n",
      "count loss: 0.049362481640241845\n",
      "occupancy loss: 0.041811216103794666\n",
      "training time: 8.0h 15m 22s\n",
      "\n",
      "Epoch 33/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6365875063410316\n",
      "count loss: 0.08735451713763144\n",
      "occupancy loss: 0.1379193772333371\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28026199377094224\n",
      "count loss: 0.037586181451704816\n",
      "occupancy loss: 0.04705962745232401\n",
      "training time: 8.0h 30m 58s\n",
      "\n",
      "Epoch 34/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6329767682609521\n",
      "count loss: 0.08603138654104998\n",
      "occupancy loss: 0.14946830912655806\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.280226673781657\n",
      "count loss: 0.029074200517516125\n",
      "occupancy loss: 0.05863893771154458\n",
      "training time: 8.0h 46m 31s\n",
      "\n",
      "Epoch 35/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6129782804191889\n",
      "count loss: 0.08200581786276534\n",
      "occupancy loss: 0.14408754700141677\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2802268476751469\n",
      "count loss: 0.060926633932201726\n",
      "occupancy loss: 0.09369995887385513\n",
      "training time: 9.0h 1m 58s\n",
      "\n",
      "Epoch 36/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6269622838968474\n",
      "count loss: 0.08377245179036122\n",
      "occupancy loss: 0.1348988024247257\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28024801659398957\n",
      "count loss: 0.057171276073696785\n",
      "occupancy loss: 0.07431598333026998\n",
      "training time: 9.0h 17m 27s\n",
      "\n",
      "Epoch 37/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6289355864593685\n",
      "count loss: 0.07925999932675094\n",
      "occupancy loss: 0.1220127008213006\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28014145469216206\n",
      "count loss: 0.04527246647768722\n",
      "occupancy loss: 0.047964245073151106\n",
      "training time: 9.0h 32m 57s\n",
      "\n",
      "Epoch 38/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6209195643928126\n",
      "count loss: 0.08075889503972475\n",
      "occupancy loss: 0.14183156270470657\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.27973886294793704\n",
      "count loss: 0.036810156765977674\n",
      "occupancy loss: 0.07757569285090786\n",
      "training time: 9.0h 48m 29s\n",
      "\n",
      "Epoch 39/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6331704325368588\n",
      "count loss: 0.08205009697050379\n",
      "occupancy loss: 0.13642148601119086\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.280238661457258\n",
      "count loss: 0.039142482057628464\n",
      "occupancy loss: 0.08820359001393903\n",
      "training time: 10.0h 3m 59s\n",
      "\n",
      "Epoch 40/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.62354555722124\n",
      "count loss: 0.08084967183703988\n",
      "occupancy loss: 0.13321475681868875\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2798094089443141\n",
      "count loss: 0.01926457136099919\n",
      "occupancy loss: 0.13005055396799756\n",
      "training time: 10.0h 19m 31s\n",
      "\n",
      "Epoch 41/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6261593076723269\n",
      "count loss: 0.07801466021713765\n",
      "occupancy loss: 0.13648690575544398\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28001970643319135\n",
      "count loss: 0.04540511860991218\n",
      "occupancy loss: 0.15960338212288433\n",
      "training time: 10.0h 35m 2s\n",
      "\n",
      "Epoch 42/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.625354686633136\n",
      "count loss: 0.0769162966179454\n",
      "occupancy loss: 0.14156217420749476\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2797113209610887\n",
      "count loss: 0.0356826265717136\n",
      "occupancy loss: 0.03444644231632958\n",
      "training time: 10.0h 50m 35s\n",
      "\n",
      "Epoch 43/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.636250030274689\n",
      "count loss: 0.07808912669134646\n",
      "occupancy loss: 0.13528100112442085\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2799043144007407\n",
      "count loss: 0.040861525993649596\n",
      "occupancy loss: 0.11966728090872632\n",
      "training time: 11.0h 6m 9s\n",
      "\n",
      "Epoch 44/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6255066435071794\n",
      "count loss: 0.0780107358899654\n",
      "occupancy loss: 0.13760124964318268\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28034108647995853\n",
      "count loss: 0.036794734322836815\n",
      "occupancy loss: 0.14970876287550244\n",
      "training time: 11.0h 21m 41s\n",
      "\n",
      "Epoch 45/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6238254564823612\n",
      "count loss: 0.07636890172224507\n",
      "occupancy loss: 0.1215377156336932\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28032001580835986\n",
      "count loss: 0.04558968401689485\n",
      "occupancy loss: 0.08617082342456711\n",
      "training time: 11.0h 37m 13s\n",
      "\n",
      "Epoch 46/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6307035366354786\n",
      "count loss: 0.07530298592400957\n",
      "occupancy loss: 0.12720870362978975\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2799187066457823\n",
      "count loss: 0.057242699860116784\n",
      "occupancy loss: 0.08192802258720223\n",
      "training time: 11.0h 52m 47s\n",
      "\n",
      "Epoch 47/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.630868545921542\n",
      "count loss: 0.07461436765866394\n",
      "occupancy loss: 0.12121575318370471\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.27989513146891576\n",
      "count loss: 0.050714614993486694\n",
      "occupancy loss: 0.0918073021632557\n",
      "training time: 12.0h 8m 22s\n",
      "\n",
      "Epoch 48/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6253331788528848\n",
      "count loss: 0.07182215667161353\n",
      "occupancy loss: 0.1195707852510128\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2803727313522858\n",
      "count loss: 0.032352587866725346\n",
      "occupancy loss: 0.07130146092099074\n",
      "training time: 12.0h 23m 57s\n",
      "\n",
      "Epoch 49/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6357642819197277\n",
      "count loss: 0.07362023066250124\n",
      "occupancy loss: 0.13185024490527647\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2799895863776001\n",
      "count loss: 0.03503650882287134\n",
      "occupancy loss: 0.05376454389518868\n",
      "training time: 12.0h 39m 31s\n",
      "\n",
      "Epoch 50/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.620981017795146\n",
      "count loss: 0.07276349049992165\n",
      "occupancy loss: 0.12320819221138363\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.27959200151686614\n",
      "count loss: 0.05345557663669446\n",
      "occupancy loss: 0.06283911782664836\n",
      "training time: 12.0h 55m 7s\n",
      "\n",
      "Epoch 51/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6219982806526443\n",
      "count loss: 0.07302341443698891\n",
      "occupancy loss: 0.128935542102457\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28031539807295397\n",
      "count loss: 0.055692211813739075\n",
      "occupancy loss: 0.06702103599361056\n",
      "training time: 13.0h 10m 43s\n",
      "\n",
      "Epoch 52/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6190194051188738\n",
      "count loss: 0.07053622721485216\n",
      "occupancy loss: 0.11415224185324786\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28024237686463177\n",
      "count loss: 0.05020672528175579\n",
      "occupancy loss: 0.18238531174682848\n",
      "training time: 13.0h 26m 17s\n",
      "\n",
      "Epoch 53/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6293096246962612\n",
      "count loss: 0.071834916682995\n",
      "occupancy loss: 0.12206759928461347\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2798327599011527\n",
      "count loss: 0.041167116969236445\n",
      "occupancy loss: 0.06551562551369923\n",
      "training time: 13.0h 41m 53s\n",
      "\n",
      "Epoch 54/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6365702563938623\n",
      "count loss: 0.07031751271841655\n",
      "occupancy loss: 0.11994467825867307\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2803926671888293\n",
      "count loss: 0.05505010434771846\n",
      "occupancy loss: 0.11644103618573116\n",
      "training time: 13.0h 57m 28s\n",
      "\n",
      "Epoch 55/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6349119772785358\n",
      "count loss: 0.0719059659093626\n",
      "occupancy loss: 0.12749459932988172\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.27988772091498987\n",
      "count loss: 0.04798208917870016\n",
      "occupancy loss: 0.07859960051463881\n",
      "training time: 14.0h 13m 3s\n",
      "\n",
      "Epoch 56/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6316855000965771\n",
      "count loss: 0.06940485024009092\n",
      "occupancy loss: 0.11622438810458122\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28025650018758125\n",
      "count loss: 0.053947427835458174\n",
      "occupancy loss: 0.08048343342722292\n",
      "training time: 14.0h 28m 38s\n",
      "\n",
      "Epoch 57/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.64117177361481\n",
      "count loss: 0.06990245807868353\n",
      "occupancy loss: 0.12082632144434301\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2799103600970849\n",
      "count loss: 0.04032341372753954\n",
      "occupancy loss: 0.12568845838873363\n",
      "training time: 14.0h 44m 14s\n",
      "\n",
      "Epoch 58/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.641214897815825\n",
      "count loss: 0.07204071095555195\n",
      "occupancy loss: 0.11741508243938173\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2797701137545536\n",
      "count loss: 0.04295546141000358\n",
      "occupancy loss: 0.12703547222624129\n",
      "training time: 14.0h 59m 53s\n",
      "\n",
      "Epoch 59/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6321642201860441\n",
      "count loss: 0.06954886456499278\n",
      "occupancy loss: 0.11271510142376603\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28087838538779153\n",
      "count loss: 0.03959942654291215\n",
      "occupancy loss: 0.11390602616189555\n",
      "training time: 15.0h 15m 27s\n",
      "\n",
      "Epoch 60/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heatmap loss: 0.631667833090941\n",
      "count loss: 0.06760439341084641\n",
      "occupancy loss: 0.12157173480037574\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.27989187500607005\n",
      "count loss: 0.03947690111928676\n",
      "occupancy loss: 0.16284791938948726\n",
      "training time: 15.0h 30m 59s\n",
      "\n",
      "Epoch 61/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6309018174922515\n",
      "count loss: 0.06676017709963245\n",
      "occupancy loss: 0.11121211752414835\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.27966567064667225\n",
      "count loss: 0.026796972195153798\n",
      "occupancy loss: 0.08945187990587859\n",
      "training time: 15.0h 46m 33s\n",
      "\n",
      "Epoch 62/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6352007173089391\n",
      "count loss: 0.06838139458362069\n",
      "occupancy loss: 0.11013147122438867\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2814122343507411\n",
      "count loss: 0.026826698655835687\n",
      "occupancy loss: 0.1288429330919323\n",
      "training time: 16.0h 2m 5s\n",
      "\n",
      "Epoch 63/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6189938414381942\n",
      "count loss: 0.06623604103551359\n",
      "occupancy loss: 0.11722928369976882\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28038966299475804\n",
      "count loss: 0.04094414192133549\n",
      "occupancy loss: 0.06354582476959485\n",
      "training time: 16.0h 17m 39s\n",
      "\n",
      "Epoch 64/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6189283335263799\n",
      "count loss: 0.06593747482946884\n",
      "occupancy loss: 0.11993889209183878\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2794885866228468\n",
      "count loss: 0.04432544983794756\n",
      "occupancy loss: 0.04752178333071887\n",
      "training time: 16.0h 33m 13s\n",
      "\n",
      "Epoch 65/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6192491174512011\n",
      "count loss: 0.06539734697339533\n",
      "occupancy loss: 0.1091219228138677\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2798208195910772\n",
      "count loss: 0.04395275463574344\n",
      "occupancy loss: 0.07307227118594611\n",
      "training time: 16.0h 48m 49s\n",
      "\n",
      "Epoch 66/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6270432546686302\n",
      "count loss: 0.06563078148913137\n",
      "occupancy loss: 0.10618027966529077\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.28015103203975866\n",
      "count loss: 0.032189835083326984\n",
      "occupancy loss: 0.07391977293116764\n",
      "training time: 17.0h 4m 25s\n",
      "\n",
      "Epoch 67/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6299318396137327\n",
      "count loss: 0.06489491997514339\n",
      "occupancy loss: 0.1168033762415477\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.27993671916478624\n",
      "count loss: 0.04230419944192456\n",
      "occupancy loss: 0.12051827521560571\n",
      "training time: 17.0h 20m 2s\n",
      "\n",
      "Epoch 68/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6403167356879714\n",
      "count loss: 0.0653573802032972\n",
      "occupancy loss: 0.11424139682780503\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2797004796557245\n",
      "count loss: 0.023412636351423877\n",
      "occupancy loss: 0.06753887633569837\n",
      "training time: 17.0h 35m 43s\n",
      "\n",
      "Epoch 69/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6324002517722165\n",
      "count loss: 0.06297833877188284\n",
      "occupancy loss: 0.11797706061023681\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.27973762632178567\n",
      "count loss: 0.04366219965536904\n",
      "occupancy loss: 0.058684797150933596\n",
      "training time: 17.0h 51m 21s\n",
      "\n",
      "Epoch 70/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6247425390993823\n",
      "count loss: 0.06093722951928725\n",
      "occupancy loss: 0.10562381640132044\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.27972225557049446\n",
      "count loss: 0.024463176275741343\n",
      "occupancy loss: 0.05756062028528032\n",
      "training time: 18.0h 6m 57s\n",
      "\n",
      "Epoch 71/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6321821930680546\n",
      "count loss: 0.06214358233217203\n",
      "occupancy loss: 0.1070514844574751\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2796216429167941\n",
      "count loss: 0.030337348166427393\n",
      "occupancy loss: 0.09685811766248804\n",
      "training time: 18.0h 22m 32s\n",
      "\n",
      "Epoch 72/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6236107609968928\n",
      "count loss: 0.062032825024702455\n",
      "occupancy loss: 0.10811882888914338\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.27993810292422366\n",
      "count loss: 0.04162191344920562\n",
      "occupancy loss: 0.07638479468925392\n",
      "training time: 18.0h 38m 5s\n",
      "\n",
      "Epoch 73/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6365353345050223\n",
      "count loss: 0.0657309556893125\n",
      "occupancy loss: 0.11497638259567713\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2797042531672492\n",
      "count loss: 0.04870199190263069\n",
      "occupancy loss: 0.07115364756092896\n",
      "training time: 18.0h 53m 40s\n",
      "\n",
      "Epoch 74/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6314291450123487\n",
      "count loss: 0.06166284776253168\n",
      "occupancy loss: 0.10590877156187872\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.2800548190462555\n",
      "count loss: 0.04718012376999966\n",
      "occupancy loss: 0.10368214831802339\n",
      "training time: 19.0h 9m 14s\n",
      "\n",
      "Epoch 75/75\n",
      "----------\n",
      "\n",
      "training \n",
      "\n",
      "heatmap loss: 0.6208645112500196\n",
      "count loss: 0.06395285288347212\n",
      "occupancy loss: 0.11397733398292718\n",
      "\n",
      "validation \n",
      "\n",
      "heatmap loss: 0.27971556479693954\n",
      "count loss: 0.02823792628343728\n",
      "occupancy loss: 0.16214161851817824\n",
      "training time: 19.0h 24m 52s\n",
      "\n",
      "Training complete in 19.0h 24m 52s\n"
     ]
    }
   ],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_3.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        #continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                             --hyperparameter_set=$hyp_st --output_name=$out  \\\n",
    "                             --models_folder=$dest_folder\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"3V\"></a>\n",
    "\n",
    "This script validates models from training, generating .csvs with precision and recall on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_3.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is trained\n",
    "    if \"{}.tar\".format(out) not in os.listdir('{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was not trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo validating $out\n",
    "    print()\n",
    "    \n",
    "    # run validation\n",
    "    !python validate_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                                --hyperparameter_set=$hyp_st \\\n",
    "                                --output_name=$out --models_folder=$dest_folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
