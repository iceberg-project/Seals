{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seal Detection Pipeline\n",
    "---\n",
    "\n",
    "This jupyter notebook will go through training CNN within the SealNet pipeline. Requires a training set generated on the [training_set_generation](https://github.com/iceberg-project/Seals/blob/paper/SealNet_code/training_set_generation.ipynb) notebook.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "---\n",
    "* [Getting started](#intro)\n",
    "    * [Setup](#setup)\n",
    "    * [Visualize training set](#vis_imgs)\n",
    "* [Pipeline 1 - Heatmap](#1)\n",
    "    * [Training](#1T)\n",
    "    * [Validation](#1V)\n",
    "* [Pipeline 2.1 - Heatmap + count](#2.1)\n",
    "    * [Training](#2.1T)\n",
    "    * [Validation](#2.1V)\n",
    "* [Pipeline 2.2 - Heatmap + occupancy](#2.2)\n",
    "    * [Training](#2.2T)\n",
    "    * [Validation](#2.2V)\n",
    "* [Pipeline 3 - Heatmap + count + occupancy](#3)\n",
    "    * [Training](#3T)\n",
    "    * [Validation](#3V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting started<a name=\"intro\"></a>\n",
    "---\n",
    "\n",
    "If you followed the *training_set_generation* jupyter notebook (also present in this repo), you should have training sets generated and hyperparameter sets to try out, and be ready to search for a best performing seal detection pipeline.  Output files in this repository are organized as follows: *'./{dest_folder}/{pipeline}/{model_settings}/{model_settings}_{file}'*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment<a name=\"setup\"></a>\n",
    "\n",
    "Before training and validating model/hyperparameter combinations inside the pipelines, we need to load the required python modules and a few global variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import required packages\n",
    "import os\n",
    "import rasterio\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import operator\n",
    "from PIL import Image \n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib as mpl\n",
    "from functools import reduce\n",
    "from utils.model_library import * \n",
    "\n",
    "%matplotlib inline\n",
    "mpl.rcParams['figure.dpi']= 400\n",
    "\n",
    "# destination folder for saved models and model stats\n",
    "dest_folder = 'saved_models'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training images (Optional)<a name=\"vis_imgs\"></a>\n",
    "\n",
    "To get a better sense for what the training set is like, the next cell will display a few random images from the training classes. Displayed images are extracted from a pool of ~75000 training images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save class names\n",
    "class_names = sorted([subdir for subdir in os.listdir('./training_sets/training_set_vanilla/training')])\n",
    "# store images\n",
    "images = []\n",
    "\n",
    "# loop over labels\n",
    "for label in class_names:\n",
    "    for path, _, files in os.walk('./training_sets/training_set_vanilla/training/{}'.format(label)):\n",
    "        files = np.random.choice(files, 5)\n",
    "        for filename in files:\n",
    "            images.append(np.asarray(Image.open(os.path.join(path, filename))))\n",
    "\n",
    "images = np.array(images)\n",
    "\n",
    "# display images \n",
    "ncols=len(class_names)\n",
    "nindex, height, width, intensity = images.shape\n",
    "nrows = nindex // ncols\n",
    "# check if rows and columns can fit the number of images\n",
    "assert nindex == nrows * ncols\n",
    "result = (images.reshape(nrows, ncols, height, width, intensity)\n",
    "          .swapaxes(1,2)\n",
    "          .reshape(height*nrows, width*ncols, intensity))\n",
    "\n",
    "plt.imshow(result)\n",
    "cur_axes = plt.gca()\n",
    "cur_axes.axes.get_xaxis().set_visible(False)\n",
    "cur_axes.axes.get_yaxis().set_visible(False)\n",
    "plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 1 - Heatmap models <a name=\"1\"></a>\n",
    "---\n",
    "\n",
    "Heatmap models work by using semantic segmentation to generate a pixel-wise probability of a cell being a seal centroid. With a matrix of probabilities, we obtain a count by applying a sigmoid transform to it, thresholding and adding over all cells. Seal locations on an image are determined by finding the *n* greatest peaks of intensity in the image, where *n* is the count for that image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"1T\"></a>\n",
    "\n",
    "The following cell trains heatmap based models on a selected combination of architectures, training sets and hyperparameters. For valid entries, see \"*/utils/model_library.py*\" within the main repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# switch pipeline\n",
    "pipeline = 'Heatmap'\n",
    "\n",
    "# generate model combinations\n",
    "combinations_1 = {'model_architecture': ['Unet'],\n",
    "                  'training_dir': ['training_set_vanilla'],\n",
    "                  'hyperparameter_set': ['E']}\n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_1 = pd.DataFrame(combinations_1)\n",
    "                    \n",
    "# create folders for resulting files\n",
    "for row in combinations_1.iterrows():\n",
    "    mdl = row[1]['model_architecture'] + '_ts-' + row[1]['training_dir'].split('_')[-1]              \n",
    "    if not os.path.exists(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)):\n",
    "        os.makedirs(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then provide model combinations created above as arguments to the training script, *train_sealnet.py*. A list of required arguments can be displayed by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%run train_sealnet.py -h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_1.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                             --hyperparameter_set=$hyp_st \\\n",
    "                             --output_name=$out --dest_folder=$dest_folder\n",
    "                              \n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"1V\"></a> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"1V\"></a>\n",
    "\n",
    "This script validates models from training, generating .csvs with precision and recall on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_1.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo validating $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python validate_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                                --hyperparameter_set=$hyp_st \\\n",
    "                                --output_name=$out --dest_folder=$dest_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 2.1 - Heatmap + count<a name=\"1.1\"></a>\n",
    "---\n",
    "\n",
    "This pipeline will also generate a seal intensity heatmap like the previous one, but it does not add over pixels to get a count. Instead, Heatmap + count models have a specialized branch to get a count by regression. Seal locations on an image are determined by finding the *n* greatest peaks of intensity in the image, where *n* is the count for that image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"2.1T\"></a>\n",
    "\n",
    "The following cell trains heatmap + count based models on a selected combination of architectures, training sets and hyperparameters. For valid entries, see \"*/utils/model_library.py*\" within the main repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# switch pipeline\n",
    "pipeline = 'Heatmap-cnt'\n",
    "\n",
    "# generate model combinations\n",
    "combinations_21 = {'model_architecture': ['UnetCntWRN'],\n",
    "                   'training_dir': ['training_set_vanilla'],\n",
    "                   'hyperparameter_set': ['E']}       \n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_21 = pd.DataFrame(combinations_21)\n",
    "                    \n",
    "\n",
    "# create folders for resulting files\n",
    "for row in combinations_21.iterrows():\n",
    "    mdl = row[1]['model_architecture'] + '_ts-' + row[1]['training_dir'].split('_')[-1]               \n",
    "    if not os.path.exists(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)):\n",
    "        os.makedirs(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train a counting model, model combinations created above are used as argument to to a new training script, *train_sealnet_count.py*, which uses MSE loss. It accepts the same arguments as the previous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"1V\"></a>\n",
    "\n",
    "This script validates models from training, generating .csvs with precision and recall on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_21.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo validating $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python validate_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                                --hyperparameter_set=$hyp_st \\\n",
    "                                --output_name=$out --dest_folder=$dest_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline 2.2 - Heatmap + occupancy <a name=\"1.2\"></a>\n",
    "---\n",
    "\n",
    "Heatmap + occupancy models, like pure Heatmap models, work by using semantic segmentation to generate a pixel-wise probability of a cell being a seal centroid. With a matrix of probabilities, we obtain a count by applying a sigmoid transform to it, thresholding and adding over all cells. After addining up cells, however, we use the output from a specialized occupancy branch -- and another threshold -- to decide if predicted counts will be set to zero (i.e. image is not occupied). Seal locations on an image are determined by finding the *n* greatest peaks of intensity in the image, where *n* is the count for that image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"2.2T\"></a>\n",
    "\n",
    "The following cell trains heatmap + occupancy based models on a selected combination of architectures, training sets and hyperparameters. For valid entries, see \"*/utils/model_library.py*\" within the main repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# switch pipeline\n",
    "pipeline = 'Heatmap-occ'\n",
    "\n",
    "# generate model combinations\n",
    "combinations_22 = {'model_architecture': ['UnetOccDense'],\n",
    "                   'training_dir': ['training_set_vanilla'],\n",
    "                   'hyperparameter_set': ['E'] }       \n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_22 = pd.DataFrame(combinations_22)\n",
    "                    \n",
    "\n",
    "# create folders for resulting files\n",
    "for row in combinations_22.iterrows():\n",
    "    mdl = row[1]['model_architecture'] + '_ts-' + row[1]['training_dir'].split('_')[-1]                  \n",
    "    if not os.path.exists(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)):\n",
    "        os.makedirs(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_22.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        #continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                             --hyperparameter_set=$hyp_st --output_name=$out  \\\n",
    "                             --dest_folder=$dest_folder\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"1V\"></a>\n",
    "\n",
    "This script validates models from training, generating .csvs with precision and recall on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_22.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo validating $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python validate_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                                --hyperparameter_set=$hyp_st \\\n",
    "                                --output_name=$out --dest_folder=$dest_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline 3 - Heatmap + Count + Occupancy<a name=\"1.3T\"></a>\n",
    "\n",
    "Like all previous ones, this pipeline will also generate a seal intensity heatmap. Similar to Heatmap + count models, models on this pipeline have a specialized branch to get a count by regression. After counting, however, we use a threshold and the output from a specialized occupancy branch to decide if we will set the count to zero (i.e. image is not occupied). Seal locations on an image are determined by finding the *n* greatest peaks of intensity in the image, where *n* is the count for that image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training<a name=\"3T\"></a>\n",
    "\n",
    "The following cell trains heatmap + count + occupancy based models on a selected combination of architectures, training sets and hyperparameters. For valid entries, see \"*/utils/model_library.py*\" within the main repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# switch pipeline\n",
    "pipeline = 'Heatmap-Cnt-Occ'\n",
    "\n",
    "# generate model combinations\n",
    "combinations_3 = {'model_architecture': ['UnetCntWRNOccDense'],\n",
    "                   'training_dir': ['training_set_vanilla'],\n",
    "                   'hyperparameter_set': ['F'] }       \n",
    "\n",
    "# read as a DataFrame\n",
    "combinations_3 = pd.DataFrame(combinations_3)\n",
    "                    \n",
    "\n",
    "# create folders for resulting files\n",
    "for row in combinations_3.iterrows():\n",
    "    mdl = row[1]['model_architecture'] + '_ts-' + row[1]['training_dir'].split('_')[-1]                  \n",
    "    if not os.path.exists(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)):\n",
    "        os.makedirs(\"./{}/{}/{}\".format(dest_folder, pipeline, mdl)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_3.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        #continue\n",
    "    \n",
    "    print()\n",
    "    !echo training $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python train_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                             --hyperparameter_set=$hyp_st --output_name=$out  \\\n",
    "                             --dest_folder=$dest_folder\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation<a name=\"1V\"></a>\n",
    "\n",
    "This script validates models from training, generating .csvs with precision and recall on the validation set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# iterate over combinations\n",
    "for row in combinations_3.iterrows():\n",
    "    \n",
    "    # read hyperparameters\n",
    "    t_dir, arch, hyp_st = row[1]['training_dir'], row[1]['model_architecture'], \\\n",
    "                          row[1]['hyperparameter_set']\n",
    "    out = arch + '_ts-' + t_dir.split('_')[-1]\n",
    "    \n",
    "    # check if model is already trained\n",
    "    if \"{}.tar\".format(out) in os.listdir('./{}/{}/{}/'.format(dest_folder, pipeline, out)): \n",
    "        print('{} was already trained'.format(out))\n",
    "        continue\n",
    "    \n",
    "    print()\n",
    "    !echo validating $out\n",
    "    print()\n",
    "    \n",
    "    # run training\n",
    "    !python validate_sealnet.py --training_dir=$t_dir --model_architecture=$arch \\\n",
    "                                --hyperparameter_set=$hyp_st \\\n",
    "                                --output_name=$out --dest_folder=$dest_folder"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
