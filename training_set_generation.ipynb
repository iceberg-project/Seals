{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training set creation\n",
    "\n",
    "## Getting started\n",
    "\n",
    "This notebook will walk you through how we created the training set to train and validate instances of sealnet. From generating a vector database to extracting patches from rasters. To recreate the trining set, you will need Qgis (tested for 2.8), Python 3.6, WV03 raster image catalog and a shapefile with a database for seal occurences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "---\n",
    "* [Exporting points](#exp)\n",
    "* [Extracting patches](#extract)\n",
    "* [Creating scene bank](#scene)\n",
    "* [Generating synthetic seal images](#synth)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exporting points <a name=\"exp\"></a>\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can extract patches from raster files, we need to export our database of seal points to a csv file. This operation requires the MMQGIS plugin and can be done by opening Qgis with the seal points shape file and selecting the following option: \n",
    "\n",
    "<img src=\"jupyter_notebook_images/export_geometry.png\">\n",
    "\n",
    "If you selected the correct shape file as the input layer, you will be prompted to save a .csv file. Keep the default name and move this .csv file to the root directory of this repository. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting patches<a name=\"extract\"></a>\n",
    "---\n",
    "\n",
    "Once the output from the geometry export is in the repository root, we are ready to extract patches from the raster files. Run the following cell to extract patches and create the training sets. This process will take around 20 minutes and requires at least 16GB of RAM. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating training_set_vanilla_grayscale:\n",
      "\n",
      "Checking input folder for invalid files:\n",
      "\n",
      "\n",
      "  WV03_20141120214545_1040010005B62F00_14NOV20214545-P1BS-500258392060_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20141107053013_10400100046B2800_14NOV07053013-P1BS-500268574010_01_P006_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20160225140323_10400100196BE200_16FEB25140323-P1BS-500638709010_01_P009_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20160210052144_10400100184CC500_16FEB10052144-P1BS-500687302080_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20160227062822_10400100181F9B00_16FEB27062822-P1BS-500638715080_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20160227062810_10400100191DC900_16FEB27062810-P1BS-500638658040_01_P002_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20151105045105_104001001323F200_15NOV05045105-P1BS-500658675020_01_P002_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  beagle2.tif is not an annotated scene.\n",
      "  WV03_20170217064537_10400100297FEA00_17FEB17064537-P1BS-057107305010_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20151009060344_1040010011162500_15OCT09060344-P1BS-500652417060_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20160204214026_104001001842DA00_16FEB04214026-P1BS-500638664060_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20170125165806_1040010028CD9C00_17JAN25165806-P1BS-057089610010_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20151008072633_1040010012A79A00_15OCT08072633-P1BS-500652459050_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20170106053006_10400100272BF900_17JAN06053006-P1BS-057089629010_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20151005095617_1040010012382500_15OCT05095617-P1BS-500652418050_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20151009060254_1040010012714300_15OCT09060254-P1BS-500652424060_01_P002_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20141120134219_104001000434FC00_14NOV20134219-P1BS-500247720120_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20150119044808_1040010006376F00_15JAN19044808-P1BS-500342279110_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20151022204340_10400100127E2F00_15OCT22204340-P1BS-500652476030_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20151030210819_104001001351DB00_15OCT30210819-P1BS-500658695070_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20141016075835_1040010003BF3000_14OCT16075835-P1BS-500258432200_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20160225140324_10400100196BE200_16FEB25140324-P1BS-500638709010_01_P010_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20151024095526_10400100123DE100_15OCT24095526-P1BS-500656020090_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20151024193848_1040010013779B00_15OCT24193848-P1BS-500656046010_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20141020184444_10400100031B9000_14OCT20184444-P1BS-500258392140_01_P008_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20170125165806_1040010028CD9C00_17JAN25165806-P1BS-057089610010_01_P002_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20151027233452_104001001368B100_15OCT27233452-P1BS-500487629030_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20151116060213_1040010014307900_15NOV16060213-P1BS-500658937040_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20160307021814_1040010018046800_16MAR07021814-P1BS-500687218040_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20150109052711_1040010006412700_15JAN09052711-P1BS-500292179110_01_P006_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20141030211711_10400100036F5800_14OCT30211711-P1BS-500231417160_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20151126133728_1040010013346700_15NOV26133728-P1BS-500666441080_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20141110205238_1040010004751700_14NOV10205238-P1BS-500231412090_01_P003_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20151005050119_1040010011172D00_15OCT05050119-P1BS-500638962090_01_P005_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "  WV03_20151021104725_10400100124A0000_15OCT21104725-P1BS-500652455030_01_P001_u08rf3031.tif.aux.xml is not a valid scene.\n",
      "\n",
      "Creating dataset:\n",
      "\n",
      "\n",
      "  Processed 1 out of 52 rasters\n",
      "\n",
      "  Processed 2 out of 52 rasters\n",
      "\n",
      "  Processed 3 out of 52 rasters\n",
      "\n",
      "  Processed 4 out of 52 rasters\n",
      "\n",
      "  Processed 5 out of 52 rasters\n",
      "\n",
      "  Processed 6 out of 52 rasters\n",
      "\n",
      "  Processed 7 out of 52 rasters\n",
      "\n",
      "  Processed 8 out of 52 rasters\n",
      "\n",
      "  Processed 9 out of 52 rasters\n",
      "\n",
      "  Processed 10 out of 52 rasters\n",
      "\n",
      "  Processed 11 out of 52 rasters\n",
      "\n",
      "  Processed 12 out of 52 rasters\n",
      "\n",
      "  Processed 13 out of 52 rasters\n",
      "\n",
      "  Processed 14 out of 52 rasters\n",
      "\n",
      "  Processed 15 out of 52 rasters\n",
      "\n",
      "  Processed 16 out of 52 rasters\n",
      "\n",
      "  Processed 17 out of 52 rasters\n",
      "\n",
      "  Processed 18 out of 52 rasters\n",
      "\n",
      "  Processed 19 out of 52 rasters\n",
      "\n",
      "  Processed 20 out of 52 rasters\n",
      "\n",
      "  Processed 21 out of 52 rasters\n",
      "\n",
      "  Processed 22 out of 52 rasters\n",
      "\n",
      "  Processed 23 out of 52 rasters\n",
      "\n",
      "  Processed 24 out of 52 rasters\n",
      "\n",
      "  Processed 25 out of 52 rasters\n",
      "\n",
      "  Processed 26 out of 52 rasters\n",
      "\n",
      "  Processed 27 out of 52 rasters\n",
      "\n",
      "  Processed 28 out of 52 rasters\n",
      "\n",
      "  Processed 29 out of 52 rasters\n",
      "\n",
      "  Processed 30 out of 52 rasters\n",
      "\n",
      "  Processed 31 out of 52 rasters\n",
      "\n",
      "  Processed 32 out of 52 rasters\n",
      "\n",
      "  Processed 33 out of 52 rasters\n",
      "\n",
      "  Processed 34 out of 52 rasters\n",
      "\n",
      "  Processed 35 out of 52 rasters\n",
      "\n",
      "  Processed 36 out of 52 rasters\n",
      "\n",
      "  Processed 37 out of 52 rasters\n",
      "\n",
      "  Processed 38 out of 52 rasters\n",
      "\n",
      "  Processed 39 out of 52 rasters\n",
      "\n",
      "  Processed 40 out of 52 rasters\n",
      "\n",
      "  Processed 41 out of 52 rasters\n",
      "\n",
      "  Processed 42 out of 52 rasters\n",
      "\n",
      "  Processed 43 out of 52 rasters\n",
      "\n",
      "  Processed 44 out of 52 rasters\n",
      "\n",
      "  Processed 45 out of 52 rasters\n",
      "\n",
      "  Processed 46 out of 52 rasters\n",
      "\n",
      "  Processed 47 out of 52 rasters\n",
      "\n",
      "  Processed 48 out of 52 rasters\n",
      "\n",
      "  Processed 49 out of 52 rasters\n",
      "\n",
      "  Processed 50 out of 52 rasters\n",
      "\n",
      "  Processed 51 out of 52 rasters\n",
      "\n",
      "  Processed 52 out of 52 rasters\n",
      "\n",
      "\n",
      "86298 training images created in 23m 12s\n"
     ]
    }
   ],
   "source": [
    "# point to folder with raster images\n",
    "raster_dir = '/home/bento/training_set_scenes'\n",
    "\n",
    "# training set vanilla\n",
    "\n",
    "# point to shapefile\n",
    "shape_file = 'temp-nodes.csv'\n",
    "# specify training labels, separate each class by an '_', classes need to correspond to ones in the shapefile\n",
    "labels = 'crabeater_crack_emperor_glacier_ice-sheet_marching-emperor_open-water_pack-ice_rock_shadow_weddell'\n",
    "# specify labels of classes used for detection\n",
    "det_classes = 'crabeater_weddell'\n",
    "\n",
    "%run create_trainingset.py --det_classes=$det_classes --rasters_dir=$raster_dir --scale_bands='450' --out_folder='training_set_vanilla' --labels=$labels --shape_file=$shape_file --rgb='0'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating scene bank<a name=\"scene\"></a>\n",
    "---\n",
    "\n",
    "One step in evaluating sealnet instances is defining how well the models can identify scenes that contain seals. In order to measure that we need to create a 'scene bank' which stores which scenes count as positives (i.e. has seals) or negatives. To generate scene banks, run the cell bellow: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating seal scene banks\n",
    "for label in ['crabeater', 'weddell', 'emperor', 'marching-emperor']:\n",
    "    out_file = \"{}_scene_bank.csv\".format(label)\n",
    "    %run create_scene_bank.py --positive_classes=$label --out_file=$out_file\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthesizing new seal haul out images<a name=\"synth\"></a>\n",
    "---\n",
    "\n",
    "We can also create synthetic images by cropping seals into different seal backgrounds. This is an easy way to create testing images, which should have a clear answer and were not seen during training. We can do this in two steps: *1) generate seal and background image banks; 2) insert a random number of seals into background images*  It might also be a useful way to generate more training data, given that the seal and background image banks are sufficiently rich and accurately represent a distribution of real seal images. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating seal bank\n",
    "\n",
    "The following cell loops through the seal point catalog and crops individual seals, saving images with a single seal into a 'seal_bank' folder inside 'training_sets'. This will also save the mean and standard deviation for the distance to the nearest neighbor, which will be later used to decide how clumped individual seals can be in synthesized images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate seal bank for crabeaters and weddells\n",
    "for spcs in ['weddell', 'crabeater']:\n",
    "    %run create_seal_bank.py --training_dir='training_set_vanilla' \\\n",
    "                             --out_folder='./training_sets/training_set_synthesized/seal_bank' \\\n",
    "                             --label=$spcs\n",
    "                    \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
